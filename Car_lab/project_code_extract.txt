# Project Code Extract
# Generated on: 2025-07-08 23:27:20

## Directory Structure
```
/Users/azeez/Google_Drive/Life/PhD/AppCV_2025/AppCV_2025/Car_lab
‚îú‚îÄ‚îÄ core
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ camera_manager.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ flask_control.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ robot_controller.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ run.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ templates
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ control.html
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ console_logger.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ debug_visualizer.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ movement_controls.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ feature_config.py
‚îú‚îÄ‚îÄ midas_depth-1.0.0-py3-none-any.whl
‚îú‚îÄ‚îÄ project_code_extract.txt
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ teaching_tools
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 6panel_pid_graph.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ canny_filter.py
‚îú‚îÄ‚îÄ week1_line_following
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ line_follower.py
‚îú‚îÄ‚îÄ week2_object_detection
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ best.onnx
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sign_detector.py
‚îî‚îÄ‚îÄ week3_speed_estimation
    ‚îú‚îÄ‚îÄ calibration_params.json
    ‚îú‚îÄ‚îÄ calibration_script.py
    ‚îú‚îÄ‚îÄ show_stats.py
    ‚îú‚îÄ‚îÄ speed_estimator.py
    ‚îî‚îÄ‚îÄ speed_estimator_skeleton.py

9 directories, 24 files

```

## Code Files


################################################################################
# FILE: core/camera_manager.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import threading
import time
import subprocess
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraManager:
    def __init__(self):
        """Initialize camera using libcamera-vid streaming"""
        self.frame = None
        self.is_running = False
        self.lock = threading.Lock()
        self.process = None
        self.current_frame = None
        
        logger.info("Camera manager initialized")
    
    def _create_placeholder_frame(self, message="Waiting for camera..."):
        """Create a frame with a message"""
        frame = np.zeros((240, 320, 3), dtype=np.uint8)
        
        # Add message
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        color = (255, 255, 255)
        thickness = 1
        
        # Calculate text size and position for centering
        text_size = cv2.getTextSize(message, font, font_scale, thickness)[0]
        text_x = (frame.shape[1] - text_size[0]) // 2
        text_y = (frame.shape[0] + text_size[1]) // 2
        
        cv2.putText(frame, message, (text_x, text_y), font, font_scale, color, thickness)
        
        # Add timestamp
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        cv2.putText(frame, timestamp, (5, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        return frame
    
    def start_streaming(self):
        """Start the camera streaming using libcamera-vid"""
        if not self.is_running:
            self.is_running = True
            self.capture_thread = threading.Thread(target=self._capture_frames)
            self.capture_thread.daemon = True
            self.capture_thread.start()
            logger.info("Camera streaming started")
    
    def _capture_frames(self):
        """Continuously capture frames using libcamera-vid streaming (from working example)"""
        logger.info("Starting libcamera-vid streaming...")
        
        try:
            # Start libcamera-vid streaming to stdout (exact command from working example)
            cmd = [
                "libcamera-vid",
                "-t", "0",  # Infinite timeout
                "--width", "320",
                "--height", "240",
                "--framerate", "15",
                "-o", "-",  # Output to stdout
                "--codec", "mjpeg",
                "--inline",
                "-n"  # No preview
            ]
            
            self.process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # Test if the process starts successfully
            time.sleep(1)
            if self.process.poll() is not None:
                stderr_output = self.process.stderr.read().decode('utf-8')
                logger.error(f"libcamera-vid failed to start. Error: {stderr_output}")
                self._fallback_placeholder_loop()
                return
            
            logger.info("libcamera-vid streaming started successfully")
            
            buffer = b""
            while self.is_running:
                try:
                    chunk = self.process.stdout.read(1024)
                    if not chunk:
                        break
                    
                    buffer += chunk
                    
                    # Look for JPEG boundaries
                    start = buffer.find(b'\xff\xd8')
                    end = buffer.find(b'\xff\xd9')
                    
                    if start != -1 and end != -1 and end > start:
                        # Extract JPEG frame
                        jpeg_data = buffer[start:end+2]
                        buffer = buffer[end+2:]
                        
                        # Decode JPEG
                        nparr = np.frombuffer(jpeg_data, np.uint8)
                        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                        
                        if frame is not None:
                            with self.lock:
                                self.current_frame = frame.copy()
                
                except Exception as e:
                    logger.error(f"Streaming error: {e}")
                    break
            
            if self.process:
                self.process.terminate()
                
        except Exception as e:
            logger.error(f"libcamera streaming setup failed: {e}")
            self._fallback_placeholder_loop()
    
    def _fallback_placeholder_loop(self):
        """Generate placeholder frames when camera fails"""
        logger.info("Using placeholder frames due to camera failure")
        frame_counter = 0
        while self.is_running:
            frame_counter += 1
            message = f"Camera Error - Frame {frame_counter}"
            with self.lock:
                self.current_frame = self._create_placeholder_frame(message)
            time.sleep(0.1)
    
    def get_frame(self):
        """Get the current frame"""
        with self.lock:
            if self.current_frame is not None:
                return self.current_frame.copy()
            else:
                return self._create_placeholder_frame()
    
    def get_jpeg_frame(self):
        """Get current frame as JPEG bytes for streaming"""
        frame = self.get_frame()
        
        # Add minimal status overlay (just timestamp)
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        cv2.putText(frame, timestamp, (5, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
        
        ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        if ret:
            return buffer.tobytes()
        else:
            # Return placeholder frame if encoding fails
            placeholder_frame = self._create_placeholder_frame("Encoding Error")
            ret, buffer = cv2.imencode('.jpg', placeholder_frame)
            return buffer.tobytes()
    
    def stop_streaming(self):
        """Stop camera streaming"""
        self.is_running = False
        if self.process:
            self.process.terminate()
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join(timeout=1)
        logger.info("Camera streaming stopped")
    
    def cleanup(self):
        """Clean shutdown of camera"""
        self.stop_streaming()
        logger.info("Camera cleaned up")

# Global camera instance
camera = CameraManager()
```


################################################################################
# FILE: core/flask_control.py
################################################################################

```python
#!/usr/bin/env python3

from flask import Flask, render_template, Response, jsonify, request
import atexit
import signal
import sys
import socket
from robot_controller import robot
from camera_manager import camera
from utils.console_logger import console_logger


# Initialize Flask app
app = Flask(__name__)

# Global state for tracking commands
last_command = "none"
command_count = 0

def get_local_ip():
    """Get the local IP address of this machine"""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
        return local_ip
    except Exception:
        try:
            hostname = socket.gethostname()
            local_ip = socket.gethostbyname(hostname)
            if local_ip.startswith("127."):
                import subprocess
                result = subprocess.run(['hostname', '-I'], capture_output=True, text=True)
                if result.returncode == 0:
                    addresses = result.stdout.strip().split()
                    if addresses:
                        local_ip = addresses[0]
            return local_ip
        except Exception:
            return "localhost"

def cleanup_handler():
    """Clean up resources on exit"""
    print("\nShutting down...")
    robot.cleanup()
    camera.cleanup()

# Register cleanup handlers
atexit.register(cleanup_handler)
signal.signal(signal.SIGINT, lambda s, f: sys.exit(0))
signal.signal(signal.SIGTERM, lambda s, f: sys.exit(0))

@app.route('/')
def index():
    """Main control page"""
    return render_template('control.html')

@app.route('/video_feed')
def video_feed():
    """Video streaming route with autonomous processing"""
    def generate():
        while True:
            # Get raw frame from camera
            frame = camera.get_frame()
            
            # Process frame for autonomous features and debug visualization
            processed_frame = robot.process_autonomous_frame(frame)
            
            # Convert to JPEG for streaming
            import cv2
            ret, buffer = cv2.imencode('.jpg', processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            if ret:
                frame_bytes = buffer.tobytes()
            else:
                frame_bytes = camera.get_jpeg_frame()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    
    return Response(generate(), mimetype='multipart/x-mixed-replace; boundary=frame')

# =============================================================================
# AUTONOMOUS CONTROL ROUTES
# =============================================================================

@app.route('/start_autonomous')
def start_autonomous():
    """Start autonomous line following mode"""
    success, message = robot.start_autonomous_mode()
    global last_command
    last_command = "autonomous started" if success else "autonomous failed"
    
    return jsonify({
        'success': success,
        'message': message
    })

@app.route('/stop_autonomous')
def stop_autonomous():
    """Stop autonomous mode"""
    success, message = robot.stop_autonomous_mode()
    global last_command
    last_command = "autonomous stopped"
    
    return jsonify({
        'success': success,
        'message': message
    })

@app.route('/autonomous_status')
def get_autonomous_status():
    """Get detailed autonomous system status"""
    status = robot.get_feature_status()
    
    # Let the robot controller determine the button text
    button_text = robot.get_autonomous_button_text()
    status['button_text'] = button_text
    
    return jsonify(status)

@app.route('/console_logs')
def get_console_logs():
    """Get recent console log messages"""
    messages = console_logger.get_recent_messages(limit=50)
    return jsonify({'messages': messages})

# =============================================================================
# CAMERA CONTROL ROUTES (FIXED - Using Query Parameters)
# =============================================================================

@app.route('/set_camera_pan')
def set_camera_pan():
    """Set camera pan angle using query parameter"""
    try:
        angle = request.args.get('angle', type=int)
        if angle is None:
            return jsonify({
                'success': False,
                'message': 'Angle parameter required'
            }), 400
        
        success = robot.set_camera_pan(angle)
        return jsonify({
            'success': success,
            'angle': angle,
            'message': f'Camera pan set to {angle}¬∞' if success else 'Camera pan failed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting camera pan: {str(e)}'
        }), 500

@app.route('/set_camera_tilt')
def set_camera_tilt():
    """Set camera tilt angle using query parameter"""
    try:
        angle = request.args.get('angle', type=int)
        if angle is None:
            return jsonify({
                'success': False,
                'message': 'Angle parameter required'
            }), 400
        
        success = robot.set_camera_tilt(angle)
        return jsonify({
            'success': success,
            'angle': angle,
            'message': f'Camera tilt set to {angle}¬∞' if success else 'Camera tilt failed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting camera tilt: {str(e)}'
        }), 500

@app.route('/camera_look_down')
def camera_look_down():
    """Preset: Point camera down for line following"""
    success = robot.camera_look_down()
    return jsonify({
        'success': success,
        'message': 'Camera pointed down for line following' if success else 'Camera positioning failed'
    })

@app.route('/camera_look_forward')
def camera_look_forward():
    """Preset: Point camera forward"""
    success = robot.camera_look_forward()
    return jsonify({
        'success': success,
        'message': 'Camera pointed forward' if success else 'Camera positioning failed'
    })

# =============================================================================
# DEBUG DATA ROUTES
# =============================================================================

@app.route('/debug_data')
def get_debug_data():
    """Get clean debug data for sidebar"""
    return jsonify(robot.get_debug_data())

@app.route('/set_debug_level')
def set_debug_level():
    """Set debugging visualization level using query parameter"""
    try:
        level = request.args.get('level', type=int)
        if level is None:
            return jsonify({
                'success': False,
                'message': 'Level parameter required'
            }), 400
        
        robot.set_debug_level(level)
        return jsonify({
            'success': True,
            'debug_level': robot.debug_level,
            'message': f'Debug level set to {level}'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting debug level: {str(e)}'
        }), 500

@app.route('/set_frame_rate')
def set_frame_rate():
    """Set frame rate using query parameter"""
    try:
        fps = request.args.get('fps', type=int)
        if fps is None:
            return jsonify({
                'success': False,
                'message': 'FPS parameter required'
            }), 400
        
        robot.set_frame_rate(fps)
        return jsonify({
            'success': True,
            'frame_rate': robot.target_fps,
            'message': f'Frame rate set to {fps} fps'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting frame rate: {str(e)}'
        }), 500
    
@app.route('/set_debug_mode')
def set_debug_mode():
    """Set debug visualization mode using query parameter"""
    try:
        mode = request.args.get('mode', 'line_following')
        success = robot.set_debug_mode(mode)
        return jsonify({
            'success': success,
            'debug_mode': robot.debug_mode,
            'available_modes': robot.available_modes,
            'message': f'Debug mode set to {mode}' if success else f'Invalid debug mode: {mode}'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting debug mode: {str(e)}'
        }), 500

@app.route('/get_week2_performance')
def get_week2_performance():
    """Get Week 2 specific performance metrics"""
    try:
        return jsonify(robot.get_debug_mode_status())
    except Exception as e:
        return jsonify({
            'error': f'Error getting performance data: {str(e)}'
        }), 500

@app.route('/update_pid_parameters')
def update_pid_parameters():
    """Update PID parameters via web interface"""
    kp = request.args.get('kp', type=float)
    ki = request.args.get('ki', type=float)
    kd = request.args.get('kd', type=float)
    
    robot.update_pid_parameters(kp=kp, ki=ki, kd=kd)
    
    return jsonify({
        'success': True,
        'message': 'PID parameters updated',
        'kp': kp,
        'ki': ki, 
        'kd': kd
    })

# =============================================================================
# EXISTING MANUAL CONTROL ROUTES (unchanged)
# =============================================================================

@app.route('/move/<direction>')
def move_robot(direction):
    """Handle movement commands"""
    global last_command, command_count
    
    if robot.autonomous_mode:
        return jsonify({
            'success': False,
            'message': 'Manual control disabled during autonomous mode',
            'command_count': command_count
        })
    
    command_count += 1
    success = False
    
    try:
        if direction == 'forward':
            success = robot.move_forward(duration=0.5, speed=50)
            last_command = "forward"
            
        elif direction == 'backward':
            success = robot.move_backward(duration=0.5, speed=50)
            last_command = "backward"
            
        elif direction == 'left':
            success = robot.turn_left(duration=0.5, speed=50, angle=-30)
            last_command = "turn left"
            
        elif direction == 'right':
            success = robot.turn_right(duration=0.5, speed=50, angle=30)
            last_command = "turn right"
            
        elif direction == 'stop':
            robot.emergency_stop()
            last_command = "emergency stop"
            success = True
            
        else:
            return jsonify({
                'success': False, 
                'message': f'Unknown direction: {direction}',
                'command_count': command_count
            })
        
        return jsonify({
            'success': success,
            'direction': direction,
            'message': f'Command {direction} {"executed" if success else "failed"}',
            'command_count': command_count
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error executing {direction}: {str(e)}',
            'command_count': command_count
        })

@app.route('/status')
def get_status():
    """Get current robot status"""
    return jsonify({
        'last_command': last_command,
        'command_count': command_count,
        'is_moving': robot.movement_controller.is_moving if robot.movement_controller.is_hardware_connected() else False,
        'robot_connected': robot.movement_controller.is_hardware_connected(),
        'autonomous_mode': robot.autonomous_mode
    })

@app.route('/test')
def test_robot():
    """Test basic robot functionality"""
    if robot.picar is None:
        return jsonify({
            'success': False,
            'message': 'Robot not connected'
        })
    
    if robot.autonomous_mode:
        return jsonify({
            'success': False,
            'message': 'Cannot test during autonomous mode'
        })
    
    try:
        robot.move_forward(duration=0.2, speed=30)
        return jsonify({
            'success': True,
            'message': 'Test movement completed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Test failed: {str(e)}'
        })
    
@app.route('/speed_data')
def get_speed_data():
    """Get current speed data for speed estimation debug mode"""
    try:
        speed_data = robot.get_speed_data()
        
        # Add real-time timestamp
        import time
        speed_data['timestamp'] = time.time()
        
        return jsonify(speed_data)
    except Exception as e:
        return jsonify({
            'error': f'Speed data error: {str(e)}',
            'current_speed': 0.0,
            'smoothed_speed': 0.0,
            'speed_history': [],
            'motor_power': 0,
            'test_active': False,
            'timestamp': time.time()
        }), 500

@app.route('/speed_test_control')
def speed_test_control():
    """Control speed testing with discrete power levels and auto-stop"""
    try:
        action = request.args.get('action', 'stop')  # start, stop, emergency_stop
        speed = request.args.get('speed', type=int, default=30)
        
        if robot.autonomous_mode:
            return jsonify({
                'success': False,
                'message': 'Speed testing disabled during autonomous mode'
            })
        
        if action == 'start':
            # Validate speed levels
            if speed not in [30, 50, 70]:
                return jsonify({
                    'success': False,
                    'message': f'Invalid speed level: {speed}. Use 30, 50, or 70.'
                })
        
        success, message = robot.control_speed_test(action, speed)
        
        return jsonify({
            'success': success,
            'message': message,
            'action': action,
            'speed': speed if action == 'start' else 0,
            'duration': 3 if action == 'start' else 0
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Speed test error: {str(e)}'
        }), 500

@app.route('/process_speed_frames')
def process_speed_frames():
    """Lightweight speed estimation without video streaming"""
    try:
        # Get raw frame from camera (no encoding)
        current_frame = camera.get_frame()
        
        # Run speed estimation only (no autonomous processing)
        speed = robot.calculate_speed_only(current_frame)
        
        # Return minimal JSON (not full debug data)
        return jsonify({
            'current_speed': speed,
            'smoothed_speed': robot.get_smoothed_speed(),
            'timestamp': time.time()
        })
    except Exception as e:
        return jsonify({'current_speed': 0.0, 'smoothed_speed': 0.0, 'error': str(e)})

if __name__ == '__main__':
    try:
        camera.start_streaming()
        local_ip = get_local_ip()
        
        print("Starting Flask server...")
        print("Open http://localhost:5000 in your browser")
        print(f"Or from another device: http://{local_ip}:5000")
        print("Use camera controls to position camera, then start autonomous mode")
        
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
        
    except KeyboardInterrupt:
        print("\nShutting down due to keyboard interrupt")
    except Exception as e:
        print(f"Error starting server: {e}")
    finally:
        cleanup_handler()
```


################################################################################
# FILE: core/robot_controller.py
################################################################################

```python
#!/usr/bin/env python3

import threading
import time
import cv2
import numpy as np
import sys
import os

# Add the parent directory to Python path to ensure imports work
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.movement_controls import MovementController
from utils.debug_visualizer import DebugVisualizer
from utils.utils import TimingUtils, CacheManager, StatusManager
from utils.console_logger import console_logger

try:
    from picarx import Picarx
except ImportError:
    print("WARNING: PicarX not available - running in simulation mode")
    Picarx = None

# =============================================================================
# EXPLICIT FEATURE CONTROL
# Students enable features when they're ready
# =============================================================================
FEATURES_ENABLED = {
    'line_following': False,   # Week 1 - Enable when ready
    'sign_detection': False,  # Week 2 - Student enables when implemented  
    'speed_estimation': True # Week 3 - Student enables when implemented
}

class RobotController:
    def __init__(self):
        """Initialize the robot controller"""
        try:
            # =================================================================
            # STUDENT TUNABLE PARAMETERS - Modify these as needed
            # =================================================================
            
            # Week 2: Object Detection & Depth Analysis Performance
            self.detection_interval = 0.5    # Run object detection every 0.5 seconds
            self.depth_interval = 1.0        # Run depth analysis every 1.0 seconds
            self.sign_stop_duration = 2.0    # Seconds to stop for detected signs
            self.stop_cooldown_duration = 1.0 # Cooldown after movement resumes
            
            # Debug and visualization settings
            self.debug_level = 0             # 0-4, higher = more debug info
            self.target_fps = 10             # Target frame rate for line following
            
            # =================================================================
            # SYSTEM VARIABLES - Don't modify these directly
            # =================================================================
            
            # Initialize hardware controller
            self.movement_controller = MovementController()
            
            # Initialize utility managers
            self.timing_utils = TimingUtils()
            self.cache_manager = CacheManager()
            self.status_manager = StatusManager()
            
            # Initialize debug visualizer
            self.debug_visualizer = DebugVisualizer()
            
            # Debug mode system
            self.debug_mode = "line_following"  # Default to Week 1
            self.available_modes = ["line_following", "object_detection", "speed_estimation", "full_system"]
            
            # Autonomous mode variables
            self.autonomous_mode = False
            self.frame_counter = 0
            self.previous_frame = None
            self.current_speed = 0.0
            self.sign_stop_until = None      # Time when sign stop expires
            
            # Performance tracking and timing
            self.last_frame_time = time.time()
            self.frame_interval = 1.0 / self.target_fps

            # Speed testing state
            self._current_test_speed = 0
            
            # Feature modules (loaded based on FEATURES_ENABLED)
            self.line_follower = None
            self.sign_detector = None
            self.speed_estimator = None
            
            # Feature status tracking
            self.feature_status = {
                'line_following': 'Disabled',
                'sign_detection': 'Disabled', 
                'speed_estimation': 'Disabled'
            }
            
            # Debug data for sidebar (clean, minimal)
            self.debug_data = {
                'error_px': 0.0,
                'steering_angle': 0.0,
                'lines_detected': 0,
                'mode': 'Manual'
            }
            
            # Load enabled features
            self._load_enabled_features()
            
            print("‚úÖ Robot controller initialized successfully")
        except Exception as e:
            print(f"‚ùå Error initializing robot: {e}")
    
    def _load_enabled_features(self):
        """Load only the features that are explicitly enabled"""
        
        print("Loading enabled features...")
        
        # Week 1: Line Following
        if FEATURES_ENABLED['line_following']:
            try:
                # Clear any cached modules to force reload
                module_name = 'week1_line_following.line_follower'
                if module_name in sys.modules:
                    del sys.modules[module_name]
                
                from week1_line_following.line_follower import LineFollower
                self.line_follower = LineFollower()
                self.feature_status['line_following'] = 'Active'
                print("‚úÖ Line following enabled and loaded")
                
            except Exception as e:
                self.feature_status['line_following'] = f'Error: {str(e)}'
                print(f"‚ùå Line following error: {e}")
        else:
            self.feature_status['line_following'] = 'Disabled'
            print("Line following disabled")
            
        # Week 2: Sign Detection
        if FEATURES_ENABLED['sign_detection']:
            try:
                if 'week2_object_detection.sign_detector' in sys.modules:
                    del sys.modules['week2_object_detection.sign_detector']
                    
                from week2_object_detection.sign_detector import SignDetector
                self.sign_detector = SignDetector()
                self.feature_status['sign_detection'] = 'Active'
                print("‚úÖ Sign detection enabled and loaded")
            except Exception as e:
                self.feature_status['sign_detection'] = f'Error: {str(e)}'
                print(f"‚ùå Sign detection error: {e}")
        else:
            self.feature_status['sign_detection'] = 'Disabled'
            print("Sign detection disabled")
            
        # Week 3: Speed Estimation
        if FEATURES_ENABLED['speed_estimation']:
            try:
                if 'week3_speed_estimation.speed_estimator' in sys.modules:
                    del sys.modules['week3_speed_estimation.speed_estimator']
                    
                from week3_speed_estimation.speed_estimator import SpeedEstimator
                self.speed_estimator = SpeedEstimator()
                self.feature_status['speed_estimation'] = 'Active' 
                print("‚úÖ Speed estimation enabled and loaded")
            except Exception as e:
                self.feature_status['speed_estimation'] = f'Error: {str(e)}'
                print(f"‚ùå Speed estimation error: {e}")
        else:
            self.feature_status['speed_estimation'] = 'Disabled'
            print("Speed estimation disabled")
        
        # Print final status
        print("Feature Status Summary:")
        for feature, status in self.feature_status.items():
            print(f"   {feature}: {status}")
    
    def start_autonomous_mode(self):
        """Start autonomous mode - either line following or straight movement"""
        console_logger.info("Starting autonomous mode...")
        
        if not self.movement_controller.is_hardware_connected():
            error_msg = "Robot hardware not connected"
            print(f"‚ùå {error_msg}")
            return False, error_msg
            
        self.autonomous_mode = True
        self.frame_counter = 0
        
        # Determine mode based on line following availability
        if FEATURES_ENABLED['line_following'] and self.line_follower:
            self.debug_data['mode'] = 'Line Following'
            console_logger.info("‚úÖ Autonomous line following started")
            return True, "Line following started"
        else:
            self.debug_data['mode'] = 'Straight Movement'
            console_logger.info("‚úÖ Autonomous straight movement started")
            return True, "Straight movement started"

    def calculate_speed_only(self, current_frame):
        """Minimal speed calculation without autonomous processing"""
        if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
            speed = self.speed_estimator.estimate_speed(current_frame, self.previous_frame)
            self.previous_frame = current_frame.copy()  # Store for next calculation
            return speed
        return 0.0
    
    def stop_autonomous_mode(self):
        """Stop autonomous mode and return to manual control"""
        self.autonomous_mode = False
        self.debug_data['mode'] = 'Manual'
        self.movement_controller.emergency_stop()
        print("Autonomous mode stopped")
        return True, "Autonomous mode stopped"
    
    def process_autonomous_frame(self, frame):
        """Main processing pipeline with clean debug separation"""
        
        display_frame = frame.copy()
        
        # Initialize debug data
        self.debug_data = {
            'error_px': 0.0,
            'steering_angle': 0.0,
            'lines_detected': 0,
            'mode': 'Autonomous' if self.autonomous_mode else 'Manual'
        }
        
        # Check if currently stopped for sign detection and cooldown logic
        current_time = time.time()
        stopped_for_sign = (self.sign_stop_until is not None and current_time < self.sign_stop_until)
        in_cooldown = self.status_manager.is_in_cooldown(current_time)
        
        # Week 2: Sign Detection (with timing and caching)
        if self.sign_detector and FEATURES_ENABLED['sign_detection'] and not stopped_for_sign and not in_cooldown:
            detected_signs = self._run_detection_with_timing(frame)
            if self.sign_detector.should_stop(detected_signs, frame):
                self.sign_stop_until = current_time + self.sign_stop_duration
                self.status_manager.set_recently_stopped(True)
                stopped_for_sign = True
                self.movement_controller.stop() 
                console_logger.info("Stopping for detected sign")
        
        # Anti-infinite-stop: Reset cooldown when robot starts moving again
        if self.status_manager.recently_stopped_for_sign and not stopped_for_sign and self.autonomous_mode:
            self.status_manager.start_cooldown(current_time, self.stop_cooldown_duration)
            console_logger.info("Stop cooldown activated")
        
        # Week 3: Speed Estimation
        if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
            self.current_speed = self.speed_estimator.estimate_speed(frame, self.previous_frame)
            self.debug_data['current_speed'] = round(self.current_speed, 1)
        
        # Week 1: Line Following (skip if stopped for sign)
        if self.line_follower and FEATURES_ENABLED['line_following']:
            try:
                steering_angle = self.line_follower.compute_steering_angle(frame, debug_level=self.debug_level)
                
                debug_frame = self.line_follower.get_debug_frame()
                if debug_frame is not None:
                    display_frame = debug_frame
                
                if hasattr(self.line_follower, 'current_debug_data'):
                    self.debug_data.update(self.line_follower.current_debug_data)
                
                # Apply control only if autonomous and not stopped for sign
                if self.autonomous_mode and not stopped_for_sign:
                    if current_time - self.last_frame_time >= self.frame_interval:
                        self.last_frame_time = current_time
                        self.frame_counter += 1
                        
                        self.movement_controller.apply_autonomous_control(steering_angle)
                
            except Exception as e:
                print(f"Line following error: {e}")
                self.feature_status['line_following'] = f'Runtime Error: {str(e)}'

        # FALLBACK: Straight movement if line following not available
        elif self.autonomous_mode and not stopped_for_sign:
            if current_time - self.last_frame_time >= self.frame_interval:
                self.last_frame_time = current_time
                self.frame_counter += 1
                
                # Just go straight (steering angle = 0)
                self.movement_controller.apply_autonomous_control(0)
        
        # Route debug visualization based on mode
        if self.debug_mode == "object_detection":
            display_frame = self.debug_visualizer.create_week2_debug_frame(
                display_frame, self.cache_manager, self.timing_utils, self.status_manager, self.sign_detector
            )
        elif self.debug_mode == "speed_estimation":
            # Get complete speed data for visualization
            speed_data = self.get_speed_data()
            display_frame = self.debug_visualizer.create_speed_estimation_debug_frame(
                display_frame, self.current_speed, speed_data
            )
        # Default: line_following mode uses existing debug frame
        
        # Store frame for next speed estimation
        self.previous_frame = frame.copy()
        
        return display_frame
    
    def _run_detection_with_timing(self, frame):
        """Run object detection with timing and caching (single inference execution)"""
        return self.timing_utils.run_detection_with_timing(
            frame, self.sign_detector, self.detection_interval, 
            self.depth_interval, self.cache_manager
        )
    
    # =============================================================================
    # DEBUG MODE CONTROL
    # =============================================================================
    
    def set_debug_mode(self, mode):
        """Switch debug visualization mode"""
        if mode in self.available_modes:
            self.debug_mode = mode
            print(f"Debug mode set to: {mode}")
            return True
        else:
            print(f"‚ùå Invalid debug mode: {mode}. Available: {self.available_modes}")
            return False
    
    def get_debug_mode_status(self):
        """Get current debug mode and performance metrics"""
        current_time = time.time()
        return {
            'debug_mode': self.debug_mode,
            'available_modes': self.available_modes,
            'detection_fps': 1.0 / self.detection_interval if self.detection_interval > 0 else 0,
            'depth_fps': 1.0 / self.depth_interval if self.depth_interval > 0 else 0,
            'last_detection_age': current_time - self.timing_utils.last_detection_time,
            'last_depth_age': current_time - self.timing_utils.last_depth_time,
            'cached_detections': len(self.cache_manager.cached_detections),
            'last_detection_inference_ms': self.timing_utils.last_detection_inference_ms,
            'last_depth_inference_ms': self.timing_utils.last_depth_inference_ms
        }
    
    # =============================================================================
    # HARDWARE CONTROL DELEGATION
    # =============================================================================
    
    def set_camera_pan(self, angle):
        """Set camera pan angle (-90 to +90 degrees)"""
        return self.movement_controller.set_camera_pan(angle)
    
    def set_camera_tilt(self, angle):
        """Set camera tilt angle (-90 to +90 degrees)"""
        return self.movement_controller.set_camera_tilt(angle)
    
    def camera_look_down(self):
        """Preset: Point camera down for line following"""
        return self.movement_controller.camera_look_down()
    
    def camera_look_forward(self):
        """Preset: Point camera forward for obstacle detection"""
        return self.movement_controller.camera_look_forward()
    
    def move_forward(self, duration=0.5, speed=50):
        """Move robot forward for specified duration"""
        return self.movement_controller.move_forward(duration, speed, self.autonomous_mode)
    
    def move_backward(self, duration=0.5, speed=50):
        """Move robot backward for specified duration"""
        return self.movement_controller.move_backward(duration, speed, self.autonomous_mode)
    
    def turn_left(self, duration=0.5, speed=50, angle=-30):
        """Turn robot left while moving forward"""
        return self.movement_controller.turn_left(duration, speed, angle, self.autonomous_mode)
    
    def turn_right(self, duration=0.5, speed=50, angle=30):
        """Turn robot right while moving forward"""
        return self.movement_controller.turn_right(duration, speed, angle, self.autonomous_mode)
    
    def emergency_stop(self):
        """Immediately stop the robot"""
        self.autonomous_mode = False
        self.movement_controller.emergency_stop()
    
    def cleanup(self):
        """Clean shutdown of robot"""
        self.emergency_stop()
        self.movement_controller.cleanup()
    
    # =============================================================================
    # DEBUG AND CONFIGURATION METHODS
    # =============================================================================
    
    def set_debug_level(self, level):
        """Set debugging visualization level (0-4)"""
        self.debug_level = max(0, min(4, level))
        print(f"Debug level set to: {self.debug_level}")
    
    def set_frame_rate(self, fps):
        """Set target frame rate"""
        self.target_fps = max(1, min(15, fps))
        self.frame_interval = 1.0 / self.target_fps
        print(f"Frame rate set to: {self.target_fps} fps")
    
    def update_pid_parameters(self, kp=None, ki=None, kd=None):
        """Update PID parameters during runtime"""
        if self.line_follower and hasattr(self.line_follower, 'update_parameters'):
            self.line_follower.update_parameters(kp=kp, ki=ki, kd=kd)
            print(f"PID parameters updated: Kp={kp}, Ki={ki}, Kd={kd}")
        else:
            print("WARNING: Cannot update PID parameters - line follower not available")
    
    def get_debug_data(self):
        """Get clean debug data for sidebar"""
        data = self.debug_data.copy()
        
        # Add Week 2 specific data when in object detection mode
        if self.debug_mode == "object_detection":
            data.update({
                'detections_count': len(self.cache_manager.cached_detections),
                'detection_inference_ms': self.timing_utils.last_detection_inference_ms,
                'depth_inference_ms': self.timing_utils.last_depth_inference_ms,
                'stop_status': self.status_manager.get_stop_status(time.time(), self.sign_stop_until)
            })
        
        return data
    
    def get_speed_data(self):
        """Get current speed data for speed estimation debug mode"""
        speed_data = {
            'current_speed': 0.0,
            'smoothed_speed': 0.0,
            'speed_history': [],
            'motor_power': 0,
            'test_active': False,
            'speed_thresholds': {
                'fast': 0.4,
                'medium': 0.15,
                'slow': 0.05
            }
        }
        
        try:
            if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
                # Get speed history from speed estimator
                if hasattr(self.speed_estimator, 'get_speed_history'):
                    history_data = self.speed_estimator.get_speed_history()
                    speed_data.update(history_data)
                
                # Add current speed
                speed_data['current_speed'] = round(self.current_speed, 3)
            
            # Add motor status from movement controller
            if hasattr(self, '_current_test_speed'):
                speed_data['motor_power'] = self._current_test_speed
                speed_data['test_active'] = self._current_test_speed > 0
            else:
                # Check if robot is moving manually
                if self.movement_controller.is_hardware_connected():
                    speed_data['test_active'] = self.movement_controller.is_moving
            
        except Exception as e:
            print(f"Speed data error: {e}")
        
        return speed_data
    
    def control_speed_test(self, action, speed_percent):
        """Control speed testing movements"""
        if not self.movement_controller.is_hardware_connected():
            return False, "Robot hardware not connected"
        
        if self.autonomous_mode:
            return False, "Cannot run speed test during autonomous mode"
        
        try:
            if action == 'start':
                # Start movement at specified speed
                self.movement_controller.picar.set_dir_servo_angle(0)  # Straight
                self.movement_controller.picar.forward(speed_percent)
                self._current_test_speed = speed_percent
                
                # Auto-stop after 3 seconds
                import threading
                def auto_stop():
                    try:
                        self.movement_controller.picar.stop()
                        self._current_test_speed = 0
                    except:
                        pass
                
                timer = threading.Timer(3.0, auto_stop)
                timer.start()
                self._speed_test_timer = timer
                
                return True, f"Speed test started at {speed_percent}% for 3 seconds"
                
            elif action == 'stop':
                # Manual stop
                self.movement_controller.picar.stop()
                self._current_test_speed = 0
                
                if hasattr(self, '_speed_test_timer'):
                    self._speed_test_timer.cancel()
                
                return True, "Speed test stopped"
                
        except Exception as e:
            return False, f"Speed test error: {str(e)}"
        
        return False, "Unknown action"
    
    def get_feature_status(self):
        """Return current status of all features"""
        return {
            'autonomous_mode': self.autonomous_mode,
            'features': self.feature_status.copy(),
            'camera_position': self.movement_controller.get_camera_position(),
            'target_fps': self.target_fps,
            'debug_level': self.debug_level,
            'debug_mode': self.debug_mode
        }
    
    def get_autonomous_button_text(self):
        """Get the appropriate button text for autonomous mode"""
        if FEATURES_ENABLED['line_following'] and self.line_follower:
            return "Start Line Following"
        else:
            return "Start Straight Movement"

# Global robot instance
robot = RobotController()
```


################################################################################
# FILE: core/run.py
################################################################################

```python
#!/usr/bin/env python3

"""
Simple script to run the PiCar-X control application
"""

import sys
import subprocess
import time
import socket

def get_local_ip():
    """Get the local IP address of this machine"""
    try:
        # Connect to a remote address (doesn't actually send data)
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
        return local_ip
    except Exception:
        try:
            # Fallback method
            hostname = socket.gethostname()
            local_ip = socket.gethostbyname(hostname)
            if local_ip.startswith("127."):
                # If we get localhost, try getting all addresses
                import subprocess
                result = subprocess.run(['hostname', '-I'], capture_output=True, text=True)
                if result.returncode == 0:
                    addresses = result.stdout.strip().split()
                    if addresses:
                        local_ip = addresses[0]
            return local_ip
        except Exception:
            return "localhost"

def check_requirements():
    """Check if required modules are available"""
    required_modules = ['flask', 'cv2', 'picarx']  # Removed vilib
    missing_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
            print(f"‚úì {module} - OK")
        except ImportError:
            print(f"‚úó {module} - MISSING")
            missing_modules.append(module)
    
    if missing_modules:
        print(f"\nMissing modules: {', '.join(missing_modules)}")
        print("Please install missing modules before running.")
        return False
    
    return True

def main():
    print("=== PiCar-X Control Application ===\n")
    
    print("Checking requirements...")
    if not check_requirements():
        sys.exit(1)
    
    print("\nStarting application...")
    print("Press Ctrl+C to stop the server")
    print("-" * 40)
    
    try:
        # Run the Flask application
        from flask_control import app, camera
        
        # Start camera
        print("üì∑ Initializing camera (may take a few seconds)...")
        camera.start_streaming()
        time.sleep(3)  # Give camera more time to start
        
        # Get the actual IP address
        local_ip = get_local_ip()
        
        print("\nüöÄ Server starting...")
        print("üì∑ Camera streaming with libcamera-vid")
        print("üåê Web interface available at:")
        print("   - Local: http://localhost:5000")
        print(f"   - Network: http://{local_ip}:5000")
        print("\nüéÆ Controls:")
        print("   - Use WASD keys or web buttons")
        print("   - Each movement lasts 0.5 seconds")
        print("   - Red button for emergency stop")
        print("   - Status updates appear in log below video")
        print("\n" + "="*50)
        
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
        
    except KeyboardInterrupt:
        print("\n\nüëã Shutting down gracefully...")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


################################################################################
# FILE: core/utils/console_logger.py
################################################################################

```python
#!/usr/bin/env python3

import time
from datetime import datetime
from collections import deque
import threading

class ConsoleLogger:
    """
    Simple console logger for robot operation messages
    Stores messages in memory for web console display
    """
    
    def __init__(self, max_messages=100):
        """Initialize logger with message history limit"""
        self.messages = deque(maxlen=max_messages)
        self.lock = threading.Lock()
        
    def _add_message(self, level, message):
        """Add a timestamped message to the log"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = {
            'timestamp': timestamp,
            'level': level,
            'message': message
        }
        
        with self.lock:
            self.messages.append(log_entry)
        
        # Also print to terminal for debugging
        print(f"[{timestamp}] {message}")
    
    def info(self, message):
        """Log an info message"""
        self._add_message('INFO', message)
    
    def warning(self, message):
        """Log a warning message"""
        self._add_message('WARN', message)
    
    def error(self, message):
        """Log an error message"""
        self._add_message('ERROR', message)
    
    def stop(self, message):
        """Log a stop event (special category)"""
        self._add_message('STOP', message)
    
    def get_recent_messages(self, limit=50):
        """Get recent messages for web console"""
        with self.lock:
            # Return most recent messages (up to limit)
            recent = list(self.messages)[-limit:]
            return recent
    
    def clear(self):
        """Clear all messages"""
        with self.lock:
            self.messages.clear()

# Global console logger instance
console_logger = ConsoleLogger()
```


################################################################################
# FILE: core/utils/debug_visualizer.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import time

class DebugVisualizer:
    """Handles all debug visualization and overlay creation"""
    
    def __init__(self):
        """Initialize the debug visualizer"""
        pass
    
    def create_week2_debug_frame(self, original_frame, cache_manager, timing_utils, status_manager, sign_detector):
        """Create dual-panel visualization for Week 2 object detection and depth"""
        height, width = original_frame.shape[:2]
        
        # Left panel: Object detection overlay
        left_panel = original_frame.copy()
        if cache_manager.cached_detections:
            left_panel = self._draw_detection_overlay(left_panel, cache_manager.cached_detections)
        
        # Add detection panel label
        cv2.putText(left_panel, "Object Detection", (10, 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Right panel: Depth analysis
        right_panel = original_frame.copy()
        
        # Get cached depth map from SignDetector
        cached_depth_map = sign_detector.get_cached_depth_map() if sign_detector else None
        
        if cached_depth_map is not None:
            # Apply depth colormap
            depth_normalized = cv2.normalize(cached_depth_map, None, 0, 255, cv2.NORM_MINMAX)
            depth_uint8 = depth_normalized.astype(np.uint8)
            depth_colored = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_PLASMA)
            right_panel = cv2.addWeighted(right_panel, 0.6, depth_colored, 0.4, 0)
            
            # Show frame counter alignment
            cv2.putText(right_panel, "[0]", (width - 40, height - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        else:
            # Show waiting status
            cv2.putText(right_panel, "Depth: Enable advanced mode", (10, height//2), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # Add depth panel label
        cv2.putText(right_panel, "Depth Analysis", (10, 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Combine panels side by side
        combined = np.hstack([left_panel, right_panel])
        
        # Add comprehensive debug information
        return self._add_week2_debug_overlay(combined, cache_manager, timing_utils, status_manager)
    
    def create_week3_debug_frame(self, original_frame, current_speed):
        """Simple placeholder for Week 3 - actual speed estimation uses create_speed_estimation_debug_frame"""
        speed_text = f"Speed: {current_speed:.1f} units/s"
        cv2.putText(original_frame, speed_text, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        cv2.putText(original_frame, "Speed Estimation Mode (Week 3)", (10, 60), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return original_frame

    def create_speed_estimation_debug_frame(self, original_frame, current_speed, speed_data):
        """Create comprehensive debug frame for Week 3 speed estimation mode"""
        debug_frame = original_frame.copy()
        height, width = debug_frame.shape[:2]
        
        # Get speed data
        smoothed_speed = speed_data.get('smoothed_speed', current_speed)
        motor_power = speed_data.get('motor_power', 0)
        test_active = speed_data.get('test_active', False)
        
        # Color coding based on speed thresholds
        if smoothed_speed > 0.4:
            color = (0, 0, 255)  # Red - fast
            status = "FAST"
        elif smoothed_speed > 0.15:
            color = (0, 165, 255)  # Orange - medium  
        elif smoothed_speed > 0.05:
            color = (0, 255, 0)  # Green - slow
            status = "SLOW"
        else:
            color = (128, 128, 128)  # Gray - stopped
            status = "STOPPED"
        
        # Large speed display in center (48px equivalent)
        speed_text = f"{smoothed_speed:.2f} m/s"
        font_scale = 2.0
        thickness = 3
        text_size = cv2.getTextSize(speed_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
        text_x = (width - text_size[0]) // 2
        text_y = height // 2
        
        # Background rectangle for speed text
        padding = 20
        cv2.rectangle(debug_frame, 
                    (text_x - padding, text_y - text_size[1] - padding),
                    (text_x + text_size[0] + padding, text_y + padding),
                    (0, 0, 0), -1)
        
        # Speed text
        cv2.putText(debug_frame, speed_text, (text_x, text_y), 
                cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)
        
        # Status text below speed
        status_y = text_y + 50
        cv2.putText(debug_frame, status, (text_x + 20, status_y), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
        
        # Motor power display (top left)
        if test_active and motor_power > 0:
            power_text = f"Motor: {motor_power}% (TEST ACTIVE)"
            cv2.putText(debug_frame, power_text, (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # Speed history indicator (bottom)
        history = speed_data.get('speed_history', [])
        if len(history) > 1:
            avg_speed = sum(history[-5:]) / min(5, len(history))
            cv2.putText(debug_frame, f"Avg (5 frames): {avg_speed:.2f} m/s", 
                    (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # Current values (top right)
        cv2.putText(debug_frame, f"Raw: {current_speed:.3f}", (width - 150, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Smooth: {smoothed_speed:.3f}", (width - 150, 50), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return debug_frame
    
    def create_speed_estimation_debug_frame(self, original_frame, current_speed, speed_data):
        """Create debug frame for Week 3 speed estimation mode"""
        debug_frame = original_frame.copy()
        height, width = debug_frame.shape[:2]
        
        # Add speed information overlay
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Large speed display in center
        speed_text = f"Speed: {current_speed:.2f} m/s"
        font_scale = 1.2
        text_size = cv2.getTextSize(speed_text, font, font_scale, 2)[0]
        text_x = (width - text_size[0]) // 2
        text_y = height // 2
        
        # Background rectangle for speed text
        cv2.rectangle(debug_frame, 
                     (text_x - 10, text_y - text_size[1] - 10),
                     (text_x + text_size[0] + 10, text_y + 10),
                     (0, 0, 0), -1)
        
        # Speed text
        color = (0, 255, 0) if current_speed > 0.1 else (128, 128, 128)
        cv2.putText(debug_frame, speed_text, (text_x, text_y), 
                   font, font_scale, color, 2)
        
        # Status information
        status_y = 30
        
        # Calibration status
        cal_status = "Calibrated" if speed_data.get('calibrated', False) else "Not Calibrated"
        cal_color = (0, 255, 0) if speed_data.get('calibrated', False) else (0, 0, 255)
        cv2.putText(debug_frame, f"Status: {cal_status}", (10, status_y), 
                   font, 0.6, cal_color, 2)
        
        # Motor power
        motor_power = speed_data.get('motor_power', 0)
        if motor_power > 0:
            cv2.putText(debug_frame, f"Motor: {motor_power}%", (10, status_y + 25), 
                       font, 0.6, (255, 255, 0), 2)
        
        # Speed history indicator
        history = speed_data.get('speed_history', [])
        if len(history) > 1:
            cv2.putText(debug_frame, f"Avg: {np.mean(history[-5:]):.2f} m/s", 
                       (10, height - 20), font, 0.5, (255, 255, 255), 1)
        
        return debug_frame
    
    def _draw_detection_overlay(self, frame, detections):
        """Draw bounding boxes and labels for detected objects"""
        for detection in detections:
            bbox = detection['bbox']
            x, y, w, h = bbox
            confidence = detection['confidence']
            class_name = detection.get('class_name', detection.get('class', 'object'))
            
            # Color coding for different object types
            if 'stop' in class_name.lower():
                color = (0, 0, 255)  # Red for stop signs
            else:
                color = (0, 255, 0)  # Green for other objects
            
            # Draw bounding box
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # Draw label with confidence
            label = f"{class_name}: {confidence:.2f}"
            font_scale = 0.5
            thickness = 1
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
            
            # Background for label
            cv2.rectangle(frame, (x, y - label_size[1] - 5), 
                         (x + label_size[0], y), color, -1)
            
            # Label text
            cv2.putText(frame, label, (x, y - 3), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        return frame
    
    def _add_week2_debug_overlay(self, combined_frame, cache_manager, timing_utils, status_manager):
        """Add comprehensive Week 2 debug information"""
        height, width = combined_frame.shape[:2]
        
        # Performance metrics
        detection_interval = getattr(timing_utils, 'detection_interval', 0.5)
        depth_interval = getattr(timing_utils, 'depth_interval', 1.0)
        
        detection_fps = 1.0 / detection_interval if detection_interval > 0 else 0
        depth_fps = 1.0 / depth_interval if depth_interval > 0 else 0
        
        current_time = time.time()
        
        # Get status from status manager
        stop_status = status_manager.get_current_status(current_time)
        
        # Top status line
        status_text = f"Status: {stop_status} | Detection: {detection_fps:.1f}fps ({timing_utils.last_detection_inference_ms:.0f}ms) | Depth: {depth_fps:.1f}fps ({timing_utils.last_depth_inference_ms:.0f}ms)"
        cv2.putText(combined_frame, status_text, (10, height - 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Detection list
        detection_text = f"Detections: {len(cache_manager.cached_detections)}"
        if cache_manager.cached_detections:
            top_detections = cache_manager.cached_detections[:3]  # Show top 3
            detection_names = [f"{d.get('class_name', d.get('class', 'obj'))}({d['confidence']:.2f})" 
                             for d in top_detections]
            detection_text += f" - {', '.join(detection_names)}"
        
        cv2.putText(combined_frame, detection_text, (10, height - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return combined_frame
```


################################################################################
# FILE: core/utils/movement_controls.py
################################################################################

```python
#!/usr/bin/env python3

import threading
import time
from .console_logger import console_logger


try:
    from picarx import Picarx
except ImportError:
    print("WARNING: PicarX not available - running in simulation mode")
    Picarx = None

class MovementController:
    """Handles all hardware movement and camera control operations"""
    
    def __init__(self):
        """Initialize the movement controller with hardware"""
        try:
            if Picarx:
                self.picar = Picarx()
                print("‚úÖ PiCar-X hardware connected")
            else:
                self.picar = None
                print("WARNING: Running without PiCar-X hardware")
                
            self.is_moving = False
            
            # Camera positioning
            self.camera_pan_angle = 0   # -90 to +90 degrees
            self.camera_tilt_angle = -30  # Start looking down for line following
            
            # Initialize camera position
            self._set_camera_position()
            
        except Exception as e:
            print(f"‚ùå Error initializing movement hardware: {e}")
            self.picar = None
    
    def _set_camera_position(self):
        """Set camera to initial position"""
        if self.picar:
            try:
                self.picar.set_cam_pan_angle(self.camera_pan_angle)
                self.picar.set_cam_tilt_angle(self.camera_tilt_angle)
                print(f"Camera positioned: pan={self.camera_pan_angle}¬∞, tilt={self.camera_tilt_angle}¬∞")
            except Exception as e:
                print(f"WARNING: Camera positioning error: {e}")
    
    def is_hardware_connected(self):
        """Check if hardware is connected and available"""
        return self.picar is not None
    
    def get_camera_position(self):
        """Get current camera position"""
        return {
            'pan': self.camera_pan_angle,
            'tilt': self.camera_tilt_angle
        }
    
    # =============================================================================
    # CAMERA CONTROL METHODS
    # =============================================================================
    
    def set_camera_pan(self, angle):
        """Set camera pan angle (-90 to +90 degrees)"""
        angle = max(-90, min(90, angle))
        self.camera_pan_angle = angle
        
        if self.picar:
            try:
                self.picar.set_cam_pan_angle(angle)
                print(f"Camera pan set to {angle}¬∞")
                return True
            except Exception as e:
                print(f"‚ùå Camera pan error: {e}")
                return False
        return False
    
    def set_camera_tilt(self, angle):
        """Set camera tilt angle (-90 to +90 degrees)"""
        angle = max(-90, min(90, angle))
        self.camera_tilt_angle = angle
        
        if self.picar:
            try:
                self.picar.set_cam_tilt_angle(angle)
                print(f"Camera tilt set to {angle}¬∞")
                return True
            except Exception as e:
                print(f"‚ùå Camera tilt error: {e}")
                return False
        return False
    
    def camera_look_down(self):
        """Preset: Point camera down for line following"""
        return self.set_camera_pan(0) and self.set_camera_tilt(-30)
    
    def camera_look_forward(self):
        """Preset: Point camera forward for obstacle detection"""
        return self.set_camera_pan(0) and self.set_camera_tilt(0)
    
    # =============================================================================
    # AUTONOMOUS MOVEMENT CONTROL
    # =============================================================================
    
    def apply_autonomous_control(self, steering_angle):
        """Apply steering and forward movement for autonomous mode"""
        if self.picar and not self.is_moving:
            try:
                self.picar.set_dir_servo_angle(steering_angle)
                self.picar.forward(1)  # 1% speed
            except Exception as e:
                print(f"Autonomous control error: {e}")
    
    # =============================================================================
    # MANUAL MOVEMENT METHODS
    # =============================================================================
    
    def _auto_stop(self):
        """Automatically stop the robot and center wheels after movement"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)
            self.is_moving = False

    def stop(self):
        """Stop the robot (gentler than emergency_stop)"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)  # Center the wheels
            self.is_moving = False
        console_logger.info("Robot stopped")
    
    def move_forward(self, duration=0.5, speed=50, autonomous_mode=False):
        """Move robot forward for specified duration"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(0)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error moving forward: {e}")
            self._auto_stop()
            return False
    
    def move_backward(self, duration=0.5, speed=50, autonomous_mode=False):
        """Move robot backward for specified duration"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(0)
            self.picar.backward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error moving backward: {e}")
            self._auto_stop()
            return False
    
    def turn_left(self, duration=0.5, speed=50, angle=-30, autonomous_mode=False):
        """Turn robot left while moving forward"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(angle)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error turning left: {e}")
            self._auto_stop()
            return False
    
    def turn_right(self, duration=0.5, speed=50, angle=30, autonomous_mode=False):
        """Turn robot right while moving forward"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(angle)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error turning right: {e}")
            self._auto_stop()
            return False
    
    def emergency_stop(self):
        """Immediately stop the robot"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)
            self.is_moving = False
        print("Emergency stop activated")
    
    def cleanup(self):
        """Clean shutdown of movement controller"""
        self.emergency_stop()
        print("Movement controller cleaned up")
```


################################################################################
# FILE: core/utils/utils.py
################################################################################

```python
#!/usr/bin/env python3

import time
import cv2
import numpy as np

class TimingUtils:
    """Handles timing and performance tracking for inference operations"""
    
    def __init__(self):
        self.last_detection_time = 0
        self.last_depth_time = 0
        self.last_detection_inference_ms = 0
        self.last_depth_inference_ms = 0
        self.detection_interval = 0.5
        self.depth_interval = 1.0
    
    def run_detection_with_timing(self, frame, sign_detector, detection_interval, depth_interval, cache_manager):
        """Run object detection with timing and caching (single inference execution)"""
        current_time = time.time()
        
        # Store intervals for later use
        self.detection_interval = detection_interval
        self.depth_interval = depth_interval
        
        # Only run detection if enough time has passed
        if current_time - self.last_detection_time >= detection_interval:
            start_time = time.perf_counter()
            detected_signs = sign_detector.detect_signs(frame)
            self.last_detection_inference_ms = (time.perf_counter() - start_time) * 1000
            
            # Cache results for debugging
            cache_manager.update_detections(detected_signs)
            self.last_detection_time = current_time
            
            # Run depth analysis if we have detections and it's time
            if detected_signs and self._should_run_depth(current_time):
                self._run_depth_with_timing(frame, detected_signs, cache_manager)
            
            return detected_signs
        else:
            # Return cached results - no additional inference
            return cache_manager.cached_detections
    
    def _should_run_depth(self, current_time):
        """Check if it's time to run depth analysis"""
        return current_time - self.last_depth_time >= self.depth_interval
    
    def _run_depth_with_timing(self, frame, detections, cache_manager):
        """Run depth estimation with timing (placeholder for MiDaS integration)"""
        start_time = time.perf_counter()
        
        # Placeholder: Simple area-based "depth" for now
        # Students will replace this with actual MiDaS integration
        depth_map = self._simple_area_based_depth(frame, detections)
        
        self.last_depth_inference_ms = (time.perf_counter() - start_time) * 1000
        cache_manager.update_depth(depth_map)
        self.last_depth_time = time.time()
    
    def _simple_area_based_depth(self, frame, detections):
        """Simple area-based depth estimation (students will replace with MiDaS)"""
        # Create a simple depth visualization based on bounding box areas
        height, width = frame.shape[:2]
        depth_map = np.zeros((height, width), dtype=np.uint8)
        
        for detection in detections:
            bbox = detection['bbox']
            x, y, w, h = bbox
            area = w * h
            
            # Larger bounding box = closer = higher depth value
            depth_value = min(255, int(area / 100))  # Simple area-to-depth conversion
            cv2.rectangle(depth_map, (x, y), (x + w, y + h), depth_value, -1)
        
        return depth_map

class CacheManager:
    """Manages cached results for debugging and visualization"""
    
    def __init__(self):
        self.cached_detections = []
        self.cached_depth_map = None
        self.detection_frame_counter = 0
        self.depth_frame_counter = 0
    
    def update_detections(self, detections):
        """Update cached detection results"""
        self.cached_detections = detections
        self.detection_frame_counter += 1
    
    def update_depth(self, depth_map):
        """Update cached depth results"""
        self.cached_depth_map = depth_map
        self.depth_frame_counter = self.detection_frame_counter

class StatusManager:
    """Manages robot status and anti-infinite-stop logic"""
    
    def __init__(self):
        self.recently_stopped_for_sign = False
        self.stop_cooldown_until = None
    
    def set_recently_stopped(self, value):
        """Set the recently stopped flag"""
        self.recently_stopped_for_sign = value
    
    def start_cooldown(self, current_time, cooldown_duration):
        """Start the stop cooldown period"""
        self.stop_cooldown_until = current_time + cooldown_duration
        self.recently_stopped_for_sign = False
    
    def is_in_cooldown(self, current_time):
        """Check if currently in stop cooldown period"""
        return (self.stop_cooldown_until is not None and 
                current_time < self.stop_cooldown_until)
    
    def get_stop_status(self, current_time, sign_stop_until=None):
        """Get current stopping status"""
        if sign_stop_until is not None and current_time < sign_stop_until:
            return "STOPPED"
        elif self.is_in_cooldown(current_time):
            return "COOLDOWN"
        else:
            return "ACTIVE"
    
    def get_current_status(self, current_time):
        """Get current status for debug display"""
        if self.is_in_cooldown(current_time):
            return "COOLDOWN"
        else:
            return "ACTIVE"
```


################################################################################
# FILE: feature_config.py
################################################################################

```python
#!/usr/bin/env python3

"""
Feature Control Configuration
Students enable features as they complete each week's implementation
"""

# =============================================================================
# STUDENT FEATURE CONTROL
# Set to True when you've completed the implementation for each week
# =============================================================================

FEATURES_ENABLED = {
    'line_following': True,    # Week 1: Set to True when line following is ready
    'sign_detection': False,   # Week 2: Set to True when sign detection is ready  
    'speed_estimation': False  # Week 3: Set to True when speed estimation is ready
}

# =============================================================================
# FEATURE DESCRIPTIONS (for reference)
# =============================================================================

FEATURE_DESCRIPTIONS = {
    'line_following': 'Computer vision + PID control for following lines',
    'sign_detection': 'ONNX model integration for detecting stop signs',
    'speed_estimation': 'Optical flow analysis for estimating robot speed'
}

# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Minimum methods required for each feature to be considered "implemented"
REQUIRED_METHODS = {
    'line_following': ['compute_steering_angle'],
    'sign_detection': ['detect_signs', 'should_stop'],
    'speed_estimation': ['estimate_speed']
}

def is_feature_enabled(feature_name):
    """Check if a feature is enabled"""
    return FEATURES_ENABLED.get(feature_name, False)

def get_enabled_features():
    """Get list of currently enabled features"""
    return [name for name, enabled in FEATURES_ENABLED.items() if enabled]

def get_feature_description(feature_name):
    """Get description of a feature"""
    return FEATURE_DESCRIPTIONS.get(feature_name, "No description available")
```


################################################################################
# FILE: teaching_tools/6panel_pid_graph.py
################################################################################

```python
#!/usr/bin/env python3
"""
Extended PID Controller Visualization ‚Äì Complete Curve Navigation Cycle
----------------------------------------------------------------------
This script generates a six‚Äëpanel figure that explains how P,‚ÄØI,‚ÄØD and the
combined PID signal evolve as a small robot negotiates a right‚Äëhand curve.
Changes compared with the original draft:

* **Derivative term is now calculated per definition**  D = Kd * (de/dt).
  ‚Äì Key‚Äëframe derivative uses the video‚Äëframe interval (0.1‚ÄØs).
  ‚Äì Dense derivative uses numpy.gradient and is optionally smoothed by a
    zero‚Äëlag Savitzky‚ÄìGolay filter (scipy).  If SciPy is absent the code
    runs without smoothing.
* Removed the manual derivative hack and its artefacts (flat purple walls).
* Axis limits auto‚Äëscale except where hard limits aid interpretation.
* Internal constants grouped near the top for quick edits.
* Prints a compact verification table that you can paste into LaTeX.

Written for Python ‚â•‚ÄØ3.8.
"""

import os
from pathlib import Path
from typing import Tuple

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon

# ---------------------------------------------------------------------------
# User‚Äëtunable parameters
# ---------------------------------------------------------------------------
Kp: float = 0.8       # degrees / pixel
Ki: float = 0.1       # degrees / pixel¬∑s
Kd: float = 0.3       # degrees / pixel¬∑s

frame_rate: float = 10.0          # camera FPS   ‚Üí Œît = 0.1 s
wheelbase: float  = 0.20          # m (robot geometry)
speed: float      = 1.0           # m/s (assumed constant)

# Smoothing (set to 0 to disable)
SG_WINDOW: int = 9     # must be odd and ‚â• polyorder + 2
SG_POLY: int   = 2

# Output
FIG_NAME = "2extended_pid_visualization.png"
FIG_DPI  = 300

# ---------------------------------------------------------------------------
# Key‚Äëframe scenario (6 frames: N‚Äë2 ‚Ä¶ N+3)
# ---------------------------------------------------------------------------
key_times  = np.arange(0.0, 0.6, 0.1)                # [0.0 ‚Ä¶ 0.5] s
key_errors = np.array([5.0, 15.0, 25.0, 15.0, 5.0, 0.0])  # px
frame_dt   = 1.0 / frame_rate                        # 0.1 s

# ---------------------------------------------------------------------------
# Helper: create smooth error profile
# ---------------------------------------------------------------------------

def make_error_profile(times: np.ndarray,
                        key_t: np.ndarray,
                        key_e: np.ndarray) -> np.ndarray:
    """Piece‚Äëwise linear interpolation through key points."""
    return np.interp(times, key_t, key_e)


# ---------------------------------------------------------------------------
# Dense timeline (for nice curves)
# ---------------------------------------------------------------------------
extended_times  = np.linspace(-0.05, 0.65, 150)        # s
extended_errors = make_error_profile(extended_times, key_times, key_errors)

dt_dense = np.gradient(extended_times)                 # variable step (‚âà0.0047 s)

# ---------------------------------------------------------------------------
# P term (straightforward)
# ---------------------------------------------------------------------------
P_terms      = Kp * extended_errors
key_P_terms  = Kp * key_errors

# ---------------------------------------------------------------------------
# I term (rectangular integration, causal)
# ---------------------------------------------------------------------------
# I_terms = np.zeros_like(extended_errors)
# int_sum = 0.0
# for i in range(1, len(extended_errors)):
#     int_sum += extended_errors[i] * dt_dense[i]
#     I_terms[i] = Ki * int_sum
# key_I_terms = I_terms[np.searchsorted(extended_times, key_times)]
I_terms = np.zeros_like(extended_errors)
int_sum = 0.0
for i in range(1, len(extended_errors)):
    int_sum += extended_errors[i] * dt_dense[i]
    I_terms[i] = Ki * int_sum

# Key frame integral (cumulative sum of discrete errors)
key_integral_sums = np.cumsum(key_errors)  # [5, 20, 45, 60, 65, 65]
key_I_terms = Ki * key_integral_sums       # Ki * cumulative sum

# ---------------------------------------------------------------------------
# D term ‚Äì‚Äì correct formulation
# ---------------------------------------------------------------------------
# 1) Key‚Äëframe derivative -----------------------------------------------------
prev_error = 3.0  # px, assumed error one frame *before* N‚Äë2
error_series = np.insert(key_errors, 0, prev_error)
error_changes = np.diff(error_series) / frame_dt      # px / s
key_D_terms   = Kd * error_changes                    # deg

# 2) Dense derivative ---------------------------------------------------------
D_terms = Kd * np.gradient(extended_errors, extended_times)

# Optional Savitzky‚ÄìGolay smoothing (keeps zero phase, causal enough here)
try:
    if SG_WINDOW > 2:
        from scipy.signal import savgol_filter
        D_terms = savgol_filter(D_terms, window_length=SG_WINDOW, polyorder=SG_POLY)
except ModuleNotFoundError:
    pass  # SciPy not installed ‚Äì carry on without smoothing

# ---------------------------------------------------------------------------
# Combined control signal
# ---------------------------------------------------------------------------
I_terms_interpolated = np.interp(extended_times, key_times, key_I_terms)
Total_terms = P_terms + I_terms_interpolated + D_terms
key_totals  = key_P_terms + key_I_terms + key_D_terms

# ---------------------------------------------------------------------------
# Robot kinematics (bicycle model, tiny sideways slip for realism)
# ---------------------------------------------------------------------------

def simulate_robot(times: np.ndarray,
                   steering_deg: np.ndarray,
                   *,
                   v: float = speed,
                   L: float = wheelbase) -> Tuple[np.ndarray, np.ndarray]:
    """Return (x, y) trajectory in metres."""
    x = np.linspace(0, 6, len(times))         # forward distance ~ track length
    y = np.zeros_like(x)
    heading = np.zeros_like(x)

    for i in range(1, len(x)):
        dt = times[i] - times[i - 1]
        steer_rad = np.radians(steering_deg[i])
        if abs(steer_rad) > 1e-3:
            R = L / np.tan(steer_rad)
            omega = v / R
        else:
            omega = 0.0
        heading[i] = heading[i - 1] + omega * dt
        y[i] = y[i - 1] + v * np.sin(heading[i]) * dt * 0.1  # damped lateral slip
    return x, y

robot_x, robot_y = simulate_robot(extended_times, Total_terms)

# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------
plt.style.use("default")
fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)

frame_labels = ["N-2", "N-1", "N", "N+1", "N+2", "N+3"]

# 1) Error signal -------------------------------------------------------------
ax1 = fig.add_subplot(gs[0, 0])
ax1.plot(extended_times, extended_errors, "b-", lw=3)
ax1.scatter(key_times, key_errors, c="red", s=80, zorder=5)
ax1.set_xlabel("Time (s)")
ax1.set_ylabel("Error (px)")
ax1.set_title("1. Error Signal e(t)")
ax1.grid(alpha=0.3)

for t, e, lbl in zip(key_times, key_errors, frame_labels):
    ax1.annotate(f"{lbl}\n{e:.0f}px", xy=(t, e), xytext=(t + 0.01, e + 2),
                 textcoords="data", arrowprops=dict(arrowstyle="->", color="red"),
                 ha="center", fontsize=9, fontweight="bold")

# 2) P term -------------------------------------------------------------------
ax2 = fig.add_subplot(gs[0, 1])
ax2.plot(extended_times, P_terms, "r-", lw=3)
ax2.scatter(key_times, key_P_terms, c="darkred", s=80)
ax2.set_xlabel("Time (s)")
ax2.set_ylabel("Control (deg)")
ax2.set_title(f"2. Proportional: P = {Kp} √ó e(t)")
ax2.grid(alpha=0.3)

# 3. INTEGRAL TERM - Enhanced visualization
ax3 = fig.add_subplot(gs[0, 2])

# Plot error curve for reference
ax3.plot(extended_times, extended_errors, 'b--', linewidth=2, alpha=0.6, label='Error e(t)')

# Create progressive hatched areas for integration
time_segments = [
    (extended_times <= key_times[1]),
    (extended_times <= key_times[2]) & (extended_times > key_times[1]),
    (extended_times <= key_times[3]) & (extended_times > key_times[2]),
    (extended_times <= key_times[4]) & (extended_times > key_times[3]),
    (extended_times <= key_times[5]) & (extended_times > key_times[4])
]
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
alphas = [0.4, 0.5, 0.6, 0.4, 0.3]

for i, (mask, color, alpha) in enumerate(zip(time_segments, colors, alphas)):
    if np.any(mask):
        times_seg = extended_times[mask]
        errors_seg = extended_errors[mask]
        ax3.fill_between(times_seg, 0, errors_seg, color=color, alpha=alpha, 
                        hatch='///', edgecolor='black', linewidth=0.5)

# Plot integral value curve
ax3_twin = ax3.twinx()
ax3_twin.plot(extended_times, I_terms, 'g-', linewidth=4, label='Integral Value')
ax3_twin.scatter(key_times, key_I_terms, color='darkgreen', s=100, zorder=5)
ax3_twin.set_ylabel('Integral Term (degrees)', color='green')
ax3_twin.tick_params(axis='y', labelcolor='green')

ax3.set_xlabel('Time (seconds)')
ax3.set_ylabel('Error (pixels)', color='blue')
ax3.set_title(f'3. Integral: I = {Ki} √ó ‚à´e(œÑ)dœÑ', fontweight='bold', fontsize=12)
ax3.grid(True, alpha=0.3)
ax3.set_ylim(-2, 30)
ax3_twin.set_ylim(0, 8)

# Annotate integral progression
for i, (t, i_val) in enumerate(zip(key_times, key_I_terms)):
    if i % 2 == 0:  # Every other point
        ax3_twin.annotate(f'{i_val:.1f}¬∞', xy=(t, i_val), xytext=(t+0.01, i_val+0.3),
                         arrowprops=dict(arrowstyle='->', color='darkgreen'),
                         fontsize=9, ha='center', fontweight='bold')

# 4) D term -------------------------------------------------------------------
ax4 = fig.add_subplot(gs[1, 0])
ax4.plot(extended_times, D_terms, color="purple", lw=3)
ax4.scatter(key_times, key_D_terms, c="indigo", s=80)
ax4.axhline(0, color="black", lw=0.8, alpha=0.4)
ax4.set_xlabel("Time (s)")
ax4.set_ylabel("Control (deg)")
ax4.set_title(f"4. Derivative: D = {Kd} √ó de/dt")
ax4.grid(alpha=0.3)


# 5) Combined PID -------------------------------------------------------------
ax5 = fig.add_subplot(gs[1, 1])
ax5.plot(extended_times, P_terms, "r-", lw=2, alpha=0.6, label="P")
ax5.plot(extended_times, I_terms_interpolated, "g-", lw=2, alpha=0.6, label="I")
ax5.plot(extended_times, D_terms, color="purple", lw=2, alpha=0.6, label="D")
ax5.plot(extended_times, Total_terms, "k-", lw=3, label="Total PID")
ax5.scatter(key_times, key_totals, c="orange", s=100, zorder=5)
ax5.axhline(30, color="red", ls="--", alpha=0.7, label="¬±30¬∞ limit")
ax5.axhline(-30, color="red", ls="--", alpha=0.7)
ax5.set_xlabel("Time (s)")
ax5.set_ylabel("Control (deg)")
ax5.set_title("5. Combined PID Output")
ax5.grid(alpha=0.3)
ax5.legend(fontsize=9)

# 6) Robot response -----------------------------------------------------------
ax6 = fig.add_subplot(gs[1, 2])
line_x = robot_x
line_y = np.ones_like(line_x) * 1.5
curve_mask = (line_x >= 2.0) & (line_x <= 4.0)
line_y[curve_mask] += 0.3 * np.sin(np.pi * (line_x[curve_mask] - 2.0) / 2.0)

ax6.plot(line_x, line_y, "k--", lw=3, label="Target Line")
ax6.plot(robot_x, robot_y + 1.5, "r-", lw=3, label="Robot Path")
for i, t in enumerate(key_times):
    idx = np.searchsorted(robot_x, 6 * t / extended_times[-1])
    if i % 2 == 0:
        ax6.scatter(robot_x[idx], robot_y[idx] + 1.5, s=120, c="orange", zorder=5)
        ax6.annotate(f"Frame {frame_labels[i]}",
                     xy=(robot_x[idx], robot_y[idx] + 1.5),
                     xytext=(robot_x[idx], robot_y[idx] + 1.8),
                     arrowprops=dict(arrowstyle="->", color="orange"),
                     ha="center", fontsize=9, fontweight="bold")
ax6.set_xlabel("Distance (m)")
ax6.set_ylabel("Lateral position (m)")
ax6.set_title("6. Robot Response ‚Äì Complete Curve Navigation")
ax6.set_ylim(1.0, 2.0)
ax6.grid(alpha=0.3)
ax6.legend()

fig.suptitle("Extended PID Line Following: Complete Curve Navigation Cycle",
             fontsize=16, fontweight="bold", y=0.95)
plt.tight_layout()

# Save figure
plt.savefig(FIG_NAME, dpi=FIG_DPI, bbox_inches="tight")
print(f"Figure saved as {FIG_NAME}")

# ---------------------------------------------------------------------------
# Verification table (key frames)
# ---------------------------------------------------------------------------
print("\nKey‚Äëframe PID calculations:")
print("Frame   t(s)  e(px)   P(¬∞)  I(¬∞)   D(¬∞)   Total(¬∞)")
print("‚Äî‚Äî‚Äî   ‚Äî‚Äî‚Äî‚Äî  ‚Äî‚Äî‚Äî‚Äî‚Äî  ‚Äî‚Äî‚Äî‚Äî‚Äî  ‚Äî‚Äî‚Äî‚Äî‚Äî  ‚Äî‚Äî‚Äî‚Äî‚Äî  ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî")
for lbl, t, e, p, i_val, d, tot in zip(frame_labels, key_times,
                                       key_errors, key_P_terms,
                                       key_I_terms, key_D_terms,
                                       key_totals):
    print(f"{lbl:5s}  {t:+.1f}  {e:+5.1f}  {p:+6.1f}  {i_val:+6.1f}  {d:+6.1f}  {tot:+7.1f}")

```


################################################################################
# FILE: teaching_tools/canny_filter.py
################################################################################

```python
import tkinter as tk
from tkinter import ttk, messagebox
import cv2
import numpy as np
from PIL import Image, ImageTk, ImageDraw
import sys
import os


class CannyDemo:
    def __init__(self, image_path):
        self.root = tk.Tk()
        self.root.title("Canny Edge Detection Demo")
        self.root.geometry("1200x1000")
        
        # Load and validate image
        if not os.path.exists(image_path):
            messagebox.showerror("Error", f"Image file not found: {image_path}")
            sys.exit(1)
            
        self.original_image = cv2.imread(image_path)
        if self.original_image is None:
            messagebox.showerror("Error", f"Unable to load image: {image_path}")
            sys.exit(1)
            
        # Convert to grayscale for processing
        self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_BGR2GRAY)
        
        # Resize image if too large for display
        self.display_width = 400
        self.display_height = 300
        self.gray_resized = cv2.resize(self.gray_image, (self.display_width, self.display_height))
        
        # Initialize parameters
        self.low_threshold = tk.IntVar(value=50)
        self.high_threshold = tk.IntVar(value=150)
        self.blur_enabled = tk.BooleanVar(value=False)
        self.blur_kernel = tk.IntVar(value=5)
        
        # Matrix window parameters
        self.matrix_size = 12
        self.window_x = tk.IntVar(value=50)  # Top-left x position
        self.window_y = tk.IntVar(value=50)  # Top-left y position
        
        # Set slider ranges based on image size
        max_x = max(0, self.display_width - self.matrix_size)
        max_y = max(0, self.display_height - self.matrix_size)
        self.max_window_x = max_x
        self.max_window_y = max_y
        
        # Section expansion states
        self.original_expanded = tk.BooleanVar(value=True)
        self.gradient_expanded = tk.BooleanVar(value=True)
        self.canny_expanded = tk.BooleanVar(value=True)
        
        # Left panel image section expansion states
        self.original_image_expanded = tk.BooleanVar(value=True)
        self.processed_image_expanded = tk.BooleanVar(value=True)
        self.canny_image_expanded = tk.BooleanVar(value=True)
        
        self.setup_ui()
        self.update_display()
        
    def setup_ui(self):
        # Main frame with scrollbar
        main_canvas = tk.Canvas(self.root)
        scrollbar = ttk.Scrollbar(self.root, orient="vertical", command=main_canvas.yview)
        scrollable_frame = ttk.Frame(main_canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: main_canvas.configure(scrollregion=main_canvas.bbox("all"))
        )
        
        main_canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        main_canvas.configure(yscrollcommand=scrollbar.set)
        
        main_canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Main content frame
        main_frame = ttk.Frame(scrollable_frame, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Configure grid weights
        main_frame.columnconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=0)
        
        # Left panel for images and controls
        left_panel = ttk.Frame(main_frame)
        left_panel.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(0, 20))
        left_panel.columnconfigure(0, weight=1)
        
        # Original image section
        self.original_image_button = tk.Button(left_panel,
                                              text="‚ñº Original Image (Grayscale)",
                                              font=('Arial', 12, 'bold'),
                                              command=self.toggle_original_image_section,
                                              relief='flat', anchor='w')
        self.original_image_button.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Original image display
        self.original_image_frame = ttk.Frame(left_panel)
        self.original_image_frame.grid(row=1, column=0, pady=(0, 15))
        
        self.original_label = ttk.Label(self.original_image_frame)
        self.original_label.grid(row=0, column=0)
        
        # Processed image section
        self.processed_image_button = tk.Button(left_panel,
                                               text="‚ñº Processed Image (with/without Blur)",
                                               font=('Arial', 12, 'bold'),
                                               command=self.toggle_processed_image_section,
                                               relief='flat', anchor='w')
        self.processed_image_button.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Processed image display
        self.processed_image_frame = ttk.Frame(left_panel)
        self.processed_image_frame.grid(row=3, column=0, pady=(0, 15))
        
        self.processed_label = ttk.Label(self.processed_image_frame)
        self.processed_label.grid(row=0, column=0)
        
        # Canny image section
        self.canny_image_button = tk.Button(left_panel,
                                           text="‚ñº Canny Edge Detection",
                                           font=('Arial', 12, 'bold'),
                                           command=self.toggle_canny_image_section,
                                           relief='flat', anchor='w')
        self.canny_image_button.grid(row=4, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Canny image display
        self.canny_image_frame = ttk.Frame(left_panel)
        self.canny_image_frame.grid(row=5, column=0, pady=(0, 20))
        
        self.canny_label = ttk.Label(self.canny_image_frame)
        self.canny_label.grid(row=0, column=0)
        
        # Controls frame
        controls_frame = ttk.LabelFrame(left_panel, text="Canny Parameters", padding="10")
        controls_frame.grid(row=6, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        controls_frame.columnconfigure(1, weight=1)
        
        # Low threshold slider
        ttk.Label(controls_frame, text="Low Threshold:").grid(row=0, column=0, sticky=tk.W, padx=(0, 10))
        self.low_scale = ttk.Scale(controls_frame, from_=0, to=255, 
                                  variable=self.low_threshold, orient=tk.HORIZONTAL,
                                  command=self.on_parameter_change)
        self.low_scale.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.low_value_label = ttk.Label(controls_frame, text="50")
        self.low_value_label.grid(row=0, column=2, sticky=tk.W)
        
        # High threshold slider
        ttk.Label(controls_frame, text="High Threshold:").grid(row=1, column=0, sticky=tk.W, padx=(0, 10))
        self.high_scale = ttk.Scale(controls_frame, from_=0, to=255, 
                                   variable=self.high_threshold, orient=tk.HORIZONTAL,
                                   command=self.on_parameter_change)
        self.high_scale.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.high_value_label = ttk.Label(controls_frame, text="150")
        self.high_value_label.grid(row=1, column=2, sticky=tk.W)
        
        # Gaussian blur checkbox
        self.blur_checkbox = ttk.Checkbutton(controls_frame, text="Gaussian Blur", 
                                           variable=self.blur_enabled,
                                           command=self.on_parameter_change)
        self.blur_checkbox.grid(row=2, column=0, sticky=tk.W, pady=(10, 0))
        
        # Blur kernel slider
        ttk.Label(controls_frame, text="Blur Kernel Size:").grid(row=3, column=0, sticky=tk.W, padx=(0, 10))
        self.kernel_scale = ttk.Scale(controls_frame, from_=1, to=15, 
                                     variable=self.blur_kernel, orient=tk.HORIZONTAL,
                                     command=self.on_kernel_change)
        self.kernel_scale.grid(row=3, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.kernel_value_label = ttk.Label(controls_frame, text="5")
        self.kernel_value_label.grid(row=3, column=2, sticky=tk.W)
        
        # Initially disable kernel slider
        self.update_kernel_state()
        
        # Right panel for matrix display
        right_panel = ttk.Frame(main_frame)
        right_panel.grid(row=0, column=1, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Matrix panel title
        ttk.Label(right_panel, text="Pixel Value Analysis (12x12)", 
                 font=('Arial', 12, 'bold')).grid(row=0, column=0, pady=(0, 10))
        
        # Window position controls
        position_frame = ttk.LabelFrame(right_panel, text="Window Position", padding="5")
        position_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        
        ttk.Label(position_frame, text="X:").grid(row=0, column=0, sticky=tk.W)
        self.x_scale = ttk.Scale(position_frame, from_=0, to=self.max_window_x,
                                variable=self.window_x, orient=tk.HORIZONTAL,
                                command=self.on_window_change)
        self.x_scale.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(5, 5))
        self.x_value_label = ttk.Label(position_frame, text="50")
        self.x_value_label.grid(row=0, column=2)
        
        ttk.Label(position_frame, text="Y:").grid(row=1, column=0, sticky=tk.W)
        self.y_scale = ttk.Scale(position_frame, from_=0, to=self.max_window_y,
                                variable=self.window_y, orient=tk.HORIZONTAL,
                                command=self.on_window_change)
        self.y_scale.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=(5, 5))
        self.y_value_label = ttk.Label(position_frame, text="50")
        self.y_value_label.grid(row=1, column=2)
        
        position_frame.columnconfigure(1, weight=1)
        
        # Create expandable sections
        self.create_expandable_sections(right_panel)
        
    def create_expandable_sections(self, parent):
        current_row = 2
        
        # Original Values Section
        self.original_section_button = tk.Button(parent, 
                                                text="‚ñº Original Grayscale Values",
                                                font=('Arial', 10, 'bold'),
                                                command=self.toggle_original_section,
                                                relief='flat', anchor='w')
        self.original_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(10, 0))
        
        self.original_section_frame = ttk.Frame(parent)
        self.original_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        current_row += 2
        
        # Gradient Calculations Section
        self.gradient_section_button = tk.Button(parent,
                                                text="‚ñº Gradient Calculations", 
                                                font=('Arial', 10, 'bold'),
                                                command=self.toggle_gradient_section,
                                                relief='flat', anchor='w')
        self.gradient_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(0, 0))
        
        self.gradient_section_frame = ttk.Frame(parent)
        self.gradient_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        current_row += 2
        
        # Final Edge Detection Section
        self.canny_section_button = tk.Button(parent,
                                             text="‚ñº Final Edge Detection",
                                             font=('Arial', 10, 'bold'), 
                                             command=self.toggle_canny_section,
                                             relief='flat', anchor='w')
        self.canny_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(0, 0))
        
        self.canny_section_frame = ttk.Frame(parent)
        self.canny_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        
        # Create matrix grids
        self.create_matrix_grids()
        
    def create_matrix_grids(self):
        # Create original values matrix
        self.original_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(self.original_section_frame, text="0", 
                               width=5, height=2, relief="solid", borderwidth=1,
                               font=('Courier', 10), bg='white')
                label.grid(row=i, column=j, padx=1, pady=1)
                row.append(label)
            self.original_matrix_labels.append(row)
            
        # Create gradient matrices (Gx, Gy, Magnitude)
        gradient_container = ttk.Frame(self.gradient_section_frame)
        gradient_container.grid(row=0, column=0)
        
        # Gx matrix
        gx_frame = ttk.Frame(gradient_container)
        gx_frame.grid(row=1, column=0, padx=(0, 10))
        ttk.Label(gx_frame, text="Gx (Horizontal)", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.gx_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(gx_frame, text="0", width=5, height=2, 
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.gx_matrix_labels.append(row)
            
        # Gy matrix
        gy_frame = ttk.Frame(gradient_container)
        gy_frame.grid(row=1, column=1, padx=(0, 10))
        ttk.Label(gy_frame, text="Gy (Vertical)", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.gy_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(gy_frame, text="0", width=5, height=2,
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.gy_matrix_labels.append(row)
            
        # Magnitude matrix
        mag_frame = ttk.Frame(gradient_container)
        mag_frame.grid(row=1, column=2)
        ttk.Label(mag_frame, text="Magnitude", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.mag_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(mag_frame, text="0", width=5, height=2,
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.mag_matrix_labels.append(row)
            
        # Create canny result matrix
        self.canny_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(self.canny_section_frame, text="0", 
                               width=5, height=2, relief="solid", borderwidth=1,
                               font=('Courier', 10), bg='white')
                label.grid(row=i, column=j, padx=1, pady=1)
                row.append(label)
            self.canny_matrix_labels.append(row)
            
    def toggle_original_image_section(self):
        self.original_image_expanded.set(not self.original_image_expanded.get())
        if self.original_image_expanded.get():
            self.original_image_frame.grid()
            self.original_image_button.config(text="‚ñº Original Image (Grayscale)")
        else:
            self.original_image_frame.grid_remove()
            self.original_image_button.config(text="‚ñ∂ Original Image (Grayscale)")
            
    def toggle_processed_image_section(self):
        self.processed_image_expanded.set(not self.processed_image_expanded.get())
        if self.processed_image_expanded.get():
            self.processed_image_frame.grid()
            self.processed_image_button.config(text="‚ñº Processed Image (with/without Blur)")
        else:
            self.processed_image_frame.grid_remove()
            self.processed_image_button.config(text="‚ñ∂ Processed Image (with/without Blur)")
            
    def toggle_canny_image_section(self):
        self.canny_image_expanded.set(not self.canny_image_expanded.get())
        if self.canny_image_expanded.get():
            self.canny_image_frame.grid()
            self.canny_image_button.config(text="‚ñº Canny Edge Detection")
        else:
            self.canny_image_frame.grid_remove()
            self.canny_image_button.config(text="‚ñ∂ Canny Edge Detection")
            
    def toggle_original_section(self):
        self.original_expanded.set(not self.original_expanded.get())
        if self.original_expanded.get():
            self.original_section_frame.grid()
            self.original_section_button.config(text="‚ñº Original Grayscale Values")
        else:
            self.original_section_frame.grid_remove()
            self.original_section_button.config(text="‚ñ∂ Original Grayscale Values")
            
    def toggle_gradient_section(self):
        self.gradient_expanded.set(not self.gradient_expanded.get())
        if self.gradient_expanded.get():
            self.gradient_section_frame.grid()
            self.gradient_section_button.config(text="‚ñº Gradient Calculations")
        else:
            self.gradient_section_frame.grid_remove()
            self.gradient_section_button.config(text="‚ñ∂ Gradient Calculations")
            
    def toggle_canny_section(self):
        self.canny_expanded.set(not self.canny_expanded.get())
        if self.canny_expanded.get():
            self.canny_section_frame.grid()
            self.canny_section_button.config(text="‚ñº Final Edge Detection")
        else:
            self.canny_section_frame.grid_remove()
            self.canny_section_button.config(text="‚ñ∂ Final Edge Detection")
        
    def on_kernel_change(self, value):
        # Ensure kernel size is odd
        kernel_size = int(float(value))
        if kernel_size % 2 == 0:
            kernel_size += 1
            self.blur_kernel.set(kernel_size)
        
        self.kernel_value_label.config(text=str(kernel_size))
        self.update_display()
        
    def on_parameter_change(self, value=None):
        # Update value labels
        self.low_value_label.config(text=str(int(self.low_threshold.get())))
        self.high_value_label.config(text=str(int(self.high_threshold.get())))
        
        # Update kernel slider state
        self.update_kernel_state()
        
        # Update display
        self.update_display()
        
    def on_window_change(self, value=None):
        # Update position labels
        self.x_value_label.config(text=str(int(self.window_x.get())))
        self.y_value_label.config(text=str(int(self.window_y.get())))
        
        # Update display
        self.update_display()
        
    def update_kernel_state(self):
        if self.blur_enabled.get():
            self.kernel_scale.config(state='normal')
            self.kernel_value_label.config(foreground='black')
        else:
            self.kernel_scale.config(state='disabled')
            self.kernel_value_label.config(foreground='gray')
            
    def draw_rectangle_on_image(self, image_array, x, y, size):
        """Draw a red rectangle on the image to show the analysis window"""
        # Convert grayscale to RGB for colored rectangle
        if len(image_array.shape) == 2:
            rgb_image = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)
        else:
            rgb_image = image_array.copy()
            
        # Draw rectangle
        cv2.rectangle(rgb_image, (x, y), (x + size, y + size), (255, 0, 0), 2)
        return rgb_image
    
    def compute_gradients(self, image):
        """Compute Sobel gradients"""
        # Sobel X (horizontal edges)
        gx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
        # Sobel Y (vertical edges)  
        gy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
        # Magnitude
        magnitude = np.sqrt(gx**2 + gy**2)
        
        return gx, gy, magnitude
            
    def update_display(self):
        # Get current window position
        x = int(self.window_x.get())
        y = int(self.window_y.get())
        
        # Display original grayscale image with rectangle
        original_with_rect = self.draw_rectangle_on_image(self.gray_resized, x, y, self.matrix_size)
        original_pil = Image.fromarray(original_with_rect)
        original_photo = ImageTk.PhotoImage(original_pil)
        self.original_label.config(image=original_photo)
        self.original_label.image = original_photo  # Keep a reference
        
        # Apply Gaussian blur if enabled
        processed_image = self.gray_resized.copy()
        if self.blur_enabled.get():
            kernel_size = int(self.blur_kernel.get())
            # Ensure kernel size is odd
            if kernel_size % 2 == 0:
                kernel_size += 1
            processed_image = cv2.GaussianBlur(processed_image, (kernel_size, kernel_size), 0)
        
        # Display processed image with rectangle
        processed_with_rect = self.draw_rectangle_on_image(processed_image, x, y, self.matrix_size)
        processed_pil = Image.fromarray(processed_with_rect)
        processed_photo = ImageTk.PhotoImage(processed_pil)
        self.processed_label.config(image=processed_photo)
        self.processed_label.image = processed_photo  # Keep a reference
        
        # Compute gradients
        gx, gy, magnitude = self.compute_gradients(processed_image)
        
        # Apply Canny edge detection
        low_thresh = int(self.low_threshold.get())
        high_thresh = int(self.high_threshold.get())
        
        # Ensure high threshold is greater than low threshold
        if high_thresh <= low_thresh:
            high_thresh = low_thresh + 1
            
        canny_edges = cv2.Canny(processed_image, low_thresh, high_thresh)
        
        # Display Canny result with rectangle
        canny_with_rect = self.draw_rectangle_on_image(canny_edges, x, y, self.matrix_size)
        canny_pil = Image.fromarray(canny_with_rect)
        canny_photo = ImageTk.PhotoImage(canny_pil)
        self.canny_label.config(image=canny_photo)
        self.canny_label.image = canny_photo  # Keep a reference
        
        # Update matrices
        self.update_matrices(x, y, processed_image, gx, gy, magnitude, canny_edges)
        
    def update_matrices(self, x, y, processed_image, gx, gy, magnitude, canny_edges):
        """Update all pixel value matrices"""
        # Extract the window regions
        original_window = self.gray_resized[y:y+self.matrix_size, x:x+self.matrix_size]
        gx_window = gx[y:y+self.matrix_size, x:x+self.matrix_size]
        gy_window = gy[y:y+self.matrix_size, x:x+self.matrix_size]
        mag_window = magnitude[y:y+self.matrix_size, x:x+self.matrix_size]
        canny_window = canny_edges[y:y+self.matrix_size, x:x+self.matrix_size]
        
        # Update original values matrix
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < original_window.shape[0] and j < original_window.shape[1]:
                    value = int(original_window[i, j])
                    self.original_matrix_labels[i][j].config(text=str(value))
                    # Color code: darker background for higher values
                    intensity = min(255, max(0, value))
                    bg_color = f'#{intensity:02x}{intensity:02x}{intensity:02x}'
                    text_color = 'white' if intensity < 128 else 'black'
                    self.original_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                else:
                    self.original_matrix_labels[i][j].config(text="", bg='white')
                    
        # Update gradient matrices
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < gx_window.shape[0] and j < gx_window.shape[1]:
                    # Gx values
                    gx_val = int(gx_window[i, j])
                    self.gx_matrix_labels[i][j].config(text=str(gx_val))
                    # Color code: blue for negative, red for positive, white for zero
                    if gx_val > 0:
                        intensity = min(255, abs(gx_val) * 2)
                        bg_color = f'#{255:02x}{255-intensity//2:02x}{255-intensity//2:02x}'  # Red scale
                        text_color = 'black' if intensity < 128 else 'white'
                    elif gx_val < 0:
                        intensity = min(255, abs(gx_val) * 2)
                        bg_color = f'#{255-intensity//2:02x}{255-intensity//2:02x}{255:02x}'  # Blue scale
                        text_color = 'black' if intensity < 128 else 'white'
                    else:
                        bg_color = 'white'
                        text_color = 'black'
                    self.gx_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                    
                    # Gy values
                    gy_val = int(gy_window[i, j])
                    self.gy_matrix_labels[i][j].config(text=str(gy_val))
                    # Same color coding as Gx
                    if gy_val > 0:
                        intensity = min(255, abs(gy_val) * 2)
                        bg_color = f'#{255:02x}{255-intensity//2:02x}{255-intensity//2:02x}'  # Red scale
                        text_color = 'black' if intensity < 128 else 'white'
                    elif gy_val < 0:
                        intensity = min(255, abs(gy_val) * 2)
                        bg_color = f'#{255-intensity//2:02x}{255-intensity//2:02x}{255:02x}'  # Blue scale
                        text_color = 'black' if intensity < 128 else 'white'
                    else:
                        bg_color = 'white'
                        text_color = 'black'
                    self.gy_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                    
                    # Magnitude values
                    mag_val = int(mag_window[i, j])
                    self.mag_matrix_labels[i][j].config(text=str(mag_val))
                    # Grayscale based on magnitude
                    intensity = min(255, max(0, mag_val))
                    bg_color = f'#{intensity:02x}{intensity:02x}{intensity:02x}'
                    text_color = 'white' if intensity < 128 else 'black'
                    self.mag_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                else:
                    self.gx_matrix_labels[i][j].config(text="", bg='white')
                    self.gy_matrix_labels[i][j].config(text="", bg='white')
                    self.mag_matrix_labels[i][j].config(text="", bg='white')
                    
        # Update canny result matrix
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < canny_window.shape[0] and j < canny_window.shape[1]:
                    value = int(canny_window[i, j])
                    self.canny_matrix_labels[i][j].config(text=str(value))
                    # Color code: white for edges (255), black for non-edges (0)
                    if value > 0:
                        self.canny_matrix_labels[i][j].config(bg='white', fg='black')
                    else:
                        self.canny_matrix_labels[i][j].config(bg='black', fg='white')
                else:
                    self.canny_matrix_labels[i][j].config(text="", bg='white')
        
    def run(self):
        self.root.mainloop()


def main():
    if len(sys.argv) != 2:
        print("Usage: python canny_demo.py <image_path>")
        print("Example: python canny_demo.py sample.jpg")
        sys.exit(1)
        
    image_path = sys.argv[1]
    app = CannyDemo(image_path)
    app.run()


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week1_line_following/__init__.py
################################################################################

```python
"""Week 1: Line Following Implementation"""

```


################################################################################
# FILE: week1_line_following/line_follower.py
################################################################################

```python
#!/usr/bin/env python3

"""
Line Following Module - Week 1 Assignment
=========================================

STUDENT IMPLEMENTATION FOCUS:
Students implement these core computer vision functions:
1. _extract_roi() - Manually crop image region
2. _convert_to_grayscale() - Convert BGR to grayscale
3. _detect_line_center() - Find the center of line in image
4. _calculate_error() - Compute steering error

The debug system works immediately with dummy values, then improves as 
students implement each function properly.
"""

import cv2
import numpy as np
import time

class LineFollower:
    def __init__(self):
        """Initialize the line follower"""
        
        # =================================================================
        # PID CONTROLLER (PROVIDED - Students focus on computer vision)
        # =================================================================
        self.Kp = 0.8    # Proportional gain
        self.Ki = 0.1    # Integral gain  
        self.Kd = 0.3    # Derivative gain
        
        # PID state variables
        self.previous_error = 0.0
        self.integral_error = 0.0
        self.last_time = time.time()
        
        # =================================================================
        # STUDENT TUNABLE PARAMETERS
        # Students can adjust these values to improve performance
        # =================================================================
        self.roi_top = 0.6        # Start ROI at 60% down the image
        self.roi_bottom = 0.9     # End ROI at 90% down the image
        self.roi_left = 0.1       # Start ROI at 10% from left
        self.roi_right = 0.9      # End ROI at 90% from left
        
        self.canny_low = 50       # Lower threshold for edge detection
        self.canny_high = 150     # Upper threshold for edge detection
        
        # =================================================================
        # STUDENT IMPLEMENTATION VARIABLES
        # These start as dummy values - students implement the functions that set them
        # =================================================================
        
        # Region of Interest - students implement _extract_roi()
        self.current_roi = None           # Will hold cropped image
        self.roi_bounds = (0, 0, 0, 0)   # (top, bottom, left, right) pixel coordinates
        
        # Grayscale conversion - students implement _convert_to_grayscale()  
        self.gray_image = None            # Will hold grayscale version
        
        # Line detection - students implement _detect_line_center()
        self.line_center_x = None         # X coordinate of detected line center
        self.line_center_y = None         # Y coordinate of detected line center
        
        # Error calculation - students implement _calculate_error()
        self.current_error = 0.0          # Pixel error (image_center - line_center)
        self.image_center_x = None        # Center X coordinate of ROI
        
        # =================================================================
        # DEBUG SYSTEM (PROVIDED - Works with dummy values)
        # =================================================================
        self.debug_frame = None
        self.current_debug_data = {
            'error_px': 0.0,
            'steering_angle': 0.0,
            'lines_detected': 0,
            'implementation_status': {
                'roi_extraction': False,
                'grayscale_conversion': False,
                'line_detection': False,
                'error_calculation': False
            }
        }
        
        print("‚úÖ Line follower initialized")
        print("üéØ TODO: Implement the 4 core computer vision functions")
    
    def compute_steering_angle(self, frame, debug_level=0):
        """
        Main processing pipeline - calls student-implemented functions
        """
        if frame is None:
            return 0.0
        
        try:
            # Step 1: Extract Region of Interest (STUDENT IMPLEMENTS)
            self._extract_roi(frame)
            
            # Step 2: Convert to grayscale (STUDENT IMPLEMENTS)
            self._convert_to_grayscale()
            
            # Step 3: Detect line center (STUDENT IMPLEMENTS)
            self._detect_line_center()
            
            # Step 4: Calculate error (STUDENT IMPLEMENTS)
            self._calculate_error()
            
            # Step 5: Apply PID control (PROVIDED)
            steering_angle = self._apply_pid_control()
            
            # Step 6: Update debug visualization (PROVIDED)
            self._update_debug_display(frame, steering_angle, debug_level)
            
            return steering_angle
            
        except Exception as e:
            print(f"Processing error: {e}")
            return 0.0
    
    # =========================================================================
    # STUDENT IMPLEMENTATION SECTION - COMPLETE THESE 4 FUNCTIONS
    # =========================================================================
    
    def _extract_roi(self, frame):
        """
        STUDENT TODO: Extract Region of Interest from the image
        
        Your task:
        1. Calculate pixel coordinates from the percentage parameters above
        2. Use array slicing to crop the image: frame[top:bottom, left:right]
        3. Store the result in self.current_roi
        4. Store the bounds in self.roi_bounds for debug visualization
        
        Example:
            height, width = frame.shape[:2]
            top_px = int(height * self.roi_top)
            # ... calculate other bounds
            self.current_roi = frame[top_px:bottom_px, left_px:right_px]
        """
        
        # STUDENT CODE HERE:
        # Remove this dummy implementation and write your own
        
        height, width = frame.shape[:2]
        
        # TODO: Calculate pixel coordinates from percentages
        # top_px = int(height * self.roi_top)
        # bottom_px = int(height * self.roi_bottom)  
        # left_px = int(width * self.roi_left)
        # right_px = int(width * self.roi_right)
        
        # TODO: Extract ROI using array slicing
        # self.current_roi = frame[top_px:bottom_px, left_px:right_px]
        # self.roi_bounds = (top_px, bottom_px, left_px, right_px)
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        self.current_roi = frame  # Just use full frame for now
        self.roi_bounds = (0, height, 0, width)
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['roi_extraction'] = False
    
    def _convert_to_grayscale(self):
        """
        STUDENT TODO: Convert the ROI to grayscale
        
        Your task:
        1. Check if self.current_roi exists and is not None
        2. Convert from BGR color to grayscale
        3. Store result in self.gray_image
        
        Methods you can use:
        - cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        - Or manual conversion: gray = 0.299*R + 0.587*G + 0.114*B
        
        Example:
            if self.current_roi is not None:
                self.gray_image = cv2.cvtColor(self.current_roi, cv2.COLOR_BGR2GRAY)
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Convert ROI to grayscale
        # if self.current_roi is not None:
        #     self.gray_image = cv2.cvtColor(self.current_roi, cv2.COLOR_BGR2GRAY)
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.current_roi is not None:
            # Just make a black image for now
            height, width = self.current_roi.shape[:2]
            self.gray_image = np.zeros((height, width), dtype=np.uint8)
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['grayscale_conversion'] = False
    
    def _detect_line_center(self):
        """
        STUDENT TODO: Find the center of the line in the grayscale image
        
        Your task:
        1. Use edge detection to find the line (cv2.Canny is recommended)
        2. Find the center point of the detected line
        3. Store coordinates in self.line_center_x and self.line_center_y
        
        Suggested approach:
        1. Apply Canny edge detection: cv2.Canny(gray, low_thresh, high_thresh)
        2. Find white pixels: np.where(edges > 0)
        3. Calculate mean position of white pixels
        
        Example:
            if self.gray_image is not None:
                edges = cv2.Canny(self.gray_image, self.canny_low, self.canny_high)
                white_pixels = np.where(edges > 0)
                if len(white_pixels[0]) > 0:
                    self.line_center_y = int(np.mean(white_pixels[0]))
                    self.line_center_x = int(np.mean(white_pixels[1]))
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Apply edge detection and find line center
        # if self.gray_image is not None:
        #     edges = cv2.Canny(self.gray_image, self.canny_low, self.canny_high)
        #     white_pixels = np.where(edges > 0)
        #     if len(white_pixels[0]) > 0:
        #         self.line_center_y = int(np.mean(white_pixels[0]))
        #         self.line_center_x = int(np.mean(white_pixels[1]))
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.gray_image is not None:
            height, width = self.gray_image.shape
            # Just put line center in middle of image for now
            self.line_center_x = width // 2
            self.line_center_y = height // 2
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['line_detection'] = False
    
    def _calculate_error(self):
        """
        STUDENT TODO: Calculate steering error
        
        Your task:
        1. Find the center X coordinate of the ROI image
        2. Calculate error as: image_center_x - line_center_x
        3. Store result in self.current_error
        
        The error tells us:
        - Positive error: line is to the LEFT of center ‚Üí steer RIGHT
        - Negative error: line is to the RIGHT of center ‚Üí steer LEFT  
        - Zero error: line is centered ‚Üí go straight
        
        Example:
            if self.current_roi is not None and self.line_center_x is not None:
                roi_width = self.current_roi.shape[1]
                self.image_center_x = roi_width // 2
                self.current_error = self.image_center_x - self.line_center_x
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Calculate error between image center and line center
        # if self.current_roi is not None and self.line_center_x is not None:
        #     roi_width = self.current_roi.shape[1]  
        #     self.image_center_x = roi_width // 2
        #     self.current_error = self.image_center_x - self.line_center_x
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.current_roi is not None:
            roi_width = self.current_roi.shape[1]
            self.image_center_x = roi_width // 2
            # Dummy error - just use 0 for now
            self.current_error = 0.0
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['error_calculation'] = False
    
    # =========================================================================
    # PROVIDED FUNCTIONS - Students don't need to modify these
    # =========================================================================
    
    def _apply_pid_control(self):
        """Apply PID control to convert error to steering angle (PROVIDED)"""
        
        current_time = time.time()
        dt = current_time - self.last_time
        
        if dt <= 0:
            dt = 0.1
        
        # PID calculation
        proportional = self.Kp * self.current_error
        
        self.integral_error += self.current_error * dt
        integral = self.Ki * self.integral_error
        
        derivative_error = (self.current_error - self.previous_error) / dt
        derivative = self.Kd * derivative_error
        
        # Combine terms and limit output
        steering_angle = proportional + integral + derivative
        steering_angle = np.clip(steering_angle, -30, 30)
        
        # Update state
        self.previous_error = self.current_error
        self.last_time = current_time
        
        return steering_angle
    
    def _update_debug_display(self, original_frame, steering_angle, debug_level):
        """Update debug visualization and data (PROVIDED)"""
        
        # Update sidebar debug data
        lines_detected = 1 if (self.line_center_x is not None and 
                              self.current_debug_data['implementation_status']['line_detection']) else 0
        
        self.current_debug_data.update({
            'error_px': round(self.current_error, 1),
            'steering_angle': round(steering_angle, 1),
            'lines_detected': lines_detected
        })
        
        # Create debug frame
        if debug_level > 0:
            self.debug_frame = self._create_debug_visualization(
                original_frame, steering_angle, debug_level
            )
        else:
            self.debug_frame = original_frame.copy()
    
    def _create_debug_visualization(self, frame, steering_angle, debug_level):
        """Create visual debug overlay (PROVIDED)"""
        
        debug_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Level 1: Show implementation status
        if debug_level >= 1:
            status_y = 30
            for func_name, implemented in self.current_debug_data['implementation_status'].items():
                color = (0, 255, 0) if implemented else (0, 0, 255)
                status = "‚úì" if implemented else "‚úó"
                text = f"{status} {func_name.replace('_', ' ').title()}"
                cv2.putText(debug_frame, text, (10, status_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                status_y += 20
            
            # Show current values
            values_text = [
                f"Error: {self.current_error:.1f}px",
                f"Steering: {steering_angle:.1f}¬∞"
            ]
            
            for i, text in enumerate(values_text):
                cv2.putText(debug_frame, text, (10, height - 40 + i*20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Level 2: Show ROI bounds
        if debug_level >= 2 and len(self.roi_bounds) == 4:
            top, bottom, left, right = self.roi_bounds
            cv2.rectangle(debug_frame, (left, top), (right, bottom), (0, 255, 255), 2)
            cv2.putText(debug_frame, "ROI", (left, top-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # Level 3: Show detected centers
        if debug_level >= 3:
            # Show image center (blue)
            if self.image_center_x is not None and len(self.roi_bounds) == 4:
                top, bottom, left, right = self.roi_bounds
                center_global_x = left + self.image_center_x
                center_global_y = (top + bottom) // 2
                cv2.circle(debug_frame, (center_global_x, center_global_y), 8, (255, 0, 0), -1)
                cv2.putText(debug_frame, "IMG CENTER", 
                           (center_global_x-40, center_global_y-15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)
            
            # Show line center (green)
            if (self.line_center_x is not None and self.line_center_y is not None and 
                len(self.roi_bounds) == 4):
                top, bottom, left, right = self.roi_bounds
                line_global_x = left + self.line_center_x
                line_global_y = top + self.line_center_y
                cv2.circle(debug_frame, (line_global_x, line_global_y), 8, (0, 255, 0), -1)
                cv2.putText(debug_frame, "LINE CENTER", 
                           (line_global_x-40, line_global_y+25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
                
                # Draw error line (red)
                if self.image_center_x is not None:
                    cv2.line(debug_frame, 
                           (center_global_x, center_global_y),
                           (line_global_x, line_global_y), 
                           (0, 0, 255), 2)
        
        return debug_frame
    
    def get_debug_frame(self):
        """Return current debug frame"""
        return self.debug_frame
    
    def update_parameters(self, **kwargs):
        """Update parameters during runtime"""
        if 'kp' in kwargs and kwargs['kp'] is not None:
            self.Kp = kwargs['kp']
        if 'ki' in kwargs and kwargs['ki'] is not None:
            self.Ki = kwargs['ki']
        if 'kd' in kwargs and kwargs['kd'] is not None:
            self.Kd = kwargs['kd']

# =========================================================================
# STUDENT TESTING SECTION
# =========================================================================

def test_implementation():
    """Test your implementation step by step"""
    
    print("üß™ Testing Line Follower Implementation")
    print("="*50)
    
    # Create test instance
    lf = LineFollower()
    
    # Create test image with white line
    test_image = np.zeros((240, 320, 3), dtype=np.uint8)
    cv2.line(test_image, (100, 50), (100, 200), (255, 255, 255), 10)
    
    print("üì∏ Processing test image...")
    
    # Test each function step by step
    print("\n1. Testing ROI extraction...")
    lf._extract_roi(test_image)
    roi_status = "‚úÖ Working" if lf.current_roi is not None else "‚ùå Not implemented"
    print(f"   ROI: {roi_status}")
    
    print("\n2. Testing grayscale conversion...")
    lf._convert_to_grayscale()
    gray_status = "‚úÖ Working" if lf.gray_image is not None else "‚ùå Not implemented"
    print(f"   Grayscale: {gray_status}")
    
    print("\n3. Testing line detection...")
    lf._detect_line_center()
    line_status = "‚úÖ Working" if lf.line_center_x is not None else "‚ùå Not implemented"
    print(f"   Line detection: {line_status}")
    
    print("\n4. Testing error calculation...")
    lf._calculate_error()
    error_status = "‚úÖ Working" if lf.image_center_x is not None else "‚ùå Not implemented"
    print(f"   Error calculation: {error_status}")
    
    # Test full pipeline
    print("\n5. Testing full pipeline...")
    steering = lf.compute_steering_angle(test_image, debug_level=1)
    print(f"   Steering angle: {steering:.1f}¬∞")
    
    print(f"\nüìä Implementation status:")
    for func, status in lf.current_debug_data['implementation_status'].items():
        icon = "‚úÖ" if status else "‚≠ï"
        print(f"   {icon} {func.replace('_', ' ').title()}")
    
    print(f"\nüéØ Next steps:")
    print("1. Implement the functions marked with ‚≠ï")
    print("2. Set implementation_status to True when complete")  
    print("3. Test with real camera via web interface")
    print("4. Tune parameters for better performance")

if __name__ == "__main__":
    test_implementation()

```


################################################################################
# FILE: week2_object_detection/sign_detector.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import os
import time
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core.utils.console_logger import console_logger


class SignDetector:
    """
    WEEK 2: PERCEPTION FOR AUTONOMOUS SYSTEMS
    ==========================================
    
    THE BIG PICTURE:
    In autonomous vehicles, we utilize multiple AI models to compose complex decision making. 
    Think about our porgram as a conductor in an orchestra, telling different instuments (tools) to come in and provide
    their sound (functionality) at the necessary moments of the song (decison process).
    
    PERCEPTION PIPELINE:
    Camera ‚Üí Object Detection ‚Üí Depth Estimation ‚Üí Decision Logic ‚Üí Robot Action
    
    TODAY'S JOURNEY:
    1. 'detect_signs()' - Use YOLO model to find objects in camera images
    2. 'detect_signs()' - Understand model outputs and tune confidence thresholds  
    3. 'should_stop()' - Make stopping decisions using bounding box size
    4. '_advanced_depth_stopping()' - Upgrade stopping decision to use the MiDaS monocular depth estimateion model
    
    LEARNING GOALS:
    - AI models are tools: understand inputs/outputs and how to apply these tools to navigate complex problems
    - Multiple models can work together
    - Threshold tuning is critical for real-world performance
    - Understand the perfomrance of simple implmentations before adding complexity

    NOTE: Use console_logger.stop() instead of print() for important messages - they'll appear in the web console!
    """
    
    def __init__(self):
        """Initialize ONNX model and detection parameters"""
        
        # Model and detection setup
        self.model_path = os.path.join(os.path.dirname(__file__), 'models', 'best.onnx')
        self.class_names = ['Stop_Sign', 'TU_Logo', 'Stahp', 'Falling_Cows']
        
        # STUDENT TUNABLE PARAMETERS - Experiment with these values
        self.confidence_threshold = 0.05      # Minimum confidence to trust detections
        self.simple_area_threshold = 5000    # Pixel area threshold for stopping
        self.depth_distance_threshold = 1.5  # Distance threshold in meters (advanced)
        
        # Model input specifications (determined by training)
        self.input_size = 640  # Model expects 640x640 input
        
        # Initialize caching for debug visualization
        self._cached_depth_map = None
        self._last_depth_inference_time = 0
        
        # Load the ONNX model
        self._load_model()
        
        # Try to load depth estimation model (optional for advanced section)
        self._load_depth_model()
        
        print("SignDetector initialized - Students: Implement detect_signs() and should_stop()!")

    def detect_signs(self, camera_frame):
        """
        STEP 1: OBJECT DETECTION PIPELINE
        =================================
        
        CONCEPT: Transform camera image ‚Üí list of detected objects
        
        THE PROCESS:
        1. Preprocess: Camera format ‚Üí Model format (we provide this)
        2. Inference: Run neural network (YOU implement this)  
        3. Investigate: Explore what the model outputs (YOU discover this)
        4. Parse: Extract useful information (YOU implement this)
        5. Filter: Keep only confident detections (YOU decide thresholds)
        """
        
        if self.session is None:
            return []  # No model loaded
        
        try:
            # STEP 1: Preprocessing (provided for you)
            # Converts 320x240 BGR camera image ‚Üí to the tensor format needed for our Yolov8 nanon model
            input_tensor = self._preprocess_frame(camera_frame)
            if input_tensor is None:
                return []
            
            print(f"Preprocessed frame shape: {input_tensor.shape}")
            
            # STEP 2: INFERENCE - YOU IMPLEMENT THIS LINE
            # TASK: Run the neural network on your preprocessed image
            # TODO: Call self.session.run() with the input tensor
            # HINT: outputs = self.session.run(None, {self.input_name: input_tensor})
            
            outputs = None  # TODO: Replace this line with actual inference call
            
            if outputs is None:
                return [] 
            
            # STEP 3: INVESTIGATE THE OUTPUTS
            # DISCOVERY TASK: What did the model give us back?
            # print(f"Model output shape: {outputs[0].shape}")
            # print(f"Output data type: {outputs[0].dtype}")
            
            # QUESTION: What do you think these dimensions mean?
            # Our model vocabulary: ['Stop_Sign', 'TU_Logo', 'Stahp', 'Falling_Cows'] (4 classes)
            # Expected shape: [1, 8, 8400] = [batch, coordinates+classes, detections]
            
            # STEP 4: PARSE THE OUTPUTS - YOU IMPLEMENT THIS
            # The output format is: [center_x, center_y, width, height, class0_conf, class1_conf, class2_conf, class3_conf]
            
            detections = [] # This is where we will store properly formated boundary boxes we detect
            
            # TODO: Reshape the outputs to work with them. Currenlty our outputs is shapped like [1,8, n], but we want ot pupulate our 'detections' list iwht list structed as [n, 8] 
            # HINT: One way of going from [1, 8, n] to [n, 8] ......
            # pred = outputs[0]  # Remove batch dimension
            # pred = np.transpose(pred, (1, 0))  # Transpose to [8400, 8]
            
            # TODO: Loop through each detection
            # for detection in pred:
            #     # TODO: Extract coordinates (first 4 values from detection)
            #     # center_x, center_y, width, height = 
            #     
            #     # TODO: Extract confidence scores (last 4 values from detection)  
            #     # class_confidences =
            #     
            #     # TODO: Find the class with highest confidence
            #     # max_confidence = np.max(class_confidences)
            #     # predicted_class_id = np.argmax(class_confidences)
            #     
            #     # TODO: Filter by confidence threshold
            #     # if max_confidence > self.confidence_threshold:
            #         # TODO: Convert center format to corner format and scale coordinates
            #         # converted_bbox = self._convert_coordinates(center_x, center_y, width, height, camera_frame.shape)
            #         # 
            #         # TODO: Add to detections list in required format
            #         # detections.append({
            #         #     'bbox': converted_bbox,
            #         #     'confidence': float(max_confidence),
            #         #     'class_name': self.class_names[predicted_class_id]
            #         # })
            
            # REQUIRED OUTPUT FORMAT:
            # [{'bbox': [x, y, w, h], 'confidence': 0.95, 'class_name': 'Stop_Sign'}, ...]
            
            return detections
            
        except Exception as e:
            print(f"Detection error: {e}")
            return []
    
    def should_stop(self, detected_signs, camera_frame):
        """
        STEP 2: DECISION LOGIC PIPELINE  
        ===============================
        
        CONCEPT: Convert perception data to robot actions
        
        THE CORE INSIGHT:
        How do we estimate distance from a single 2D camera image?
        
        APPROACH 1 - SIZE-BASED REASONING (Simple & Effective):
        - Objects appear larger when closer to camera
        - Measure bounding box area in pixels  
        - Large area = close object = STOP
        - Small area = far object = CONTINUE
        
        APPROACH 2 - DEPTH ESTIMATION (Advanced):
        - Use an AI model to estimate depth at every pixel
        - Sample depths within the bounding box
        - If depth estimates show we're close to the object...... STOP!
        
        IMPORTANT: We must check ALL detected objects
        - Multiple signs might be detected in one frame
        - If any object is close enough, stop the robot
        - Iterate through the entire detected_signs list
        """
        
        if not detected_signs:
            return False  # No objects detected, safe to continue
        
        # APPROACH 1: SIZE-BASED STOPPING (Implement this first)
        # CONCEPT: Bigger bounding box = closer object
        
        # TASK: Check ALL detected objects to see if ANY are close enough to stop
        # TODO: Loop through detected_signs and check each detection
        # for detection in detected_signs:
        #     bbox = detection['bbox']  # [x, y, width, height]
        #     
        #     # TODO: Calculate area of this detection using the width and the heigh
        #     # area = 
        #     
        #     # TODO: Compare to threshold
        #     # if area > self.simple_area_threshold:
        #         # print(f"STOPPING: {detection['class_name']} area {area} > threshold {self.simple_area_threshold}")
        #         # return True
        
        # PLACEHOLDER: Remove this when you implement the loop above
        largest_detection = None  # Replace this
        largest_area = 0
        
        # TODO: Calculate area and compare to threshold
        # EXPERIMENT: Try different thresholds (start with 5000 pixels)
        # - Too low = stops too far away
        # - Too high = doesn't stop until very close
        
        area_threshold = self.simple_area_threshold  # Tune this parameter
        
        if largest_area > area_threshold:
            print(f"üõë STOPPING: Object area {largest_area} > threshold {area_threshold}")
            return True
        
        # APPROACH 2: DEPTH-BASED STOPPING (Advanced - implement after area works)
        # CONCEPT: Use actual relative distance measurements that come from our monocular depth estimation package
        
        # UNCOMMENT THIS SECTION WHEN USING THE ADVANCED APPROACH, also comment out the return statmeent above that's based on the area threshold
        # return self._advanced_depth_stopping(detected_signs, camera_frame)
        
        return False
    
    def _advanced_depth_stopping(self, detected_signs, camera_frame):
        """
        ADVANCED: DEPTH-BASED DISTANCE ESTIMATION
        =========================================
        
        CONCEPT: Get actual distance measurements using depth estimation
        
        THE PROCESS:
        1. Run depth estimation model on camera frame
        2. For each detected object, sample depth within its bounding box
        3. Calculate average/median depth ‚Üí real distance
        4. Stop if distance < threshold
        
        DEBUGGING INTEGRATION:
        - This function caches depth results for visualization
        - When you implement this, depth maps will appear in debug panel
        """
        
        if self.depth_estimator is None:
            print("‚ö†Ô∏è  Depth estimator not available, falling back to area-based method")
            return False
        
        try:
            # STEP 1: RUN DEPTH MODEL - YOU IMPLEMENT THIS LINE
            # TODO: Get depth map from MiDaS model
            # HINT: depth_map = self.depth_estimator.predict(camera_frame)
            
            depth_map = None  # TODO: Replace this line with actual depth inference
            
            if depth_map is None:
                return False  # Fallback to area-based method
            
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # DEBUGGING: Cache depth results for visualization 
            # UNCOMMENT the line below to see depth maps in debug panel
            # self._cached_depth_map = depth_map
            
            # STEP 2: SAMPLE DEPTH VALUES FOR EACH DETECTION
            # DISCOVERY QUESTION: Where in the depth map should you look?
            # ANSWER: Look at pixels inside the detected bounding boxes!
            
            # IMPORTANT: Check ALL detected objects, not just the largest one
            for detection in detected_signs:
                bbox = detection['bbox']  # [x, y, w, h]
                x, y, w, h = bbox
                
                # TODO: Extract depth values within bounding box
                # HINT: depth_region = depth_map[y:y+h, x:x+w]
                
                # DISCOVERY QUESTION: How do you convert depth map values to real distances?
                # EXPERIMENT: Print depth values and see what range they have
                # HINT: Smaller depth values often mean closer objects
                
                # TODO: Calculate representative depth (mean, median, minimum?)
                # representative_depth = np.mean(depth_region)  # or np.median, np.min
                
                # TODO: Convert to real-world distance if needed
                # estimated_distance = self._depth_to_distance(representative_depth)
                
                # TODO: Compare to distance threshold
                # if estimated_distance < self.depth_distance_threshold:
                #     console_logger.stop(f"üõë STOPPING: {detection['class_name']} at {estimated_distance:.1f}m < {self.depth_distance_threshold}m")
                #     return True
            
            return False  # No objects close enough to stop
            
        except Exception as e:
            print(f"Depth analysis error: {e}")
            return False
    
    def _convert_coordinates(self, center_x, center_y, width, height, original_shape):
        """
        Helper function: Convert model coordinates to image coordinates (provided)
        
        TECHNICAL DETAILS:
        - Convert center format [cx, cy, w, h] to corner format [x, y, w, h]
        - Scale from model size (640x640) back to original camera size (320x240)
        """
        orig_height, orig_width = original_shape[:2]
        
        # Convert center format to corner format
        x = center_x - width / 2
        y = center_y - height / 2
        
        # Scale coordinates from model size to original image size
        scale_x = orig_width / self.input_size
        scale_y = orig_height / self.input_size
        
        x = int(x * scale_x)
        y = int(y * scale_y)
        width = int(width * scale_x)
        height = int(height * scale_y)
        
        # Ensure coordinates are within image bounds
        x = max(0, min(x, orig_width - 1))
        y = max(0, min(y, orig_height - 1))
        width = min(width, orig_width - x)
        height = min(height, orig_height - y)
        
        return [x, y, width, height]

    def _depth_to_distance(self, depth_value):
        """
        Helper function: Convert depth map value to real-world distance (provided)
        
        TECHNICAL NOTE:
        This conversion depends on the specific depth model and calibration.
        MiDaS outputs relative depth, so this is a simplified conversion.
        """
        # Simple conversion - students can experiment with this
        # MiDaS typically outputs inverse depth, so smaller values = closer objects
        if depth_value <= 0:
            return float('inf')  # Invalid depth
        
        # Empirical conversion (you may need to calibrate this for your setup)
        estimated_distance = 1.0 / (depth_value + 0.1)  # Avoid division by zero
        return estimated_distance
    
    def _load_model(self):
        """Load the ONNX model (provided helper function)"""
        try:
            import onnxruntime as ort
            
            if os.path.exists(self.model_path):
                # Optimize for CPU performance
                providers = ['CPUExecutionProvider']
                sess_options = ort.SessionOptions()
                sess_options.inter_op_num_threads = 2
                sess_options.intra_op_num_threads = 2
                sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                self.session = ort.InferenceSession(
                    self.model_path, 
                    sess_options=sess_options,
                    providers=providers
                )
                
                self.input_name = self.session.get_inputs()[0].name
                self.input_shape = self.session.get_inputs()[0].shape
                
                print(f"‚úÖ ONNX model loaded: {self.model_path}")
                print(f"   Model input shape: {self.input_shape}")
                print(f"   Model classes: {self.class_names}")
            else:
                print(f"‚ùå Model file not found: {self.model_path}")
                self.session = None
                
        except ImportError:
            print("‚ùå ONNX Runtime not available. Please install: pip install onnxruntime")
            self.session = None
        except Exception as e:
            print(f"‚ùå Error loading ONNX model: {e}")
            self.session = None
    
    def _load_depth_model(self):
        """Try to load depth estimation model (optional for advanced section)"""
        try:
            # Students will install this when they reach the advanced section
            from midas_depth import DepthEstimator
            self.depth_estimator = DepthEstimator()
            print("‚úÖ Depth estimation model loaded (advanced features available)")
        except ImportError:
            self.depth_estimator = None
            print("‚ö™ Depth estimation not available (install midas_depth for advanced features)")
        except Exception as e:
            self.depth_estimator = None
            print(f"‚ö†Ô∏è  Depth model loading failed: {e}")
    
    def _preprocess_frame(self, camera_frame):
        """
        Preprocess camera frame for ONNX model input (provided helper function)
        
        TECHNICAL DETAILS (students don't need to implement this):
        - Resize 320x240 camera frame to 640x640 model input
        - Convert BGR to RGB color space
        - Normalize pixel values from 0-255 to 0.0-1.0
        - Rearrange dimensions for neural network format
        """
        if camera_frame is None:
            return None
            
        # Resize to model input size
        resized = cv2.resize(camera_frame, (self.input_size, self.input_size))
        
        # Convert to float and normalize
        input_tensor = resized.astype(np.float32)
        input_tensor /= 255.0
        
        # Rearrange dimensions: HWC ‚Üí CHW ‚Üí BCHW
        input_tensor = np.transpose(input_tensor, (2, 0, 1))  # HWC to CHW
        input_tensor = np.expand_dims(input_tensor, axis=0)   # Add batch dimension
        
        return input_tensor
    
    def get_cached_depth_map(self):
        """Return cached depth map for debug visualization (provided)"""
        return self._cached_depth_map
```


################################################################################
# FILE: week3_speed_estimation/calibration_script.py
################################################################################

```python
#!/usr/bin/env python3
"""
Week 3 Speed Estimation - Calibration Data Collection Script
===========================================================

This script automatically collects optical flow calibration data by:
1. Recording video while driving the robot at specified power for a known distance
2. Processing optical flow calculations offline from the recorded video
3. Calculating the relationship between optical flow and real speed
4. Saving results to cumulative CSV file

Usage:
    python calibration_script.py --dist 2.0 --pow 30
    python calibration_script.py --dist 2.5 --pow 40
    python calibration_script.py --dist 3.0 --pow 50
"""

import argparse
import time
import cv2
import numpy as np
import pandas as pd
import os
from datetime import datetime
from pathlib import Path
import sys

# Add the parent directory to Python path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import robot hardware modules
from core.camera_manager import camera
from core.utils.movement_controls import MovementController

class CalibrationCollector:
    def __init__(self):
        """Initialize calibration data collector"""
        
        # File paths
        self.csv_file = Path("calibration_data.csv")
        self.videos_dir = Path("videos")
        self.videos_dir.mkdir(exist_ok=True)
        
        # Hardware setup
        self.movement_controller = MovementController()
        
        # Ultrasonic sensor setup
        self.ultrasonic_stop_distance = 0.2  # meters from wall
        
        # Optical flow parameters
        self.feature_params = {
            'maxCorners': 100,
            'qualityLevel': 0.2,
            'minDistance': 10,
            'blockSize': 7
        }
        
        self.lk_params = {
            'winSize': (15, 15),
            'maxLevel': 2,
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Ensure CSV exists with headers
        self._initialize_csv()
        
        print("‚úÖ Calibration collector initialized")
        print(f"üìÅ Data will be saved to: {self.csv_file}")
        print(f"üé• Videos will be saved to: {self.videos_dir}/")

    def _initialize_csv(self):
        """Create CSV file with headers if it doesn't exist"""
        if not self.csv_file.exists():
            headers = [
                'run_id', 'timestamp', 'distance', 'motor_power', 
                'recorded_time', 'calculated_speed', 'avg_optical_flow', 
                'num_features', 'include_in_calibration',
                'cumulative_slope', 'cumulative_intercept', 'cumulative_r_squared', 'cumulative_n_points'
            ]
            
            df = pd.DataFrame(columns=headers)
            df.to_csv(self.csv_file, index=False)
            print(f"üìù Created new calibration data file: {self.csv_file}")
        else:
            print(f"üìñ Using existing calibration data file: {self.csv_file}")

    def get_ultrasonic_distance(self):
        """Get distance from PiCar-X ultrasonic sensor"""
        try:
            # Using PiCar-X ultrasonic sensor through movement controller
            distance = self.movement_controller.picar.ultrasonic.read()
            return distance / 100.0  # Convert cm to meters
        except Exception as e:
            print(f"Ultrasonic sensor error: {e}")
            return 1.0  # Default safe distance

    def wait_for_camera_ready(self):
        """Wait for camera to be properly initialized and streaming with real frames"""
        print("üì∑ Waiting for camera to be ready...")
        
        # First, explicitly start streaming if not already running
        if not camera.is_running:
            print("   Starting camera streaming...")
            camera.start_streaming()
            time.sleep(2)  # Give camera time to initialize
        
        # Wait for actual camera frames (not placeholder frames)
        max_attempts = 15
        for attempt in range(max_attempts):
            try:
                frame = camera.get_frame()
                if frame is not None and frame.shape[0] > 0 and frame.shape[1] > 0:
                    # Check if this is a real camera frame, not a placeholder
                    # Placeholder frames are solid color or have text - real frames have more variation
                    
                    # Convert to grayscale to check variance
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    variance = np.var(gray)
                    
                    # Real camera frames should have significant pixel variance
                    # Placeholder frames typically have very low variance
                    if variance > 100:  # Threshold for real vs placeholder frames
                        print("‚úÖ Camera ready with real frames")
                        print(f"   Frame variance: {variance:.1f} (good)")
                        return True
                    else:
                        print(f"   Attempt {attempt + 1}/{max_attempts} - placeholder frame detected (variance: {variance:.1f})")
                else:
                    print(f"   Attempt {attempt + 1}/{max_attempts} - no frame received")
            except Exception as e:
                print(f"   Attempt {attempt + 1}/{max_attempts} - error: {e}")
            
            time.sleep(1)
        
        print("‚ùå Camera failed to provide real frames")
        print("   Still getting placeholder frames after initialization")
        return False

    def record_video_during_movement(self, distance, motor_power):
        """Record video while robot moves, return video path and movement stats"""
        
        # Get next run ID
        if self.csv_file.exists():
            df = pd.read_csv(self.csv_file)
            run_id = len(df) + 1
        else:
            run_id = 1
        
        # Setup video recording
        video_filename = f"run_{run_id:03d}_{distance:.1f}m_{motor_power}pct_raw.mp4"
        video_path = self.videos_dir / video_filename
        
        # Make sure camera is ready
        if not self.wait_for_camera_ready():
            print("‚ùå Cannot proceed without camera")
            return None, None
        
        # Setup for data collection
        print(f"üéØ Position robot {distance}m from wall")
        print(f"üõë Robot will auto-stop at {self.ultrasonic_stop_distance}m from wall")
        input("üëÜ Press Enter when robot is positioned correctly...")
        
        # Verify initial distance
        initial_distance = self.get_ultrasonic_distance()
        print(f"üì° Initial distance reading: {initial_distance:.2f}m")
        
        if abs(initial_distance - distance) > 0.5:
            print(f"‚ö†Ô∏è  Warning: Expected {distance}m but reading {initial_distance:.2f}m")
            print("üìù Continuing with actual distance reading...")
        
        # Get first frame to setup video writer
        first_frame = camera.get_frame()
        if first_frame is None:
            print("‚ùå Cannot get camera frame")
            return None, None
            
        height, width = first_frame.shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        fps = 20
        
        out = cv2.VideoWriter(str(video_path), fourcc, fps, (width, height))
        
        print(f"üé¨ Starting video recording...")
        print(f"üöÄ Starting motor at {motor_power}% power...")
        
        # Start robot movement
        self.movement_controller.picar.set_dir_servo_angle(0)  # Straight
        self.movement_controller.picar.forward(motor_power)
        self.movement_controller.picar.set_cam_tilt_angle(-10)
        
        start_time = time.time()
        frame_count = 0
        
        try:
            while True:
                # Get current frame
                current_frame = camera.get_frame()
                if current_frame is not None:
                    # Record raw frame without any text overlays
                    out.write(current_frame)
                    frame_count += 1
                
                # Check if we should stop (ultrasonic sensor)
                current_distance = self.get_ultrasonic_distance()
                if current_distance <= self.ultrasonic_stop_distance:
                    print(f"üõë Stopping - reached {current_distance:.2f}m from wall")
                    break
                
                # Safety timeout
                current_time = time.time() - start_time
                if current_time > 15.0:  # 15 second max run time
                    print("‚è∞ Timeout - stopping for safety")
                    break
                
                time.sleep(0.05)  # 20 FPS recording
                
        finally:
            # Stop the robot and close video
            self.movement_controller.picar.stop()
            out.release()
            print("‚úã Robot stopped")
        
        # Calculate movement results
        total_time = time.time() - start_time
        actual_distance = distance - self.ultrasonic_stop_distance
        calculated_speed = actual_distance / total_time
        
        movement_stats = {
            'run_id': run_id,
            'total_time': total_time,
            'actual_distance': actual_distance,
            'calculated_speed': calculated_speed,
            'frame_count': frame_count
        }
        
        print(f"üìä MOVEMENT RESULTS:")
        print(f"   Time: {total_time:.2f}s")
        print(f"   Distance: {actual_distance:.2f}m")
        print(f"   Speed: {calculated_speed:.3f} m/s")
        print(f"   Frames recorded: {frame_count}")
        print(f"üé• Raw video saved: {video_path}")
        
        return video_path, movement_stats

    def calculate_optical_flow(self, prev_frame, curr_frame):
        """Calculate optical flow magnitude between two frames"""
        
        # Convert to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        
        # Detect features in previous frame
        features = cv2.goodFeaturesToTrack(prev_gray, **self.feature_params)
        
        if features is None or len(features) == 0:
            return 0.0, 0, []
        
        # Track features using Lucas-Kanade
        new_features, status, error = cv2.calcOpticalFlowPyrLK(
            prev_gray, curr_gray, features, None, **self.lk_params)
        
        # Calculate flow magnitudes for successfully tracked features
        flow_magnitudes = []
        good_features = []
        
        for i, (prev_pt, curr_pt) in enumerate(zip(features, new_features)):
            if status[i] == 1:  # Successfully tracked
                dx = curr_pt[0][0] - prev_pt[0][0]
                dy = curr_pt[0][1] - prev_pt[0][1]
                magnitude = np.sqrt(dx*dx + dy*dy)
                flow_magnitudes.append(magnitude)
                good_features.append((prev_pt, curr_pt, magnitude))
        
        avg_flow = np.mean(flow_magnitudes) if flow_magnitudes else 0.0
        return avg_flow, len(good_features), good_features

    def create_flow_visualization(self, frame, features_data):
        """Create visualization of optical flow on frame"""
        vis_frame = frame.copy()
        
        # Draw tracked features and flow vectors
        for prev_pt, curr_pt, magnitude in features_data:
            # Draw feature points
            cv2.circle(vis_frame, tuple(prev_pt[0].astype(int)), 3, (0, 255, 0), -1)
            cv2.circle(vis_frame, tuple(curr_pt[0].astype(int)), 3, (0, 0, 255), -1)
            
            # Draw flow vector
            cv2.arrowedLine(vis_frame, 
                          tuple(prev_pt[0].astype(int)), 
                          tuple(curr_pt[0].astype(int)), 
                          (255, 0, 255), 2, tipLength=0.3)
        
        return vis_frame

    def process_optical_flow_from_video(self, video_path, movement_stats):
        """Process optical flow from recorded video"""
        
        print(f"üîÑ Processing optical flow from video...")
        
        # Open video file
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print("‚ùå Could not open video file")
            return None
        
        # Setup output video with flow visualization
        flow_video_path = video_path.parent / video_path.name.replace('_raw.mp4', '_flow.mp4')
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(flow_video_path), fourcc, fps, (width, height))
        
        # Process frames
        flow_data = []
        prev_frame = None
        frame_number = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_number += 1
            
            if prev_frame is not None:
                # Calculate optical flow
                flow_mag, num_features, features_list = self.calculate_optical_flow(prev_frame, frame)
                
                if flow_mag > 0:
                    flow_data.append({
                        'flow_magnitude': flow_mag,
                        'num_features': num_features,
                        'frame_number': frame_number
                    })
                
                # Create visualization
                vis_frame = self.create_flow_visualization(frame, features_list)
                
                # Add flow info to frame
                cv2.putText(vis_frame, f"Flow: {flow_mag:.1f} px/frame", 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(vis_frame, f"Features: {num_features}", 
                           (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                out.write(vis_frame)
            else:
                # First frame - just write as is
                out.write(frame)
            
            prev_frame = frame
        
        cap.release()
        out.release()
        
        # Calculate average optical flow
        if flow_data:
            avg_optical_flow = np.mean([f['flow_magnitude'] for f in flow_data])
            avg_features = np.mean([f['num_features'] for f in flow_data])
        else:
            avg_optical_flow = 0.0
            avg_features = 0
            print("‚ö†Ô∏è  No optical flow data calculated!")
        
        print(f"‚úÖ Optical flow processing complete")
        print(f"   Average flow: {avg_optical_flow:.2f} px/frame")
        print(f"   Average features: {avg_features:.0f}")
        print(f"üé• Flow visualization saved: {flow_video_path}")
        
        return {
            'avg_optical_flow': avg_optical_flow,
            'num_features': avg_features,
            'flow_video_path': flow_video_path
        }

    def run_calibration(self, distance, motor_power):
        """Run a single calibration data collection sequence"""
        
        print(f"\n{'='*50}")
        print(f"üöó CALIBRATION RUN")
        print(f"üìè Target distance: {distance}m")
        print(f"‚ö° Motor power: {motor_power}%")
        print(f"{'='*50}")
        
        # Step 1: Record video during movement
        video_path, movement_stats = self.record_video_during_movement(distance, motor_power)
        
        if video_path is None or movement_stats is None:
            print("‚ùå Video recording failed")
            return
        
        # Step 2: Process optical flow from recorded video
        flow_results = self.process_optical_flow_from_video(video_path, movement_stats)
        
        if flow_results is None:
            print("‚ùå Optical flow processing failed")
            return
        
        # Step 3: Save results to CSV
        self._save_to_csv(
            movement_stats['run_id'],
            distance,
            motor_power,
            movement_stats['total_time'],
            movement_stats['calculated_speed'],
            flow_results['avg_optical_flow'],
            flow_results['num_features']
        )
        
        print(f"üíæ Data saved to CSV")

    def _save_to_csv(self, run_id, distance, motor_power, recorded_time, 
                     calculated_speed, avg_optical_flow, num_features):
        """Save run data to CSV and calculate cumulative statistics"""
        
        # Create new row data
        new_data = {
            'run_id': run_id,
            'timestamp': datetime.now().isoformat(),
            'distance': distance,
            'motor_power': motor_power,
            'recorded_time': recorded_time,
            'calculated_speed': calculated_speed,
            'avg_optical_flow': avg_optical_flow,
            'num_features': num_features,
            'include_in_calibration': True
        }
        
        # Load existing data
        if self.csv_file.exists():
            df = pd.read_csv(self.csv_file)
        else:
            df = pd.DataFrame()
        
        # Add new row
        new_row = pd.DataFrame([new_data])
        df = pd.concat([df, new_row], ignore_index=True)
        
        # Calculate cumulative statistics
        calibration_data = df[df['include_in_calibration'] == True]
        
        if len(calibration_data) >= 2:
            flows = calibration_data['avg_optical_flow'].values
            speeds = calibration_data['calculated_speed'].values
            
            # Linear regression: speed = slope * flow + intercept
            slope, intercept = np.polyfit(flows, speeds, 1)
            
            # Calculate R-squared
            predicted_speeds = slope * flows + intercept
            ss_res = np.sum((speeds - predicted_speeds) ** 2)
            ss_tot = np.sum((speeds - np.mean(speeds)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            n_points = len(calibration_data)
        else:
            slope, intercept, r_squared, n_points = 0, 0, 0, len(calibration_data)
        
        # Update the last row with cumulative stats
        df.loc[df.index[-1], 'cumulative_slope'] = slope
        df.loc[df.index[-1], 'cumulative_intercept'] = intercept
        df.loc[df.index[-1], 'cumulative_r_squared'] = r_squared
        df.loc[df.index[-1], 'cumulative_n_points'] = n_points
        
        # Save CSV
        df.to_csv(self.csv_file, index=False)
        
        # Print cumulative results
        print(f"\nüîÑ CUMULATIVE CALIBRATION ({n_points} runs):")
        if n_points >= 2:
            print(f"   Linear fit: speed = {slope:.6f} √ó flow + ({intercept:.6f})")
            print(f"   R-squared: {r_squared:.3f}")
            print(f"\nüìã Copy to speed_estimator.py:")
            print(f"   self.flow_to_speed_slope = {slope:.6f}")
            print(f"   self.flow_to_speed_intercept = {intercept:.6f}")
        else:
            print(f"   Need at least 2 runs for calibration")


def main():
    parser = argparse.ArgumentParser(description='Collect optical flow calibration data')
    parser.add_argument('--dist', type=float, required=True,
                       help='Distance from wall in meters (e.g., 2.0)')
    parser.add_argument('--pow', type=int, required=True,
                       help='Motor power percentage (e.g., 30)')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.dist <= 0 or args.dist > 5:
        print("‚ùå Error: Distance must be between 0 and 5 meters")
        return
    
    if args.pow <= 0 or args.pow > 100:
        print("‚ùå Error: Power must be between 1 and 100 percent")
        return
    
    # Run calibration
    collector = CalibrationCollector()
    collector.run_calibration(args.dist, args.pow)


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week3_speed_estimation/show_stats.py
################################################################################

```python
#!/usr/bin/env python3
"""
Week 3 Speed Estimation - Calibration Statistics Viewer
=======================================================

This script displays current calibration statistics without running the robot.
Shows individual run data and cumulative calibration parameters.

Usage:
    python show_stats.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import sys

class CalibrationStatsViewer:
    def __init__(self):
        """Initialize calibration statistics viewer"""
        self.csv_file = Path("calibration_data.csv")
        
        if not self.csv_file.exists():
            print("‚ùå No calibration data found!")
            print("   Run calibration_script.py first to collect data")
            sys.exit(1)
        
        # Load data
        self.df = pd.read_csv(self.csv_file)
        print(f"üìñ Loaded calibration data from: {self.csv_file}")

    def show_summary_stats(self):
        """Display summary statistics"""
        total_runs = len(self.df)
        calibration_runs = len(self.df[self.df['include_in_calibration'] == True])
        
        print(f"\n{'='*60}")
        print(f"üìä CALIBRATION STATISTICS SUMMARY")
        print(f"{'='*60}")
        print(f"Total runs: {total_runs}")
        print(f"Runs used for calibration: {calibration_runs}")
        
        if total_runs > 0:
            last_updated = self.df['timestamp'].iloc[-1]
            print(f"Last updated: {last_updated}")

    def show_current_calibration(self):
        """Display current calibration parameters"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) < 2:
            print(f"\n‚ö†Ô∏è  INSUFFICIENT DATA FOR CALIBRATION")
            print(f"   Need at least 2 runs, currently have {len(calibration_data)}")
            return
        
        # Get latest cumulative stats
        latest_row = self.df.iloc[-1]
        slope = latest_row['cumulative_slope']
        intercept = latest_row['cumulative_intercept']
        r_squared = latest_row['cumulative_r_squared']
        n_points = int(latest_row['cumulative_n_points'])
        
        print(f"\nüéØ CURRENT CALIBRATION:")
        print(f"   speed = {slope:.6f} √ó flow + ({intercept:.6f})")
        print(f"   R-squared: {r_squared:.3f}")
        print(f"   Based on {n_points} data points")
        
        # Quality assessment
        if r_squared >= 0.95:
            quality = "Excellent! üåü"
        elif r_squared >= 0.90:
            quality = "Very Good ‚úÖ"
        elif r_squared >= 0.80:
            quality = "Good üëç"
        elif r_squared >= 0.70:
            quality = "Fair ‚ö†Ô∏è"
        else:
            quality = "Poor - collect more data ‚ùå"
        
        print(f"   Calibration quality: {quality}")
        
        print(f"\nüìã COPY TO SPEED_ESTIMATOR.PY:")
        print(f"   self.flow_to_speed_slope = {slope:.6f}")
        print(f"   self.flow_to_speed_intercept = {intercept:.6f}")

    def show_individual_runs(self):
        """Display individual run data"""
        print(f"\nüìù INDIVIDUAL RUN DATA:")
        print(f"{'Run':<4} {'Distance':<8} {'Power':<6} {'Speed':<8} {'Flow':<8} {'Features':<9} {'Include'}")
        print(f"{'-'*60}")
        
        for _, row in self.df.iterrows():
            run_id = int(row['run_id'])
            distance = row['distance']
            power = int(row['motor_power'])
            speed = row['calculated_speed']
            flow = row['avg_optical_flow']
            features = int(row['num_features'])
            include = "‚úì" if row['include_in_calibration'] else "‚úó"
            
            print(f"{run_id:<4} {distance:<8.1f} {power:<6}% {speed:<8.3f} {flow:<8.1f} {features:<9} {include}")

    def show_data_distribution(self):
        """Show data distribution statistics"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) == 0:
            return
        
        print(f"\nüìà DATA DISTRIBUTION:")
        
        # Speed range
        speed_min = calibration_data['calculated_speed'].min()
        speed_max = calibration_data['calculated_speed'].max()
        speed_mean = calibration_data['calculated_speed'].mean()
        
        print(f"   Speed range: {speed_min:.3f} - {speed_max:.3f} m/s (avg: {speed_mean:.3f})")
        
        # Flow range
        flow_min = calibration_data['avg_optical_flow'].min()
        flow_max = calibration_data['avg_optical_flow'].max()
        flow_mean = calibration_data['avg_optical_flow'].mean()
        
        print(f"   Flow range: {flow_min:.1f} - {flow_max:.1f} px/frame (avg: {flow_mean:.1f})")
        
        # Distance and power coverage
        distances = calibration_data['distance'].unique()
        powers = calibration_data['motor_power'].unique()
        
        print(f"   Distances tested: {sorted(distances)} meters")
        print(f"   Power levels tested: {sorted(powers)}%")

    def create_scatter_plot(self, save_plot=False):
        """Create scatter plot of flow vs speed"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) < 2:
            print("‚ö†Ô∏è  Not enough data for scatter plot")
            return
        
        flows = calibration_data['avg_optical_flow'].values
        speeds = calibration_data['calculated_speed'].values
        
        # Get calibration line
        latest_row = self.df.iloc[-1]
        slope = latest_row['cumulative_slope']
        intercept = latest_row['cumulative_intercept']
        
        # Create plot
        plt.figure(figsize=(10, 6))
        plt.scatter(flows, speeds, c='blue', alpha=0.7, s=100, edgecolors='black')
        
        # Plot calibration line
        flow_range = np.linspace(flows.min() * 0.9, flows.max() * 1.1, 100)
        speed_line = slope * flow_range + intercept
        plt.plot(flow_range, speed_line, 'r--', linewidth=2, 
                label=f'Calibration: speed = {slope:.4f} √ó flow + {intercept:.4f}')
        
        plt.xlabel('Optical Flow (pixels/frame)')
        plt.ylabel('Speed (m/s)')
        plt.title('Optical Flow vs Speed Calibration')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add R¬≤ to plot
        r_squared = latest_row['cumulative_r_squared']
        plt.text(0.05, 0.95, f'R¬≤ = {r_squared:.3f}', transform=plt.gca().transAxes,
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        if save_plot:
            plot_path = Path("calibration_plot.png")
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            print(f"üìä Plot saved to: {plot_path}")
        
        plt.show()

    def show_recommendations(self):
        """Show recommendations for improving calibration"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        print(f"\nüí° RECOMMENDATIONS:")
        
        if len(calibration_data) < 3:
            print("   ‚Ä¢ Collect more data points (aim for 5-8 runs)")
        
        if len(calibration_data) > 0:
            # Check speed range
            speed_range = calibration_data['calculated_speed'].max() - calibration_data['calculated_speed'].min()
            if speed_range < 0.3:
                print("   ‚Ä¢ Try more varied motor power settings for wider speed range")
            
            # Check R-squared
            if len(calibration_data) >= 2:
                r_squared = self.df.iloc[-1]['cumulative_r_squared']
                if r_squared < 0.90:
                    print("   ‚Ä¢ Consider collecting more data to improve fit quality")
                    print("   ‚Ä¢ Check videos for consistent feature tracking")
        
        # Check distance variety
        distances = calibration_data['distance'].unique() if len(calibration_data) > 0 else []
        if len(distances) < 2:
            print("   ‚Ä¢ Test multiple distance settings (2.0m, 2.5m, 3.0m)")
        
        # Check power variety
        powers = calibration_data['motor_power'].unique() if len(calibration_data) > 0 else []
        if len(powers) < 2:
            print("   ‚Ä¢ Test multiple power settings (30%, 40%, 50%)")

    def toggle_run_inclusion(self, run_id):
        """Toggle whether a specific run is included in calibration"""
        if run_id < 1 or run_id > len(self.df):
            print(f"‚ùå Invalid run ID: {run_id}")
            return
        
        # Toggle inclusion status
        current_status = self.df.loc[self.df['run_id'] == run_id, 'include_in_calibration'].iloc[0]
        new_status = not current_status
        self.df.loc[self.df['run_id'] == run_id, 'include_in_calibration'] = new_status
        
        # Recalculate cumulative statistics
        self._recalculate_cumulative_stats()
        
        # Save updated CSV
        self.df.to_csv(self.csv_file, index=False)
        
        status_text = "included" if new_status else "excluded"
        print(f"‚úÖ Run {run_id} {status_text} from calibration")

    def _recalculate_cumulative_stats(self):
        """Recalculate cumulative statistics after toggling run inclusion"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) >= 2:
            flows = calibration_data['avg_optical_flow'].values
            speeds = calibration_data['calculated_speed'].values
            
            # Linear regression
            slope, intercept = np.polyfit(flows, speeds, 1)
            
            # Calculate R-squared
            predicted_speeds = slope * flows + intercept
            ss_res = np.sum((speeds - predicted_speeds) ** 2)
            ss_tot = np.sum((speeds - np.mean(speeds)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            n_points = len(calibration_data)
        else:
            slope, intercept, r_squared, n_points = 0, 0, 0, len(calibration_data)
        
        # Update all rows with new cumulative stats
        self.df['cumulative_slope'] = slope
        self.df['cumulative_intercept'] = intercept
        self.df['cumulative_r_squared'] = r_squared
        self.df['cumulative_n_points'] = n_points

    def interactive_menu(self):
        """Display interactive menu for data management"""
        while True:
            print(f"\n{'='*40}")
            print("üìã CALIBRATION DATA MANAGER")
            print(f"{'='*40}")
            print("1. Show summary")
            print("2. Show current calibration")
            print("3. Show individual runs")
            print("4. Show data distribution")
            print("5. Create scatter plot")
            print("6. Toggle run inclusion")
            print("7. Show recommendations")
            print("8. Exit")
            
            try:
                choice = input("\nSelect option (1-8): ").strip()
                
                if choice == '1':
                    self.show_summary_stats()
                elif choice == '2':
                    self.show_current_calibration()
                elif choice == '3':
                    self.show_individual_runs()
                elif choice == '4':
                    self.show_data_distribution()
                elif choice == '5':
                    try:
                        import matplotlib.pyplot as plt
                        self.create_scatter_plot(save_plot=True)
                    except ImportError:
                        print("‚ö†Ô∏è  Matplotlib not available for plotting")
                elif choice == '6':
                    self.show_individual_runs()
                    try:
                        run_id = int(input("Enter run ID to toggle: "))
                        self.toggle_run_inclusion(run_id)
                    except ValueError:
                        print("‚ùå Please enter a valid run ID number")
                elif choice == '7':
                    self.show_recommendations()
                elif choice == '8':
                    print("üëã Goodbye!")
                    break
                else:
                    print("‚ùå Invalid choice. Please select 1-8.")
                    
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")


def main():
    """Main function - show all stats by default or interactive mode"""
    import sys
    
    viewer = CalibrationStatsViewer()
    
    # If run with --interactive flag, show menu
    if len(sys.argv) > 1 and sys.argv[1] == '--interactive':
        viewer.interactive_menu()
    else:
        # Default: show all statistics
        viewer.show_summary_stats()
        viewer.show_current_calibration()
        viewer.show_individual_runs()
        viewer.show_data_distribution()
        viewer.show_recommendations()


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week3_speed_estimation/speed_estimator.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import json
import os

class SpeedEstimator:
    """
    Week 3 Implementation: Optical Flow Speed Estimation
    
    Students will implement optical flow analysis to estimate robot speed
    from camera frames, following the Lucas-Kanade method from the academic paper.
    """
    
    def __init__(self):
        """Initialize optical flow parameters and load calibration"""
        
        # Optical flow parameters (students may need to tune these)
        self.feature_params = {
            'maxCorners': 100,      # Maximum number of features to track
            'qualityLevel': 0.3,    # Quality level for corner detection
            'minDistance': 7,       # Minimum distance between features
            'blockSize': 7          # Size of averaging block for corner detection
        }
        
        self.lk_params = {
            'winSize': (15, 15),    # Window size for Lucas-Kanade
            'maxLevel': 2,          # Maximum pyramid levels
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Speed calibration parameters (loaded from calibration file)
        self.calibration_loaded = False
        self.flow_to_speed_slope = 1.0
        self.flow_to_speed_intercept = 0.0
        
        # Internal state for tracking
        self.previous_gray = None
        self.previous_features = None

        self.speed_history = []
        self.max_history_length = 10
        
        # Exponential smoothing for speed
        self.smoothed_speed = 0.0
        self.alpha = 0.3  # Smoothing factor: 0.1=very smooth, 0.9=very responsive

        # Load calibration parameters
        self._load_calibration()
        
        print("SpeedEstimator initialized - Students: Implement estimate_speed()!")
    
    def _load_calibration(self):
        """Load speed calibration parameters from file"""
        try:
            calibration_file = os.path.join(os.path.dirname(__file__), 'calibration_params.json')
            
            if os.path.exists(calibration_file):
                with open(calibration_file, 'r') as f:
                    params = json.load(f)
                
                self.flow_to_speed_slope = params.get('slope', 1.0)
                self.flow_to_speed_intercept = params.get('intercept', 0.0)
                self.calibration_loaded = True
                
                print(f"‚úÖ Calibration loaded: slope={self.flow_to_speed_slope:.3f}, intercept={self.flow_to_speed_intercept:.3f}")
            else:
                print(f"‚ö†Ô∏è  No calibration file found: {calibration_file}")
                print("   Using default parameters. Run speed calibration first!")
                
        except Exception as e:
            print(f"‚ùå Error loading calibration: {e}")
    
    def estimate_speed(self, current_frame, previous_frame):
        """
        Main speed estimation function students must implement
        
        Args:
            current_frame: numpy array of shape (240, 320, 3) - current RGB image
            previous_frame: numpy array of shape (240, 320, 3) - previous RGB image
            
        Returns:
            speed: float - estimated speed in units/second (calibrated)
        """
        
        if previous_frame is None:
            return 0.0
        
        try:
            # =============================================================
            # STEP 1: CONVERT TO GRAYSCALE (following paper section 1.3)
            # Optical flow works on grayscale images
            # =============================================================
            
            # Convert both frames to grayscale using cv2.cvtColor()
            current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
            previous_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
            
            # =============================================================
            # STEP 2: FEATURE DETECTION (following paper)
            # Find good features to track in the previous frame
            # =============================================================
            
            # Use cv2.goodFeaturesToTrack() to find corner features
            # This identifies distinct points that can be reliably tracked
            features_prev = cv2.goodFeaturesToTrack(
                previous_gray,
                maxCorners=self.feature_params['maxCorners'],
                qualityLevel=self.feature_params['qualityLevel'],
                minDistance=self.feature_params['minDistance'],
                blockSize=self.feature_params['blockSize']
            )
            
            if features_prev is None or len(features_prev) == 0:
                return 0.0  # No features to track
            
            # =============================================================
            # STEP 3: OPTICAL FLOW CALCULATION (following paper section 1.1)
            # Use Lucas-Kanade method to track features between frames
            # =============================================================
            
            # Use cv2.calcOpticalFlowPyrLK() to track features from previous to current frame
            # This implements the Lucas-Kanade algorithm with pyramids
            features_curr, status, error = cv2.calcOpticalFlowPyrLK(
                previous_gray,
                current_gray,
                features_prev,
                None,
                **self.lk_params
            )
            
            # =============================================================
            # STEP 4: FILTER GOOD FEATURES
            # Only use features that were successfully tracked
            # =============================================================
            
            if features_curr is None or status is None:
                return 0.0
            
            # Filter out features where tracking failed (status == 0)
            # Keep only good features for flow calculation
            good_prev = []
            good_curr = []
            
            for i, (status_flag, err) in enumerate(zip(status, error)):
                if status_flag == 1:  # Good tracking
                    good_prev.append(features_prev[i])
                    good_curr.append(features_curr[i])
            
            if len(good_prev) < 10:  # Need minimum features for reliable estimation
                return 0.0
            
            # =============================================================
            # STEP 5: CALCULATE FLOW MAGNITUDE
            # Compute the magnitude of optical flow vectors
            # =============================================================
            
            # For each tracked feature, calculate the displacement vector
            # Compute the magnitude (length) of each displacement
            # Take the average magnitude as the overall flow measure
            flow_magnitudes = []
            
            # Calculate flow magnitude for each good feature pair
            for prev_pt, curr_pt in zip(good_prev, good_curr):
                dx = curr_pt[0][0] - prev_pt[0][0]
                dy = curr_pt[0][1] - prev_pt[0][1]
                magnitude = np.sqrt(dx*dx + dy*dy)
                flow_magnitudes.append(magnitude)
            
            if not flow_magnitudes:
                return 0.0
            
            # Average flow magnitude
            avg_flow_magnitude = np.mean(flow_magnitudes)
            
            # =============================================================
            # STEP 6: CONVERT FLOW TO SPEED (using calibration)
            # Apply calibration parameters to get real-world speed
            # =============================================================
            
            # Apply linear calibration: speed = slope * flow + intercept
            # This converts pixel flow to real-world speed units
            if self.calibration_loaded:
                speed = self.flow_to_speed_slope * avg_flow_magnitude + self.flow_to_speed_intercept
            else:
                # Without calibration, just return raw flow magnitude
                speed = avg_flow_magnitude
            
            # Ensure speed is non-negative
            speed = max(0.0, speed)

            # Apply exponential smoothing (only smooth valid readings)
            if speed > 0.02:  # Only smooth speeds above 0.02 m/s (noise threshold)
                self.smoothed_speed = self.alpha * speed + (1 - self.alpha) * self.smoothed_speed
            elif speed < 0.01:  # If speed is very low, gradually decay smoothed speed
                self.smoothed_speed *= 0.95  # Slow decay when stopped

            # Store speed in history for smoothing
            self.speed_history.append(speed)
            if len(self.speed_history) > self.max_history_length:
                self.speed_history.pop(0)
            
            return float(speed)
            
        except Exception as e:
            print(f"Speed estimation error: {e}")
            return 0.0
    
    def is_calibrated(self):
        """Check if speed calibration parameters are available"""
        return self.calibration_loaded
    
    def get_speed_history(self):
        """Get speed history for web interface and smoothing"""
        if not self.speed_history:
            return {
                'current_speed': 0.0,
                'smoothed_speed': 0.0,
                'speed_history': [],
                'avg_speed': 0.0
            }
        
        return {
            'current_speed': self.speed_history[-1],
            'smoothed_speed': self.smoothed_speed,  # ‚Üê Fixed: added 'self.'
            'speed_history': self.speed_history.copy(),
            'avg_speed': sum(self.speed_history) / len(self.speed_history)
        }

# =============================================================================
# HELPFUL REFERENCE CODE (for students to adapt)
# =============================================================================

"""
EXAMPLE FEATURE DETECTION:
features = cv2.goodFeaturesToTrack(
    gray_image,
    maxCorners=100,
    qualityLevel=0.3,
    minDistance=7,
    blockSize=7
)

EXAMPLE OPTICAL FLOW:
features_next, status, error = cv2.calcOpticalFlowPyrLK(
    old_gray, 
    new_gray, 
    features_prev, 
    None, 
    **lk_params
)

EXAMPLE FLOW MAGNITUDE CALCULATION:
for i, (prev_pt, curr_pt) in enumerate(zip(good_prev, good_curr)):
    if status[i] == 1:  # Successfully tracked
        dx = curr_pt[0][0] - prev_pt[0][0]
        dy = curr_pt[0][1] - prev_pt[0][1]
        magnitude = np.sqrt(dx*dx + dy*dy)
        flow_magnitudes.append(magnitude)

avg_flow = np.mean(flow_magnitudes)
"""
```


################################################################################
# FILE: week3_speed_estimation/speed_estimator_skeleton.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import json
import os

class SpeedEstimator:
    """
    Week 3 Implementation: Optical Flow Speed Estimation
    
    Students will implement optical flow analysis to estimate robot speed
    from camera frames, following the Lucas-Kanade method from the academic paper.
    """
    
    def __init__(self):
        """Initialize optical flow parameters and load calibration"""
        
        # Optical flow parameters (students may need to tune these)
        self.feature_params = {
            'maxCorners': 100,      # Maximum number of features to track
            'qualityLevel': 0.3,    # Quality level for corner detection
            'minDistance': 7,       # Minimum distance between features
            'blockSize': 7          # Size of averaging block for corner detection
        }
        
        self.lk_params = {
            'winSize': (15, 15),    # Window size for Lucas-Kanade
            'maxLevel': 2,          # Maximum pyramid levels
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Speed calibration parameters (loaded from calibration file)
        self.calibration_loaded = False
        self.flow_to_speed_slope = 1.0
        self.flow_to_speed_intercept = 0.0
        
        # Internal state for tracking
        self.previous_gray = None
        self.previous_features = None

        self.speed_history = []
        self.max_history_length = 10

        # Load calibration parameters
        self._load_calibration()
        
        print("SpeedEstimator initialized - Students: Implement estimate_speed()!")
    
    def _load_calibration(self):
        """Load speed calibration parameters from file"""
        try:
            calibration_file = os.path.join(os.path.dirname(__file__), 'calibration_params.json')
            
            if os.path.exists(calibration_file):
                with open(calibration_file, 'r') as f:
                    params = json.load(f)
                
                self.flow_to_speed_slope = params.get('slope', 1.0)
                self.flow_to_speed_intercept = params.get('intercept', 0.0)
                self.calibration_loaded = True
                
                print(f"‚úÖ Calibration loaded: slope={self.flow_to_speed_slope:.3f}, intercept={self.flow_to_speed_intercept:.3f}")
            else:
                print(f"‚ö†Ô∏è  No calibration file found: {calibration_file}")
                print("   Using default parameters. Run speed calibration first!")
                
        except Exception as e:
            print(f"‚ùå Error loading calibration: {e}")
    
    def estimate_speed(self, current_frame, previous_frame):
        """
        Main speed estimation function students must implement
        
        Args:
            current_frame: numpy array of shape (240, 320, 3) - current RGB image
            previous_frame: numpy array of shape (240, 320, 3) - previous RGB image
            
        Returns:
            speed: float - estimated speed in units/second (calibrated)
        """
        
        if previous_frame is None:
            return 0.0
        
        try:
            # =============================================================
            # STEP 1: CONVERT TO GRAYSCALE (following paper section 1.3)
            # Optical flow works on grayscale images
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Convert both frames to grayscale using cv2.cvtColor()
            
            current_gray = None   # TODO: Convert current frame to grayscale
            previous_gray = None  # TODO: Convert previous frame to grayscale
            
            # =============================================================
            # STEP 2: FEATURE DETECTION (following paper)
            # Find good features to track in the previous frame
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Use cv2.goodFeaturesToTrack() to find corner features
            # This identifies distinct points that can be reliably tracked
            
            features_prev = None  # TODO: Detect features in previous frame
            
            if features_prev is None or len(features_prev) == 0:
                return 0.0  # No features to track
            
            # =============================================================
            # STEP 3: OPTICAL FLOW CALCULATION (following paper section 1.1)
            # Use Lucas-Kanade method to track features between frames
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Use cv2.calcOpticalFlowPyrLK() to track features from previous to current frame
            # This implements the Lucas-Kanade algorithm with pyramids
            
            features_curr = None  # TODO: Track features using Lucas-Kanade
            status = None         # TODO: Get tracking status for each feature
            error = None          # TODO: Get tracking error for each feature
            
            # =============================================================
            # STEP 4: FILTER GOOD FEATURES
            # Only use features that were successfully tracked
            # =============================================================
            
            if features_curr is None or status is None:
                return 0.0
            
            # STUDENTS IMPLEMENT:
            # Filter out features where tracking failed (status == 0)
            # Keep only good features for flow calculation
            
            good_prev = []  # TODO: Filter previous features
            good_curr = []  # TODO: Filter current features
            
            # for i, (status_flag, err) in enumerate(zip(status, error)):
            #     if status_flag == 1:  # Good tracking
            #         good_prev.append(features_prev[i])
            #         good_curr.append(features_curr[i])
            
            if len(good_prev) < 10:  # Need minimum features for reliable estimation
                return 0.0
            
            # =============================================================
            # STEP 5: CALCULATE FLOW MAGNITUDE
            # Compute the magnitude of optical flow vectors
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # For each tracked feature, calculate the displacement vector
            # Compute the magnitude (length) of each displacement
            # Take the average magnitude as the overall flow measure
            
            flow_magnitudes = []
            
            # TODO: Calculate flow magnitude for each good feature pair
            # for prev_pt, curr_pt in zip(good_prev, good_curr):
            #     dx = curr_pt[0] - prev_pt[0]
            #     dy = curr_pt[1] - prev_pt[1]
            #     magnitude = np.sqrt(dx*dx + dy*dy)
            #     flow_magnitudes.append(magnitude)
            
            if not flow_magnitudes:
                return 0.0
            
            # Average flow magnitude
            avg_flow_magnitude = np.mean(flow_magnitudes)
            
            # =============================================================
            # STEP 6: CONVERT FLOW TO SPEED (using calibration)
            # Apply calibration parameters to get real-world speed
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Apply linear calibration: speed = slope * flow + intercept
            # This converts pixel flow to real-world speed units
            
            if self.calibration_loaded:
                speed = self.flow_to_speed_slope * avg_flow_magnitude + self.flow_to_speed_intercept
            else:
                # Without calibration, just return raw flow magnitude
                speed = avg_flow_magnitude
            
            # Ensure speed is non-negative
            speed = max(0.0, speed)

            self.speed_history.append(speed)
            if len(self.speed_history) > self.max_history_length:
                self.speed_history.pop(0)
            
            return float(speed)
            
        except Exception as e:
            print(f"Speed estimation error: {e}")
            return 0.0
    
    def is_calibrated(self):
        """Check if speed calibration parameters are available"""
        return self.calibration_loaded
    
    def get_speed_history(self):
        """Get speed history for web interface and smoothing"""
        if not self.speed_history:
            return {
                'current_speed': 0.0,
                'smoothed_speed': 0.0,
                'speed_history': [],
                'avg_speed': 0.0
            }
        
        # Calculate smoothed speed (average of last 3 readings)
        smoothing_window = min(3, len(self.speed_history))
        recent_speeds = self.speed_history[-smoothing_window:]
        smoothed_speed = sum(recent_speeds) / len(recent_speeds)
        
        return {
            'current_speed': self.speed_history[-1],
            'smoothed_speed': smoothed_speed,
            'speed_history': self.speed_history.copy(),
            'avg_speed': sum(self.speed_history) / len(self.speed_history)
        }

# =============================================================================
# HELPFUL REFERENCE CODE (for students to adapt)
# =============================================================================

"""
EXAMPLE FEATURE DETECTION:
features = cv2.goodFeaturesToTrack(
    gray_image,
    maxCorners=100,
    qualityLevel=0.3,
    minDistance=7,
    blockSize=7
)

EXAMPLE OPTICAL FLOW:
features_next, status, error = cv2.calcOpticalFlowPyrLK(
    old_gray, 
    new_gray, 
    features_prev, 
    None, 
    **lk_params
)

EXAMPLE FLOW MAGNITUDE CALCULATION:
for i, (prev_pt, curr_pt) in enumerate(zip(good_prev, good_curr)):
    if status[i] == 1:  # Successfully tracked
        dx = curr_pt[0][0] - prev_pt[0][0]
        dy = curr_pt[0][1] - prev_pt[0][1]
        magnitude = np.sqrt(dx*dx + dy*dy)
        flow_magnitudes.append(magnitude)

avg_flow = np.mean(flow_magnitudes)
"""
```