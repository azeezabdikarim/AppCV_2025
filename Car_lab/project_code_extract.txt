# Project Code Extract
# Generated on: 2025-07-08 23:27:20

## Directory Structure
```
/Users/azeez/Google_Drive/Life/PhD/AppCV_2025/AppCV_2025/Car_lab
├── core
│   ├── camera_manager.py
│   ├── flask_control.py
│   ├── robot_controller.py
│   ├── run.py
│   ├── templates
│   │   └── control.html
│   └── utils
│       ├── console_logger.py
│       ├── debug_visualizer.py
│       ├── movement_controls.py
│       └── utils.py
├── feature_config.py
├── midas_depth-1.0.0-py3-none-any.whl
├── project_code_extract.txt
├── requirements.txt
├── teaching_tools
│   ├── 6panel_pid_graph.py
│   └── canny_filter.py
├── week1_line_following
│   ├── __init__.py
│   └── line_follower.py
├── week2_object_detection
│   ├── models
│   │   └── best.onnx
│   └── sign_detector.py
└── week3_speed_estimation
    ├── calibration_params.json
    ├── calibration_script.py
    ├── show_stats.py
    ├── speed_estimator.py
    └── speed_estimator_skeleton.py

9 directories, 24 files

```

## Code Files


################################################################################
# FILE: core/camera_manager.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import threading
import time
import subprocess
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CameraManager:
    def __init__(self):
        """Initialize camera using libcamera-vid streaming"""
        self.frame = None
        self.is_running = False
        self.lock = threading.Lock()
        self.process = None
        self.current_frame = None
        
        logger.info("Camera manager initialized")
    
    def _create_placeholder_frame(self, message="Waiting for camera..."):
        """Create a frame with a message"""
        frame = np.zeros((240, 320, 3), dtype=np.uint8)
        
        # Add message
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        color = (255, 255, 255)
        thickness = 1
        
        # Calculate text size and position for centering
        text_size = cv2.getTextSize(message, font, font_scale, thickness)[0]
        text_x = (frame.shape[1] - text_size[0]) // 2
        text_y = (frame.shape[0] + text_size[1]) // 2
        
        cv2.putText(frame, message, (text_x, text_y), font, font_scale, color, thickness)
        
        # Add timestamp
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        cv2.putText(frame, timestamp, (5, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        return frame
    
    def start_streaming(self):
        """Start the camera streaming using libcamera-vid"""
        if not self.is_running:
            self.is_running = True
            self.capture_thread = threading.Thread(target=self._capture_frames)
            self.capture_thread.daemon = True
            self.capture_thread.start()
            logger.info("Camera streaming started")
    
    def _capture_frames(self):
        """Continuously capture frames using libcamera-vid streaming (from working example)"""
        logger.info("Starting libcamera-vid streaming...")
        
        try:
            # Start libcamera-vid streaming to stdout (exact command from working example)
            cmd = [
                "libcamera-vid",
                "-t", "0",  # Infinite timeout
                "--width", "320",
                "--height", "240",
                "--framerate", "15",
                "-o", "-",  # Output to stdout
                "--codec", "mjpeg",
                "--inline",
                "-n"  # No preview
            ]
            
            self.process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # Test if the process starts successfully
            time.sleep(1)
            if self.process.poll() is not None:
                stderr_output = self.process.stderr.read().decode('utf-8')
                logger.error(f"libcamera-vid failed to start. Error: {stderr_output}")
                self._fallback_placeholder_loop()
                return
            
            logger.info("libcamera-vid streaming started successfully")
            
            buffer = b""
            while self.is_running:
                try:
                    chunk = self.process.stdout.read(1024)
                    if not chunk:
                        break
                    
                    buffer += chunk
                    
                    # Look for JPEG boundaries
                    start = buffer.find(b'\xff\xd8')
                    end = buffer.find(b'\xff\xd9')
                    
                    if start != -1 and end != -1 and end > start:
                        # Extract JPEG frame
                        jpeg_data = buffer[start:end+2]
                        buffer = buffer[end+2:]
                        
                        # Decode JPEG
                        nparr = np.frombuffer(jpeg_data, np.uint8)
                        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                        
                        if frame is not None:
                            with self.lock:
                                self.current_frame = frame.copy()
                
                except Exception as e:
                    logger.error(f"Streaming error: {e}")
                    break
            
            if self.process:
                self.process.terminate()
                
        except Exception as e:
            logger.error(f"libcamera streaming setup failed: {e}")
            self._fallback_placeholder_loop()
    
    def _fallback_placeholder_loop(self):
        """Generate placeholder frames when camera fails"""
        logger.info("Using placeholder frames due to camera failure")
        frame_counter = 0
        while self.is_running:
            frame_counter += 1
            message = f"Camera Error - Frame {frame_counter}"
            with self.lock:
                self.current_frame = self._create_placeholder_frame(message)
            time.sleep(0.1)
    
    def get_frame(self):
        """Get the current frame"""
        with self.lock:
            if self.current_frame is not None:
                return self.current_frame.copy()
            else:
                return self._create_placeholder_frame()
    
    def get_jpeg_frame(self):
        """Get current frame as JPEG bytes for streaming"""
        frame = self.get_frame()
        
        # Add minimal status overlay (just timestamp)
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        cv2.putText(frame, timestamp, (5, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
        
        ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        if ret:
            return buffer.tobytes()
        else:
            # Return placeholder frame if encoding fails
            placeholder_frame = self._create_placeholder_frame("Encoding Error")
            ret, buffer = cv2.imencode('.jpg', placeholder_frame)
            return buffer.tobytes()
    
    def stop_streaming(self):
        """Stop camera streaming"""
        self.is_running = False
        if self.process:
            self.process.terminate()
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join(timeout=1)
        logger.info("Camera streaming stopped")
    
    def cleanup(self):
        """Clean shutdown of camera"""
        self.stop_streaming()
        logger.info("Camera cleaned up")

# Global camera instance
camera = CameraManager()
```


################################################################################
# FILE: core/flask_control.py
################################################################################

```python
#!/usr/bin/env python3

from flask import Flask, render_template, Response, jsonify, request
import atexit
import signal
import sys
import socket
from robot_controller import robot
from camera_manager import camera
from utils.console_logger import console_logger


# Initialize Flask app
app = Flask(__name__)

# Global state for tracking commands
last_command = "none"
command_count = 0

def get_local_ip():
    """Get the local IP address of this machine"""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
        return local_ip
    except Exception:
        try:
            hostname = socket.gethostname()
            local_ip = socket.gethostbyname(hostname)
            if local_ip.startswith("127."):
                import subprocess
                result = subprocess.run(['hostname', '-I'], capture_output=True, text=True)
                if result.returncode == 0:
                    addresses = result.stdout.strip().split()
                    if addresses:
                        local_ip = addresses[0]
            return local_ip
        except Exception:
            return "localhost"

def cleanup_handler():
    """Clean up resources on exit"""
    print("\nShutting down...")
    robot.cleanup()
    camera.cleanup()

# Register cleanup handlers
atexit.register(cleanup_handler)
signal.signal(signal.SIGINT, lambda s, f: sys.exit(0))
signal.signal(signal.SIGTERM, lambda s, f: sys.exit(0))

@app.route('/')
def index():
    """Main control page"""
    return render_template('control.html')

@app.route('/video_feed')
def video_feed():
    """Video streaming route with autonomous processing"""
    def generate():
        while True:
            # Get raw frame from camera
            frame = camera.get_frame()
            
            # Process frame for autonomous features and debug visualization
            processed_frame = robot.process_autonomous_frame(frame)
            
            # Convert to JPEG for streaming
            import cv2
            ret, buffer = cv2.imencode('.jpg', processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            if ret:
                frame_bytes = buffer.tobytes()
            else:
                frame_bytes = camera.get_jpeg_frame()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    
    return Response(generate(), mimetype='multipart/x-mixed-replace; boundary=frame')

# =============================================================================
# AUTONOMOUS CONTROL ROUTES
# =============================================================================

@app.route('/start_autonomous')
def start_autonomous():
    """Start autonomous line following mode"""
    success, message = robot.start_autonomous_mode()
    global last_command
    last_command = "autonomous started" if success else "autonomous failed"
    
    return jsonify({
        'success': success,
        'message': message
    })

@app.route('/stop_autonomous')
def stop_autonomous():
    """Stop autonomous mode"""
    success, message = robot.stop_autonomous_mode()
    global last_command
    last_command = "autonomous stopped"
    
    return jsonify({
        'success': success,
        'message': message
    })

@app.route('/autonomous_status')
def get_autonomous_status():
    """Get detailed autonomous system status"""
    status = robot.get_feature_status()
    
    # Let the robot controller determine the button text
    button_text = robot.get_autonomous_button_text()
    status['button_text'] = button_text
    
    return jsonify(status)

@app.route('/console_logs')
def get_console_logs():
    """Get recent console log messages"""
    messages = console_logger.get_recent_messages(limit=50)
    return jsonify({'messages': messages})

# =============================================================================
# CAMERA CONTROL ROUTES (FIXED - Using Query Parameters)
# =============================================================================

@app.route('/set_camera_pan')
def set_camera_pan():
    """Set camera pan angle using query parameter"""
    try:
        angle = request.args.get('angle', type=int)
        if angle is None:
            return jsonify({
                'success': False,
                'message': 'Angle parameter required'
            }), 400
        
        success = robot.set_camera_pan(angle)
        return jsonify({
            'success': success,
            'angle': angle,
            'message': f'Camera pan set to {angle}°' if success else 'Camera pan failed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting camera pan: {str(e)}'
        }), 500

@app.route('/set_camera_tilt')
def set_camera_tilt():
    """Set camera tilt angle using query parameter"""
    try:
        angle = request.args.get('angle', type=int)
        if angle is None:
            return jsonify({
                'success': False,
                'message': 'Angle parameter required'
            }), 400
        
        success = robot.set_camera_tilt(angle)
        return jsonify({
            'success': success,
            'angle': angle,
            'message': f'Camera tilt set to {angle}°' if success else 'Camera tilt failed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting camera tilt: {str(e)}'
        }), 500

@app.route('/camera_look_down')
def camera_look_down():
    """Preset: Point camera down for line following"""
    success = robot.camera_look_down()
    return jsonify({
        'success': success,
        'message': 'Camera pointed down for line following' if success else 'Camera positioning failed'
    })

@app.route('/camera_look_forward')
def camera_look_forward():
    """Preset: Point camera forward"""
    success = robot.camera_look_forward()
    return jsonify({
        'success': success,
        'message': 'Camera pointed forward' if success else 'Camera positioning failed'
    })

# =============================================================================
# DEBUG DATA ROUTES
# =============================================================================

@app.route('/debug_data')
def get_debug_data():
    """Get clean debug data for sidebar"""
    return jsonify(robot.get_debug_data())

@app.route('/set_debug_level')
def set_debug_level():
    """Set debugging visualization level using query parameter"""
    try:
        level = request.args.get('level', type=int)
        if level is None:
            return jsonify({
                'success': False,
                'message': 'Level parameter required'
            }), 400
        
        robot.set_debug_level(level)
        return jsonify({
            'success': True,
            'debug_level': robot.debug_level,
            'message': f'Debug level set to {level}'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting debug level: {str(e)}'
        }), 500

@app.route('/set_frame_rate')
def set_frame_rate():
    """Set frame rate using query parameter"""
    try:
        fps = request.args.get('fps', type=int)
        if fps is None:
            return jsonify({
                'success': False,
                'message': 'FPS parameter required'
            }), 400
        
        robot.set_frame_rate(fps)
        return jsonify({
            'success': True,
            'frame_rate': robot.target_fps,
            'message': f'Frame rate set to {fps} fps'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting frame rate: {str(e)}'
        }), 500
    
@app.route('/set_debug_mode')
def set_debug_mode():
    """Set debug visualization mode using query parameter"""
    try:
        mode = request.args.get('mode', 'line_following')
        success = robot.set_debug_mode(mode)
        return jsonify({
            'success': success,
            'debug_mode': robot.debug_mode,
            'available_modes': robot.available_modes,
            'message': f'Debug mode set to {mode}' if success else f'Invalid debug mode: {mode}'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error setting debug mode: {str(e)}'
        }), 500

@app.route('/get_week2_performance')
def get_week2_performance():
    """Get Week 2 specific performance metrics"""
    try:
        return jsonify(robot.get_debug_mode_status())
    except Exception as e:
        return jsonify({
            'error': f'Error getting performance data: {str(e)}'
        }), 500

@app.route('/update_pid_parameters')
def update_pid_parameters():
    """Update PID parameters via web interface"""
    kp = request.args.get('kp', type=float)
    ki = request.args.get('ki', type=float)
    kd = request.args.get('kd', type=float)
    
    robot.update_pid_parameters(kp=kp, ki=ki, kd=kd)
    
    return jsonify({
        'success': True,
        'message': 'PID parameters updated',
        'kp': kp,
        'ki': ki, 
        'kd': kd
    })

# =============================================================================
# EXISTING MANUAL CONTROL ROUTES (unchanged)
# =============================================================================

@app.route('/move/<direction>')
def move_robot(direction):
    """Handle movement commands"""
    global last_command, command_count
    
    if robot.autonomous_mode:
        return jsonify({
            'success': False,
            'message': 'Manual control disabled during autonomous mode',
            'command_count': command_count
        })
    
    command_count += 1
    success = False
    
    try:
        if direction == 'forward':
            success = robot.move_forward(duration=0.5, speed=50)
            last_command = "forward"
            
        elif direction == 'backward':
            success = robot.move_backward(duration=0.5, speed=50)
            last_command = "backward"
            
        elif direction == 'left':
            success = robot.turn_left(duration=0.5, speed=50, angle=-30)
            last_command = "turn left"
            
        elif direction == 'right':
            success = robot.turn_right(duration=0.5, speed=50, angle=30)
            last_command = "turn right"
            
        elif direction == 'stop':
            robot.emergency_stop()
            last_command = "emergency stop"
            success = True
            
        else:
            return jsonify({
                'success': False, 
                'message': f'Unknown direction: {direction}',
                'command_count': command_count
            })
        
        return jsonify({
            'success': success,
            'direction': direction,
            'message': f'Command {direction} {"executed" if success else "failed"}',
            'command_count': command_count
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Error executing {direction}: {str(e)}',
            'command_count': command_count
        })

@app.route('/status')
def get_status():
    """Get current robot status"""
    return jsonify({
        'last_command': last_command,
        'command_count': command_count,
        'is_moving': robot.movement_controller.is_moving if robot.movement_controller.is_hardware_connected() else False,
        'robot_connected': robot.movement_controller.is_hardware_connected(),
        'autonomous_mode': robot.autonomous_mode
    })

@app.route('/test')
def test_robot():
    """Test basic robot functionality"""
    if robot.picar is None:
        return jsonify({
            'success': False,
            'message': 'Robot not connected'
        })
    
    if robot.autonomous_mode:
        return jsonify({
            'success': False,
            'message': 'Cannot test during autonomous mode'
        })
    
    try:
        robot.move_forward(duration=0.2, speed=30)
        return jsonify({
            'success': True,
            'message': 'Test movement completed'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Test failed: {str(e)}'
        })
    
@app.route('/speed_data')
def get_speed_data():
    """Get current speed data for speed estimation debug mode"""
    try:
        speed_data = robot.get_speed_data()
        
        # Add real-time timestamp
        import time
        speed_data['timestamp'] = time.time()
        
        return jsonify(speed_data)
    except Exception as e:
        return jsonify({
            'error': f'Speed data error: {str(e)}',
            'current_speed': 0.0,
            'smoothed_speed': 0.0,
            'speed_history': [],
            'motor_power': 0,
            'test_active': False,
            'timestamp': time.time()
        }), 500

@app.route('/speed_test_control')
def speed_test_control():
    """Control speed testing with discrete power levels and auto-stop"""
    try:
        action = request.args.get('action', 'stop')  # start, stop, emergency_stop
        speed = request.args.get('speed', type=int, default=30)
        
        if robot.autonomous_mode:
            return jsonify({
                'success': False,
                'message': 'Speed testing disabled during autonomous mode'
            })
        
        if action == 'start':
            # Validate speed levels
            if speed not in [30, 50, 70]:
                return jsonify({
                    'success': False,
                    'message': f'Invalid speed level: {speed}. Use 30, 50, or 70.'
                })
        
        success, message = robot.control_speed_test(action, speed)
        
        return jsonify({
            'success': success,
            'message': message,
            'action': action,
            'speed': speed if action == 'start' else 0,
            'duration': 3 if action == 'start' else 0
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Speed test error: {str(e)}'
        }), 500

@app.route('/process_speed_frames')
def process_speed_frames():
    """Lightweight speed estimation without video streaming"""
    try:
        # Get raw frame from camera (no encoding)
        current_frame = camera.get_frame()
        
        # Run speed estimation only (no autonomous processing)
        speed = robot.calculate_speed_only(current_frame)
        
        # Return minimal JSON (not full debug data)
        return jsonify({
            'current_speed': speed,
            'smoothed_speed': robot.get_smoothed_speed(),
            'timestamp': time.time()
        })
    except Exception as e:
        return jsonify({'current_speed': 0.0, 'smoothed_speed': 0.0, 'error': str(e)})

if __name__ == '__main__':
    try:
        camera.start_streaming()
        local_ip = get_local_ip()
        
        print("Starting Flask server...")
        print("Open http://localhost:5000 in your browser")
        print(f"Or from another device: http://{local_ip}:5000")
        print("Use camera controls to position camera, then start autonomous mode")
        
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
        
    except KeyboardInterrupt:
        print("\nShutting down due to keyboard interrupt")
    except Exception as e:
        print(f"Error starting server: {e}")
    finally:
        cleanup_handler()
```


################################################################################
# FILE: core/robot_controller.py
################################################################################

```python
#!/usr/bin/env python3

import threading
import time
import cv2
import numpy as np
import sys
import os

# Add the parent directory to Python path to ensure imports work
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.movement_controls import MovementController
from utils.debug_visualizer import DebugVisualizer
from utils.utils import TimingUtils, CacheManager, StatusManager
from utils.console_logger import console_logger

try:
    from picarx import Picarx
except ImportError:
    print("WARNING: PicarX not available - running in simulation mode")
    Picarx = None

# =============================================================================
# EXPLICIT FEATURE CONTROL
# Students enable features when they're ready
# =============================================================================
FEATURES_ENABLED = {
    'line_following': False,   # Week 1 - Enable when ready
    'sign_detection': False,  # Week 2 - Student enables when implemented  
    'speed_estimation': True # Week 3 - Student enables when implemented
}

class RobotController:
    def __init__(self):
        """Initialize the robot controller"""
        try:
            # =================================================================
            # STUDENT TUNABLE PARAMETERS - Modify these as needed
            # =================================================================
            
            # Week 2: Object Detection & Depth Analysis Performance
            self.detection_interval = 0.5    # Run object detection every 0.5 seconds
            self.depth_interval = 1.0        # Run depth analysis every 1.0 seconds
            self.sign_stop_duration = 2.0    # Seconds to stop for detected signs
            self.stop_cooldown_duration = 1.0 # Cooldown after movement resumes
            
            # Debug and visualization settings
            self.debug_level = 0             # 0-4, higher = more debug info
            self.target_fps = 10             # Target frame rate for line following
            
            # =================================================================
            # SYSTEM VARIABLES - Don't modify these directly
            # =================================================================
            
            # Initialize hardware controller
            self.movement_controller = MovementController()
            
            # Initialize utility managers
            self.timing_utils = TimingUtils()
            self.cache_manager = CacheManager()
            self.status_manager = StatusManager()
            
            # Initialize debug visualizer
            self.debug_visualizer = DebugVisualizer()
            
            # Debug mode system
            self.debug_mode = "line_following"  # Default to Week 1
            self.available_modes = ["line_following", "object_detection", "speed_estimation", "full_system"]
            
            # Autonomous mode variables
            self.autonomous_mode = False
            self.frame_counter = 0
            self.previous_frame = None
            self.current_speed = 0.0
            self.sign_stop_until = None      # Time when sign stop expires
            
            # Performance tracking and timing
            self.last_frame_time = time.time()
            self.frame_interval = 1.0 / self.target_fps

            # Speed testing state
            self._current_test_speed = 0
            
            # Feature modules (loaded based on FEATURES_ENABLED)
            self.line_follower = None
            self.sign_detector = None
            self.speed_estimator = None
            
            # Feature status tracking
            self.feature_status = {
                'line_following': 'Disabled',
                'sign_detection': 'Disabled', 
                'speed_estimation': 'Disabled'
            }
            
            # Debug data for sidebar (clean, minimal)
            self.debug_data = {
                'error_px': 0.0,
                'steering_angle': 0.0,
                'lines_detected': 0,
                'mode': 'Manual'
            }
            
            # Load enabled features
            self._load_enabled_features()
            
            print("✅ Robot controller initialized successfully")
        except Exception as e:
            print(f"❌ Error initializing robot: {e}")
    
    def _load_enabled_features(self):
        """Load only the features that are explicitly enabled"""
        
        print("Loading enabled features...")
        
        # Week 1: Line Following
        if FEATURES_ENABLED['line_following']:
            try:
                # Clear any cached modules to force reload
                module_name = 'week1_line_following.line_follower'
                if module_name in sys.modules:
                    del sys.modules[module_name]
                
                from week1_line_following.line_follower import LineFollower
                self.line_follower = LineFollower()
                self.feature_status['line_following'] = 'Active'
                print("✅ Line following enabled and loaded")
                
            except Exception as e:
                self.feature_status['line_following'] = f'Error: {str(e)}'
                print(f"❌ Line following error: {e}")
        else:
            self.feature_status['line_following'] = 'Disabled'
            print("Line following disabled")
            
        # Week 2: Sign Detection
        if FEATURES_ENABLED['sign_detection']:
            try:
                if 'week2_object_detection.sign_detector' in sys.modules:
                    del sys.modules['week2_object_detection.sign_detector']
                    
                from week2_object_detection.sign_detector import SignDetector
                self.sign_detector = SignDetector()
                self.feature_status['sign_detection'] = 'Active'
                print("✅ Sign detection enabled and loaded")
            except Exception as e:
                self.feature_status['sign_detection'] = f'Error: {str(e)}'
                print(f"❌ Sign detection error: {e}")
        else:
            self.feature_status['sign_detection'] = 'Disabled'
            print("Sign detection disabled")
            
        # Week 3: Speed Estimation
        if FEATURES_ENABLED['speed_estimation']:
            try:
                if 'week3_speed_estimation.speed_estimator' in sys.modules:
                    del sys.modules['week3_speed_estimation.speed_estimator']
                    
                from week3_speed_estimation.speed_estimator import SpeedEstimator
                self.speed_estimator = SpeedEstimator()
                self.feature_status['speed_estimation'] = 'Active' 
                print("✅ Speed estimation enabled and loaded")
            except Exception as e:
                self.feature_status['speed_estimation'] = f'Error: {str(e)}'
                print(f"❌ Speed estimation error: {e}")
        else:
            self.feature_status['speed_estimation'] = 'Disabled'
            print("Speed estimation disabled")
        
        # Print final status
        print("Feature Status Summary:")
        for feature, status in self.feature_status.items():
            print(f"   {feature}: {status}")
    
    def start_autonomous_mode(self):
        """Start autonomous mode - either line following or straight movement"""
        console_logger.info("Starting autonomous mode...")
        
        if not self.movement_controller.is_hardware_connected():
            error_msg = "Robot hardware not connected"
            print(f"❌ {error_msg}")
            return False, error_msg
            
        self.autonomous_mode = True
        self.frame_counter = 0
        
        # Determine mode based on line following availability
        if FEATURES_ENABLED['line_following'] and self.line_follower:
            self.debug_data['mode'] = 'Line Following'
            console_logger.info("✅ Autonomous line following started")
            return True, "Line following started"
        else:
            self.debug_data['mode'] = 'Straight Movement'
            console_logger.info("✅ Autonomous straight movement started")
            return True, "Straight movement started"

    def calculate_speed_only(self, current_frame):
        """Minimal speed calculation without autonomous processing"""
        if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
            speed = self.speed_estimator.estimate_speed(current_frame, self.previous_frame)
            self.previous_frame = current_frame.copy()  # Store for next calculation
            return speed
        return 0.0
    
    def stop_autonomous_mode(self):
        """Stop autonomous mode and return to manual control"""
        self.autonomous_mode = False
        self.debug_data['mode'] = 'Manual'
        self.movement_controller.emergency_stop()
        print("Autonomous mode stopped")
        return True, "Autonomous mode stopped"
    
    def process_autonomous_frame(self, frame):
        """Main processing pipeline with clean debug separation"""
        
        display_frame = frame.copy()
        
        # Initialize debug data
        self.debug_data = {
            'error_px': 0.0,
            'steering_angle': 0.0,
            'lines_detected': 0,
            'mode': 'Autonomous' if self.autonomous_mode else 'Manual'
        }
        
        # Check if currently stopped for sign detection and cooldown logic
        current_time = time.time()
        stopped_for_sign = (self.sign_stop_until is not None and current_time < self.sign_stop_until)
        in_cooldown = self.status_manager.is_in_cooldown(current_time)
        
        # Week 2: Sign Detection (with timing and caching)
        if self.sign_detector and FEATURES_ENABLED['sign_detection'] and not stopped_for_sign and not in_cooldown:
            detected_signs = self._run_detection_with_timing(frame)
            if self.sign_detector.should_stop(detected_signs, frame):
                self.sign_stop_until = current_time + self.sign_stop_duration
                self.status_manager.set_recently_stopped(True)
                stopped_for_sign = True
                self.movement_controller.stop() 
                console_logger.info("Stopping for detected sign")
        
        # Anti-infinite-stop: Reset cooldown when robot starts moving again
        if self.status_manager.recently_stopped_for_sign and not stopped_for_sign and self.autonomous_mode:
            self.status_manager.start_cooldown(current_time, self.stop_cooldown_duration)
            console_logger.info("Stop cooldown activated")
        
        # Week 3: Speed Estimation
        if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
            self.current_speed = self.speed_estimator.estimate_speed(frame, self.previous_frame)
            self.debug_data['current_speed'] = round(self.current_speed, 1)
        
        # Week 1: Line Following (skip if stopped for sign)
        if self.line_follower and FEATURES_ENABLED['line_following']:
            try:
                steering_angle = self.line_follower.compute_steering_angle(frame, debug_level=self.debug_level)
                
                debug_frame = self.line_follower.get_debug_frame()
                if debug_frame is not None:
                    display_frame = debug_frame
                
                if hasattr(self.line_follower, 'current_debug_data'):
                    self.debug_data.update(self.line_follower.current_debug_data)
                
                # Apply control only if autonomous and not stopped for sign
                if self.autonomous_mode and not stopped_for_sign:
                    if current_time - self.last_frame_time >= self.frame_interval:
                        self.last_frame_time = current_time
                        self.frame_counter += 1
                        
                        self.movement_controller.apply_autonomous_control(steering_angle)
                
            except Exception as e:
                print(f"Line following error: {e}")
                self.feature_status['line_following'] = f'Runtime Error: {str(e)}'

        # FALLBACK: Straight movement if line following not available
        elif self.autonomous_mode and not stopped_for_sign:
            if current_time - self.last_frame_time >= self.frame_interval:
                self.last_frame_time = current_time
                self.frame_counter += 1
                
                # Just go straight (steering angle = 0)
                self.movement_controller.apply_autonomous_control(0)
        
        # Route debug visualization based on mode
        if self.debug_mode == "object_detection":
            display_frame = self.debug_visualizer.create_week2_debug_frame(
                display_frame, self.cache_manager, self.timing_utils, self.status_manager, self.sign_detector
            )
        elif self.debug_mode == "speed_estimation":
            # Get complete speed data for visualization
            speed_data = self.get_speed_data()
            display_frame = self.debug_visualizer.create_speed_estimation_debug_frame(
                display_frame, self.current_speed, speed_data
            )
        # Default: line_following mode uses existing debug frame
        
        # Store frame for next speed estimation
        self.previous_frame = frame.copy()
        
        return display_frame
    
    def _run_detection_with_timing(self, frame):
        """Run object detection with timing and caching (single inference execution)"""
        return self.timing_utils.run_detection_with_timing(
            frame, self.sign_detector, self.detection_interval, 
            self.depth_interval, self.cache_manager
        )
    
    # =============================================================================
    # DEBUG MODE CONTROL
    # =============================================================================
    
    def set_debug_mode(self, mode):
        """Switch debug visualization mode"""
        if mode in self.available_modes:
            self.debug_mode = mode
            print(f"Debug mode set to: {mode}")
            return True
        else:
            print(f"❌ Invalid debug mode: {mode}. Available: {self.available_modes}")
            return False
    
    def get_debug_mode_status(self):
        """Get current debug mode and performance metrics"""
        current_time = time.time()
        return {
            'debug_mode': self.debug_mode,
            'available_modes': self.available_modes,
            'detection_fps': 1.0 / self.detection_interval if self.detection_interval > 0 else 0,
            'depth_fps': 1.0 / self.depth_interval if self.depth_interval > 0 else 0,
            'last_detection_age': current_time - self.timing_utils.last_detection_time,
            'last_depth_age': current_time - self.timing_utils.last_depth_time,
            'cached_detections': len(self.cache_manager.cached_detections),
            'last_detection_inference_ms': self.timing_utils.last_detection_inference_ms,
            'last_depth_inference_ms': self.timing_utils.last_depth_inference_ms
        }
    
    # =============================================================================
    # HARDWARE CONTROL DELEGATION
    # =============================================================================
    
    def set_camera_pan(self, angle):
        """Set camera pan angle (-90 to +90 degrees)"""
        return self.movement_controller.set_camera_pan(angle)
    
    def set_camera_tilt(self, angle):
        """Set camera tilt angle (-90 to +90 degrees)"""
        return self.movement_controller.set_camera_tilt(angle)
    
    def camera_look_down(self):
        """Preset: Point camera down for line following"""
        return self.movement_controller.camera_look_down()
    
    def camera_look_forward(self):
        """Preset: Point camera forward for obstacle detection"""
        return self.movement_controller.camera_look_forward()
    
    def move_forward(self, duration=0.5, speed=50):
        """Move robot forward for specified duration"""
        return self.movement_controller.move_forward(duration, speed, self.autonomous_mode)
    
    def move_backward(self, duration=0.5, speed=50):
        """Move robot backward for specified duration"""
        return self.movement_controller.move_backward(duration, speed, self.autonomous_mode)
    
    def turn_left(self, duration=0.5, speed=50, angle=-30):
        """Turn robot left while moving forward"""
        return self.movement_controller.turn_left(duration, speed, angle, self.autonomous_mode)
    
    def turn_right(self, duration=0.5, speed=50, angle=30):
        """Turn robot right while moving forward"""
        return self.movement_controller.turn_right(duration, speed, angle, self.autonomous_mode)
    
    def emergency_stop(self):
        """Immediately stop the robot"""
        self.autonomous_mode = False
        self.movement_controller.emergency_stop()
    
    def cleanup(self):
        """Clean shutdown of robot"""
        self.emergency_stop()
        self.movement_controller.cleanup()
    
    # =============================================================================
    # DEBUG AND CONFIGURATION METHODS
    # =============================================================================
    
    def set_debug_level(self, level):
        """Set debugging visualization level (0-4)"""
        self.debug_level = max(0, min(4, level))
        print(f"Debug level set to: {self.debug_level}")
    
    def set_frame_rate(self, fps):
        """Set target frame rate"""
        self.target_fps = max(1, min(15, fps))
        self.frame_interval = 1.0 / self.target_fps
        print(f"Frame rate set to: {self.target_fps} fps")
    
    def update_pid_parameters(self, kp=None, ki=None, kd=None):
        """Update PID parameters during runtime"""
        if self.line_follower and hasattr(self.line_follower, 'update_parameters'):
            self.line_follower.update_parameters(kp=kp, ki=ki, kd=kd)
            print(f"PID parameters updated: Kp={kp}, Ki={ki}, Kd={kd}")
        else:
            print("WARNING: Cannot update PID parameters - line follower not available")
    
    def get_debug_data(self):
        """Get clean debug data for sidebar"""
        data = self.debug_data.copy()
        
        # Add Week 2 specific data when in object detection mode
        if self.debug_mode == "object_detection":
            data.update({
                'detections_count': len(self.cache_manager.cached_detections),
                'detection_inference_ms': self.timing_utils.last_detection_inference_ms,
                'depth_inference_ms': self.timing_utils.last_depth_inference_ms,
                'stop_status': self.status_manager.get_stop_status(time.time(), self.sign_stop_until)
            })
        
        return data
    
    def get_speed_data(self):
        """Get current speed data for speed estimation debug mode"""
        speed_data = {
            'current_speed': 0.0,
            'smoothed_speed': 0.0,
            'speed_history': [],
            'motor_power': 0,
            'test_active': False,
            'speed_thresholds': {
                'fast': 0.4,
                'medium': 0.15,
                'slow': 0.05
            }
        }
        
        try:
            if self.speed_estimator and FEATURES_ENABLED['speed_estimation']:
                # Get speed history from speed estimator
                if hasattr(self.speed_estimator, 'get_speed_history'):
                    history_data = self.speed_estimator.get_speed_history()
                    speed_data.update(history_data)
                
                # Add current speed
                speed_data['current_speed'] = round(self.current_speed, 3)
            
            # Add motor status from movement controller
            if hasattr(self, '_current_test_speed'):
                speed_data['motor_power'] = self._current_test_speed
                speed_data['test_active'] = self._current_test_speed > 0
            else:
                # Check if robot is moving manually
                if self.movement_controller.is_hardware_connected():
                    speed_data['test_active'] = self.movement_controller.is_moving
            
        except Exception as e:
            print(f"Speed data error: {e}")
        
        return speed_data
    
    def control_speed_test(self, action, speed_percent):
        """Control speed testing movements"""
        if not self.movement_controller.is_hardware_connected():
            return False, "Robot hardware not connected"
        
        if self.autonomous_mode:
            return False, "Cannot run speed test during autonomous mode"
        
        try:
            if action == 'start':
                # Start movement at specified speed
                self.movement_controller.picar.set_dir_servo_angle(0)  # Straight
                self.movement_controller.picar.forward(speed_percent)
                self._current_test_speed = speed_percent
                
                # Auto-stop after 3 seconds
                import threading
                def auto_stop():
                    try:
                        self.movement_controller.picar.stop()
                        self._current_test_speed = 0
                    except:
                        pass
                
                timer = threading.Timer(3.0, auto_stop)
                timer.start()
                self._speed_test_timer = timer
                
                return True, f"Speed test started at {speed_percent}% for 3 seconds"
                
            elif action == 'stop':
                # Manual stop
                self.movement_controller.picar.stop()
                self._current_test_speed = 0
                
                if hasattr(self, '_speed_test_timer'):
                    self._speed_test_timer.cancel()
                
                return True, "Speed test stopped"
                
        except Exception as e:
            return False, f"Speed test error: {str(e)}"
        
        return False, "Unknown action"
    
    def get_feature_status(self):
        """Return current status of all features"""
        return {
            'autonomous_mode': self.autonomous_mode,
            'features': self.feature_status.copy(),
            'camera_position': self.movement_controller.get_camera_position(),
            'target_fps': self.target_fps,
            'debug_level': self.debug_level,
            'debug_mode': self.debug_mode
        }
    
    def get_autonomous_button_text(self):
        """Get the appropriate button text for autonomous mode"""
        if FEATURES_ENABLED['line_following'] and self.line_follower:
            return "Start Line Following"
        else:
            return "Start Straight Movement"

# Global robot instance
robot = RobotController()
```


################################################################################
# FILE: core/run.py
################################################################################

```python
#!/usr/bin/env python3

"""
Simple script to run the PiCar-X control application
"""

import sys
import subprocess
import time
import socket

def get_local_ip():
    """Get the local IP address of this machine"""
    try:
        # Connect to a remote address (doesn't actually send data)
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
            s.connect(("8.8.8.8", 80))
            local_ip = s.getsockname()[0]
        return local_ip
    except Exception:
        try:
            # Fallback method
            hostname = socket.gethostname()
            local_ip = socket.gethostbyname(hostname)
            if local_ip.startswith("127."):
                # If we get localhost, try getting all addresses
                import subprocess
                result = subprocess.run(['hostname', '-I'], capture_output=True, text=True)
                if result.returncode == 0:
                    addresses = result.stdout.strip().split()
                    if addresses:
                        local_ip = addresses[0]
            return local_ip
        except Exception:
            return "localhost"

def check_requirements():
    """Check if required modules are available"""
    required_modules = ['flask', 'cv2', 'picarx']  # Removed vilib
    missing_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
            print(f"✓ {module} - OK")
        except ImportError:
            print(f"✗ {module} - MISSING")
            missing_modules.append(module)
    
    if missing_modules:
        print(f"\nMissing modules: {', '.join(missing_modules)}")
        print("Please install missing modules before running.")
        return False
    
    return True

def main():
    print("=== PiCar-X Control Application ===\n")
    
    print("Checking requirements...")
    if not check_requirements():
        sys.exit(1)
    
    print("\nStarting application...")
    print("Press Ctrl+C to stop the server")
    print("-" * 40)
    
    try:
        # Run the Flask application
        from flask_control import app, camera
        
        # Start camera
        print("📷 Initializing camera (may take a few seconds)...")
        camera.start_streaming()
        time.sleep(3)  # Give camera more time to start
        
        # Get the actual IP address
        local_ip = get_local_ip()
        
        print("\n🚀 Server starting...")
        print("📷 Camera streaming with libcamera-vid")
        print("🌐 Web interface available at:")
        print("   - Local: http://localhost:5000")
        print(f"   - Network: http://{local_ip}:5000")
        print("\n🎮 Controls:")
        print("   - Use WASD keys or web buttons")
        print("   - Each movement lasts 0.5 seconds")
        print("   - Red button for emergency stop")
        print("   - Status updates appear in log below video")
        print("\n" + "="*50)
        
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
        
    except KeyboardInterrupt:
        print("\n\n👋 Shutting down gracefully...")
        
    except Exception as e:
        print(f"\n❌ Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


################################################################################
# FILE: core/utils/console_logger.py
################################################################################

```python
#!/usr/bin/env python3

import time
from datetime import datetime
from collections import deque
import threading

class ConsoleLogger:
    """
    Simple console logger for robot operation messages
    Stores messages in memory for web console display
    """
    
    def __init__(self, max_messages=100):
        """Initialize logger with message history limit"""
        self.messages = deque(maxlen=max_messages)
        self.lock = threading.Lock()
        
    def _add_message(self, level, message):
        """Add a timestamped message to the log"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = {
            'timestamp': timestamp,
            'level': level,
            'message': message
        }
        
        with self.lock:
            self.messages.append(log_entry)
        
        # Also print to terminal for debugging
        print(f"[{timestamp}] {message}")
    
    def info(self, message):
        """Log an info message"""
        self._add_message('INFO', message)
    
    def warning(self, message):
        """Log a warning message"""
        self._add_message('WARN', message)
    
    def error(self, message):
        """Log an error message"""
        self._add_message('ERROR', message)
    
    def stop(self, message):
        """Log a stop event (special category)"""
        self._add_message('STOP', message)
    
    def get_recent_messages(self, limit=50):
        """Get recent messages for web console"""
        with self.lock:
            # Return most recent messages (up to limit)
            recent = list(self.messages)[-limit:]
            return recent
    
    def clear(self):
        """Clear all messages"""
        with self.lock:
            self.messages.clear()

# Global console logger instance
console_logger = ConsoleLogger()
```


################################################################################
# FILE: core/utils/debug_visualizer.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import time

class DebugVisualizer:
    """Handles all debug visualization and overlay creation"""
    
    def __init__(self):
        """Initialize the debug visualizer"""
        pass
    
    def create_week2_debug_frame(self, original_frame, cache_manager, timing_utils, status_manager, sign_detector):
        """Create dual-panel visualization for Week 2 object detection and depth"""
        height, width = original_frame.shape[:2]
        
        # Left panel: Object detection overlay
        left_panel = original_frame.copy()
        if cache_manager.cached_detections:
            left_panel = self._draw_detection_overlay(left_panel, cache_manager.cached_detections)
        
        # Add detection panel label
        cv2.putText(left_panel, "Object Detection", (10, 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Right panel: Depth analysis
        right_panel = original_frame.copy()
        
        # Get cached depth map from SignDetector
        cached_depth_map = sign_detector.get_cached_depth_map() if sign_detector else None
        
        if cached_depth_map is not None:
            # Apply depth colormap
            depth_normalized = cv2.normalize(cached_depth_map, None, 0, 255, cv2.NORM_MINMAX)
            depth_uint8 = depth_normalized.astype(np.uint8)
            depth_colored = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_PLASMA)
            right_panel = cv2.addWeighted(right_panel, 0.6, depth_colored, 0.4, 0)
            
            # Show frame counter alignment
            cv2.putText(right_panel, "[0]", (width - 40, height - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        else:
            # Show waiting status
            cv2.putText(right_panel, "Depth: Enable advanced mode", (10, height//2), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # Add depth panel label
        cv2.putText(right_panel, "Depth Analysis", (10, 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Combine panels side by side
        combined = np.hstack([left_panel, right_panel])
        
        # Add comprehensive debug information
        return self._add_week2_debug_overlay(combined, cache_manager, timing_utils, status_manager)
    
    def create_week3_debug_frame(self, original_frame, current_speed):
        """Simple placeholder for Week 3 - actual speed estimation uses create_speed_estimation_debug_frame"""
        speed_text = f"Speed: {current_speed:.1f} units/s"
        cv2.putText(original_frame, speed_text, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        cv2.putText(original_frame, "Speed Estimation Mode (Week 3)", (10, 60), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return original_frame

    def create_speed_estimation_debug_frame(self, original_frame, current_speed, speed_data):
        """Create comprehensive debug frame for Week 3 speed estimation mode"""
        debug_frame = original_frame.copy()
        height, width = debug_frame.shape[:2]
        
        # Get speed data
        smoothed_speed = speed_data.get('smoothed_speed', current_speed)
        motor_power = speed_data.get('motor_power', 0)
        test_active = speed_data.get('test_active', False)
        
        # Color coding based on speed thresholds
        if smoothed_speed > 0.4:
            color = (0, 0, 255)  # Red - fast
            status = "FAST"
        elif smoothed_speed > 0.15:
            color = (0, 165, 255)  # Orange - medium  
        elif smoothed_speed > 0.05:
            color = (0, 255, 0)  # Green - slow
            status = "SLOW"
        else:
            color = (128, 128, 128)  # Gray - stopped
            status = "STOPPED"
        
        # Large speed display in center (48px equivalent)
        speed_text = f"{smoothed_speed:.2f} m/s"
        font_scale = 2.0
        thickness = 3
        text_size = cv2.getTextSize(speed_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
        text_x = (width - text_size[0]) // 2
        text_y = height // 2
        
        # Background rectangle for speed text
        padding = 20
        cv2.rectangle(debug_frame, 
                    (text_x - padding, text_y - text_size[1] - padding),
                    (text_x + text_size[0] + padding, text_y + padding),
                    (0, 0, 0), -1)
        
        # Speed text
        cv2.putText(debug_frame, speed_text, (text_x, text_y), 
                cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)
        
        # Status text below speed
        status_y = text_y + 50
        cv2.putText(debug_frame, status, (text_x + 20, status_y), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
        
        # Motor power display (top left)
        if test_active and motor_power > 0:
            power_text = f"Motor: {motor_power}% (TEST ACTIVE)"
            cv2.putText(debug_frame, power_text, (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # Speed history indicator (bottom)
        history = speed_data.get('speed_history', [])
        if len(history) > 1:
            avg_speed = sum(history[-5:]) / min(5, len(history))
            cv2.putText(debug_frame, f"Avg (5 frames): {avg_speed:.2f} m/s", 
                    (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # Current values (top right)
        cv2.putText(debug_frame, f"Raw: {current_speed:.3f}", (width - 150, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(debug_frame, f"Smooth: {smoothed_speed:.3f}", (width - 150, 50), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return debug_frame
    
    def create_speed_estimation_debug_frame(self, original_frame, current_speed, speed_data):
        """Create debug frame for Week 3 speed estimation mode"""
        debug_frame = original_frame.copy()
        height, width = debug_frame.shape[:2]
        
        # Add speed information overlay
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Large speed display in center
        speed_text = f"Speed: {current_speed:.2f} m/s"
        font_scale = 1.2
        text_size = cv2.getTextSize(speed_text, font, font_scale, 2)[0]
        text_x = (width - text_size[0]) // 2
        text_y = height // 2
        
        # Background rectangle for speed text
        cv2.rectangle(debug_frame, 
                     (text_x - 10, text_y - text_size[1] - 10),
                     (text_x + text_size[0] + 10, text_y + 10),
                     (0, 0, 0), -1)
        
        # Speed text
        color = (0, 255, 0) if current_speed > 0.1 else (128, 128, 128)
        cv2.putText(debug_frame, speed_text, (text_x, text_y), 
                   font, font_scale, color, 2)
        
        # Status information
        status_y = 30
        
        # Calibration status
        cal_status = "Calibrated" if speed_data.get('calibrated', False) else "Not Calibrated"
        cal_color = (0, 255, 0) if speed_data.get('calibrated', False) else (0, 0, 255)
        cv2.putText(debug_frame, f"Status: {cal_status}", (10, status_y), 
                   font, 0.6, cal_color, 2)
        
        # Motor power
        motor_power = speed_data.get('motor_power', 0)
        if motor_power > 0:
            cv2.putText(debug_frame, f"Motor: {motor_power}%", (10, status_y + 25), 
                       font, 0.6, (255, 255, 0), 2)
        
        # Speed history indicator
        history = speed_data.get('speed_history', [])
        if len(history) > 1:
            cv2.putText(debug_frame, f"Avg: {np.mean(history[-5:]):.2f} m/s", 
                       (10, height - 20), font, 0.5, (255, 255, 255), 1)
        
        return debug_frame
    
    def _draw_detection_overlay(self, frame, detections):
        """Draw bounding boxes and labels for detected objects"""
        for detection in detections:
            bbox = detection['bbox']
            x, y, w, h = bbox
            confidence = detection['confidence']
            class_name = detection.get('class_name', detection.get('class', 'object'))
            
            # Color coding for different object types
            if 'stop' in class_name.lower():
                color = (0, 0, 255)  # Red for stop signs
            else:
                color = (0, 255, 0)  # Green for other objects
            
            # Draw bounding box
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            
            # Draw label with confidence
            label = f"{class_name}: {confidence:.2f}"
            font_scale = 0.5
            thickness = 1
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
            
            # Background for label
            cv2.rectangle(frame, (x, y - label_size[1] - 5), 
                         (x + label_size[0], y), color, -1)
            
            # Label text
            cv2.putText(frame, label, (x, y - 3), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
        
        return frame
    
    def _add_week2_debug_overlay(self, combined_frame, cache_manager, timing_utils, status_manager):
        """Add comprehensive Week 2 debug information"""
        height, width = combined_frame.shape[:2]
        
        # Performance metrics
        detection_interval = getattr(timing_utils, 'detection_interval', 0.5)
        depth_interval = getattr(timing_utils, 'depth_interval', 1.0)
        
        detection_fps = 1.0 / detection_interval if detection_interval > 0 else 0
        depth_fps = 1.0 / depth_interval if depth_interval > 0 else 0
        
        current_time = time.time()
        
        # Get status from status manager
        stop_status = status_manager.get_current_status(current_time)
        
        # Top status line
        status_text = f"Status: {stop_status} | Detection: {detection_fps:.1f}fps ({timing_utils.last_detection_inference_ms:.0f}ms) | Depth: {depth_fps:.1f}fps ({timing_utils.last_depth_inference_ms:.0f}ms)"
        cv2.putText(combined_frame, status_text, (10, height - 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # Detection list
        detection_text = f"Detections: {len(cache_manager.cached_detections)}"
        if cache_manager.cached_detections:
            top_detections = cache_manager.cached_detections[:3]  # Show top 3
            detection_names = [f"{d.get('class_name', d.get('class', 'obj'))}({d['confidence']:.2f})" 
                             for d in top_detections]
            detection_text += f" - {', '.join(detection_names)}"
        
        cv2.putText(combined_frame, detection_text, (10, height - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return combined_frame
```


################################################################################
# FILE: core/utils/movement_controls.py
################################################################################

```python
#!/usr/bin/env python3

import threading
import time
from .console_logger import console_logger


try:
    from picarx import Picarx
except ImportError:
    print("WARNING: PicarX not available - running in simulation mode")
    Picarx = None

class MovementController:
    """Handles all hardware movement and camera control operations"""
    
    def __init__(self):
        """Initialize the movement controller with hardware"""
        try:
            if Picarx:
                self.picar = Picarx()
                print("✅ PiCar-X hardware connected")
            else:
                self.picar = None
                print("WARNING: Running without PiCar-X hardware")
                
            self.is_moving = False
            
            # Camera positioning
            self.camera_pan_angle = 0   # -90 to +90 degrees
            self.camera_tilt_angle = -30  # Start looking down for line following
            
            # Initialize camera position
            self._set_camera_position()
            
        except Exception as e:
            print(f"❌ Error initializing movement hardware: {e}")
            self.picar = None
    
    def _set_camera_position(self):
        """Set camera to initial position"""
        if self.picar:
            try:
                self.picar.set_cam_pan_angle(self.camera_pan_angle)
                self.picar.set_cam_tilt_angle(self.camera_tilt_angle)
                print(f"Camera positioned: pan={self.camera_pan_angle}°, tilt={self.camera_tilt_angle}°")
            except Exception as e:
                print(f"WARNING: Camera positioning error: {e}")
    
    def is_hardware_connected(self):
        """Check if hardware is connected and available"""
        return self.picar is not None
    
    def get_camera_position(self):
        """Get current camera position"""
        return {
            'pan': self.camera_pan_angle,
            'tilt': self.camera_tilt_angle
        }
    
    # =============================================================================
    # CAMERA CONTROL METHODS
    # =============================================================================
    
    def set_camera_pan(self, angle):
        """Set camera pan angle (-90 to +90 degrees)"""
        angle = max(-90, min(90, angle))
        self.camera_pan_angle = angle
        
        if self.picar:
            try:
                self.picar.set_cam_pan_angle(angle)
                print(f"Camera pan set to {angle}°")
                return True
            except Exception as e:
                print(f"❌ Camera pan error: {e}")
                return False
        return False
    
    def set_camera_tilt(self, angle):
        """Set camera tilt angle (-90 to +90 degrees)"""
        angle = max(-90, min(90, angle))
        self.camera_tilt_angle = angle
        
        if self.picar:
            try:
                self.picar.set_cam_tilt_angle(angle)
                print(f"Camera tilt set to {angle}°")
                return True
            except Exception as e:
                print(f"❌ Camera tilt error: {e}")
                return False
        return False
    
    def camera_look_down(self):
        """Preset: Point camera down for line following"""
        return self.set_camera_pan(0) and self.set_camera_tilt(-30)
    
    def camera_look_forward(self):
        """Preset: Point camera forward for obstacle detection"""
        return self.set_camera_pan(0) and self.set_camera_tilt(0)
    
    # =============================================================================
    # AUTONOMOUS MOVEMENT CONTROL
    # =============================================================================
    
    def apply_autonomous_control(self, steering_angle):
        """Apply steering and forward movement for autonomous mode"""
        if self.picar and not self.is_moving:
            try:
                self.picar.set_dir_servo_angle(steering_angle)
                self.picar.forward(1)  # 1% speed
            except Exception as e:
                print(f"Autonomous control error: {e}")
    
    # =============================================================================
    # MANUAL MOVEMENT METHODS
    # =============================================================================
    
    def _auto_stop(self):
        """Automatically stop the robot and center wheels after movement"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)
            self.is_moving = False

    def stop(self):
        """Stop the robot (gentler than emergency_stop)"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)  # Center the wheels
            self.is_moving = False
        console_logger.info("Robot stopped")
    
    def move_forward(self, duration=0.5, speed=50, autonomous_mode=False):
        """Move robot forward for specified duration"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(0)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error moving forward: {e}")
            self._auto_stop()
            return False
    
    def move_backward(self, duration=0.5, speed=50, autonomous_mode=False):
        """Move robot backward for specified duration"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(0)
            self.picar.backward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error moving backward: {e}")
            self._auto_stop()
            return False
    
    def turn_left(self, duration=0.5, speed=50, angle=-30, autonomous_mode=False):
        """Turn robot left while moving forward"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(angle)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error turning left: {e}")
            self._auto_stop()
            return False
    
    def turn_right(self, duration=0.5, speed=50, angle=30, autonomous_mode=False):
        """Turn robot right while moving forward"""
        if not self.picar or self.is_moving or autonomous_mode:
            return False
        
        try:
            self.is_moving = True
            self.picar.set_dir_servo_angle(angle)
            self.picar.forward(speed)
            timer = threading.Timer(duration, self._auto_stop)
            timer.start()
            return True
        except Exception as e:
            print(f"Error turning right: {e}")
            self._auto_stop()
            return False
    
    def emergency_stop(self):
        """Immediately stop the robot"""
        if self.picar:
            self.picar.stop()
            self.picar.set_dir_servo_angle(0)
            self.is_moving = False
        print("Emergency stop activated")
    
    def cleanup(self):
        """Clean shutdown of movement controller"""
        self.emergency_stop()
        print("Movement controller cleaned up")
```


################################################################################
# FILE: core/utils/utils.py
################################################################################

```python
#!/usr/bin/env python3

import time
import cv2
import numpy as np

class TimingUtils:
    """Handles timing and performance tracking for inference operations"""
    
    def __init__(self):
        self.last_detection_time = 0
        self.last_depth_time = 0
        self.last_detection_inference_ms = 0
        self.last_depth_inference_ms = 0
        self.detection_interval = 0.5
        self.depth_interval = 1.0
    
    def run_detection_with_timing(self, frame, sign_detector, detection_interval, depth_interval, cache_manager):
        """Run object detection with timing and caching (single inference execution)"""
        current_time = time.time()
        
        # Store intervals for later use
        self.detection_interval = detection_interval
        self.depth_interval = depth_interval
        
        # Only run detection if enough time has passed
        if current_time - self.last_detection_time >= detection_interval:
            start_time = time.perf_counter()
            detected_signs = sign_detector.detect_signs(frame)
            self.last_detection_inference_ms = (time.perf_counter() - start_time) * 1000
            
            # Cache results for debugging
            cache_manager.update_detections(detected_signs)
            self.last_detection_time = current_time
            
            # Run depth analysis if we have detections and it's time
            if detected_signs and self._should_run_depth(current_time):
                self._run_depth_with_timing(frame, detected_signs, cache_manager)
            
            return detected_signs
        else:
            # Return cached results - no additional inference
            return cache_manager.cached_detections
    
    def _should_run_depth(self, current_time):
        """Check if it's time to run depth analysis"""
        return current_time - self.last_depth_time >= self.depth_interval
    
    def _run_depth_with_timing(self, frame, detections, cache_manager):
        """Run depth estimation with timing (placeholder for MiDaS integration)"""
        start_time = time.perf_counter()
        
        # Placeholder: Simple area-based "depth" for now
        # Students will replace this with actual MiDaS integration
        depth_map = self._simple_area_based_depth(frame, detections)
        
        self.last_depth_inference_ms = (time.perf_counter() - start_time) * 1000
        cache_manager.update_depth(depth_map)
        self.last_depth_time = time.time()
    
    def _simple_area_based_depth(self, frame, detections):
        """Simple area-based depth estimation (students will replace with MiDaS)"""
        # Create a simple depth visualization based on bounding box areas
        height, width = frame.shape[:2]
        depth_map = np.zeros((height, width), dtype=np.uint8)
        
        for detection in detections:
            bbox = detection['bbox']
            x, y, w, h = bbox
            area = w * h
            
            # Larger bounding box = closer = higher depth value
            depth_value = min(255, int(area / 100))  # Simple area-to-depth conversion
            cv2.rectangle(depth_map, (x, y), (x + w, y + h), depth_value, -1)
        
        return depth_map

class CacheManager:
    """Manages cached results for debugging and visualization"""
    
    def __init__(self):
        self.cached_detections = []
        self.cached_depth_map = None
        self.detection_frame_counter = 0
        self.depth_frame_counter = 0
    
    def update_detections(self, detections):
        """Update cached detection results"""
        self.cached_detections = detections
        self.detection_frame_counter += 1
    
    def update_depth(self, depth_map):
        """Update cached depth results"""
        self.cached_depth_map = depth_map
        self.depth_frame_counter = self.detection_frame_counter

class StatusManager:
    """Manages robot status and anti-infinite-stop logic"""
    
    def __init__(self):
        self.recently_stopped_for_sign = False
        self.stop_cooldown_until = None
    
    def set_recently_stopped(self, value):
        """Set the recently stopped flag"""
        self.recently_stopped_for_sign = value
    
    def start_cooldown(self, current_time, cooldown_duration):
        """Start the stop cooldown period"""
        self.stop_cooldown_until = current_time + cooldown_duration
        self.recently_stopped_for_sign = False
    
    def is_in_cooldown(self, current_time):
        """Check if currently in stop cooldown period"""
        return (self.stop_cooldown_until is not None and 
                current_time < self.stop_cooldown_until)
    
    def get_stop_status(self, current_time, sign_stop_until=None):
        """Get current stopping status"""
        if sign_stop_until is not None and current_time < sign_stop_until:
            return "STOPPED"
        elif self.is_in_cooldown(current_time):
            return "COOLDOWN"
        else:
            return "ACTIVE"
    
    def get_current_status(self, current_time):
        """Get current status for debug display"""
        if self.is_in_cooldown(current_time):
            return "COOLDOWN"
        else:
            return "ACTIVE"
```


################################################################################
# FILE: feature_config.py
################################################################################

```python
#!/usr/bin/env python3

"""
Feature Control Configuration
Students enable features as they complete each week's implementation
"""

# =============================================================================
# STUDENT FEATURE CONTROL
# Set to True when you've completed the implementation for each week
# =============================================================================

FEATURES_ENABLED = {
    'line_following': True,    # Week 1: Set to True when line following is ready
    'sign_detection': False,   # Week 2: Set to True when sign detection is ready  
    'speed_estimation': False  # Week 3: Set to True when speed estimation is ready
}

# =============================================================================
# FEATURE DESCRIPTIONS (for reference)
# =============================================================================

FEATURE_DESCRIPTIONS = {
    'line_following': 'Computer vision + PID control for following lines',
    'sign_detection': 'ONNX model integration for detecting stop signs',
    'speed_estimation': 'Optical flow analysis for estimating robot speed'
}

# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Minimum methods required for each feature to be considered "implemented"
REQUIRED_METHODS = {
    'line_following': ['compute_steering_angle'],
    'sign_detection': ['detect_signs', 'should_stop'],
    'speed_estimation': ['estimate_speed']
}

def is_feature_enabled(feature_name):
    """Check if a feature is enabled"""
    return FEATURES_ENABLED.get(feature_name, False)

def get_enabled_features():
    """Get list of currently enabled features"""
    return [name for name, enabled in FEATURES_ENABLED.items() if enabled]

def get_feature_description(feature_name):
    """Get description of a feature"""
    return FEATURE_DESCRIPTIONS.get(feature_name, "No description available")
```


################################################################################
# FILE: teaching_tools/6panel_pid_graph.py
################################################################################

```python
#!/usr/bin/env python3
"""
Extended PID Controller Visualization – Complete Curve Navigation Cycle
----------------------------------------------------------------------
This script generates a six‑panel figure that explains how P, I, D and the
combined PID signal evolve as a small robot negotiates a right‑hand curve.
Changes compared with the original draft:

* **Derivative term is now calculated per definition**  D = Kd * (de/dt).
  – Key‑frame derivative uses the video‑frame interval (0.1 s).
  – Dense derivative uses numpy.gradient and is optionally smoothed by a
    zero‑lag Savitzky–Golay filter (scipy).  If SciPy is absent the code
    runs without smoothing.
* Removed the manual derivative hack and its artefacts (flat purple walls).
* Axis limits auto‑scale except where hard limits aid interpretation.
* Internal constants grouped near the top for quick edits.
* Prints a compact verification table that you can paste into LaTeX.

Written for Python ≥ 3.8.
"""

import os
from pathlib import Path
from typing import Tuple

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon

# ---------------------------------------------------------------------------
# User‑tunable parameters
# ---------------------------------------------------------------------------
Kp: float = 0.8       # degrees / pixel
Ki: float = 0.1       # degrees / pixel·s
Kd: float = 0.3       # degrees / pixel·s

frame_rate: float = 10.0          # camera FPS   → Δt = 0.1 s
wheelbase: float  = 0.20          # m (robot geometry)
speed: float      = 1.0           # m/s (assumed constant)

# Smoothing (set to 0 to disable)
SG_WINDOW: int = 9     # must be odd and ≥ polyorder + 2
SG_POLY: int   = 2

# Output
FIG_NAME = "2extended_pid_visualization.png"
FIG_DPI  = 300

# ---------------------------------------------------------------------------
# Key‑frame scenario (6 frames: N‑2 … N+3)
# ---------------------------------------------------------------------------
key_times  = np.arange(0.0, 0.6, 0.1)                # [0.0 … 0.5] s
key_errors = np.array([5.0, 15.0, 25.0, 15.0, 5.0, 0.0])  # px
frame_dt   = 1.0 / frame_rate                        # 0.1 s

# ---------------------------------------------------------------------------
# Helper: create smooth error profile
# ---------------------------------------------------------------------------

def make_error_profile(times: np.ndarray,
                        key_t: np.ndarray,
                        key_e: np.ndarray) -> np.ndarray:
    """Piece‑wise linear interpolation through key points."""
    return np.interp(times, key_t, key_e)


# ---------------------------------------------------------------------------
# Dense timeline (for nice curves)
# ---------------------------------------------------------------------------
extended_times  = np.linspace(-0.05, 0.65, 150)        # s
extended_errors = make_error_profile(extended_times, key_times, key_errors)

dt_dense = np.gradient(extended_times)                 # variable step (≈0.0047 s)

# ---------------------------------------------------------------------------
# P term (straightforward)
# ---------------------------------------------------------------------------
P_terms      = Kp * extended_errors
key_P_terms  = Kp * key_errors

# ---------------------------------------------------------------------------
# I term (rectangular integration, causal)
# ---------------------------------------------------------------------------
# I_terms = np.zeros_like(extended_errors)
# int_sum = 0.0
# for i in range(1, len(extended_errors)):
#     int_sum += extended_errors[i] * dt_dense[i]
#     I_terms[i] = Ki * int_sum
# key_I_terms = I_terms[np.searchsorted(extended_times, key_times)]
I_terms = np.zeros_like(extended_errors)
int_sum = 0.0
for i in range(1, len(extended_errors)):
    int_sum += extended_errors[i] * dt_dense[i]
    I_terms[i] = Ki * int_sum

# Key frame integral (cumulative sum of discrete errors)
key_integral_sums = np.cumsum(key_errors)  # [5, 20, 45, 60, 65, 65]
key_I_terms = Ki * key_integral_sums       # Ki * cumulative sum

# ---------------------------------------------------------------------------
# D term –– correct formulation
# ---------------------------------------------------------------------------
# 1) Key‑frame derivative -----------------------------------------------------
prev_error = 3.0  # px, assumed error one frame *before* N‑2
error_series = np.insert(key_errors, 0, prev_error)
error_changes = np.diff(error_series) / frame_dt      # px / s
key_D_terms   = Kd * error_changes                    # deg

# 2) Dense derivative ---------------------------------------------------------
D_terms = Kd * np.gradient(extended_errors, extended_times)

# Optional Savitzky–Golay smoothing (keeps zero phase, causal enough here)
try:
    if SG_WINDOW > 2:
        from scipy.signal import savgol_filter
        D_terms = savgol_filter(D_terms, window_length=SG_WINDOW, polyorder=SG_POLY)
except ModuleNotFoundError:
    pass  # SciPy not installed – carry on without smoothing

# ---------------------------------------------------------------------------
# Combined control signal
# ---------------------------------------------------------------------------
I_terms_interpolated = np.interp(extended_times, key_times, key_I_terms)
Total_terms = P_terms + I_terms_interpolated + D_terms
key_totals  = key_P_terms + key_I_terms + key_D_terms

# ---------------------------------------------------------------------------
# Robot kinematics (bicycle model, tiny sideways slip for realism)
# ---------------------------------------------------------------------------

def simulate_robot(times: np.ndarray,
                   steering_deg: np.ndarray,
                   *,
                   v: float = speed,
                   L: float = wheelbase) -> Tuple[np.ndarray, np.ndarray]:
    """Return (x, y) trajectory in metres."""
    x = np.linspace(0, 6, len(times))         # forward distance ~ track length
    y = np.zeros_like(x)
    heading = np.zeros_like(x)

    for i in range(1, len(x)):
        dt = times[i] - times[i - 1]
        steer_rad = np.radians(steering_deg[i])
        if abs(steer_rad) > 1e-3:
            R = L / np.tan(steer_rad)
            omega = v / R
        else:
            omega = 0.0
        heading[i] = heading[i - 1] + omega * dt
        y[i] = y[i - 1] + v * np.sin(heading[i]) * dt * 0.1  # damped lateral slip
    return x, y

robot_x, robot_y = simulate_robot(extended_times, Total_terms)

# ---------------------------------------------------------------------------
# Plotting
# ---------------------------------------------------------------------------
plt.style.use("default")
fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)

frame_labels = ["N-2", "N-1", "N", "N+1", "N+2", "N+3"]

# 1) Error signal -------------------------------------------------------------
ax1 = fig.add_subplot(gs[0, 0])
ax1.plot(extended_times, extended_errors, "b-", lw=3)
ax1.scatter(key_times, key_errors, c="red", s=80, zorder=5)
ax1.set_xlabel("Time (s)")
ax1.set_ylabel("Error (px)")
ax1.set_title("1. Error Signal e(t)")
ax1.grid(alpha=0.3)

for t, e, lbl in zip(key_times, key_errors, frame_labels):
    ax1.annotate(f"{lbl}\n{e:.0f}px", xy=(t, e), xytext=(t + 0.01, e + 2),
                 textcoords="data", arrowprops=dict(arrowstyle="->", color="red"),
                 ha="center", fontsize=9, fontweight="bold")

# 2) P term -------------------------------------------------------------------
ax2 = fig.add_subplot(gs[0, 1])
ax2.plot(extended_times, P_terms, "r-", lw=3)
ax2.scatter(key_times, key_P_terms, c="darkred", s=80)
ax2.set_xlabel("Time (s)")
ax2.set_ylabel("Control (deg)")
ax2.set_title(f"2. Proportional: P = {Kp} × e(t)")
ax2.grid(alpha=0.3)

# 3. INTEGRAL TERM - Enhanced visualization
ax3 = fig.add_subplot(gs[0, 2])

# Plot error curve for reference
ax3.plot(extended_times, extended_errors, 'b--', linewidth=2, alpha=0.6, label='Error e(t)')

# Create progressive hatched areas for integration
time_segments = [
    (extended_times <= key_times[1]),
    (extended_times <= key_times[2]) & (extended_times > key_times[1]),
    (extended_times <= key_times[3]) & (extended_times > key_times[2]),
    (extended_times <= key_times[4]) & (extended_times > key_times[3]),
    (extended_times <= key_times[5]) & (extended_times > key_times[4])
]
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
alphas = [0.4, 0.5, 0.6, 0.4, 0.3]

for i, (mask, color, alpha) in enumerate(zip(time_segments, colors, alphas)):
    if np.any(mask):
        times_seg = extended_times[mask]
        errors_seg = extended_errors[mask]
        ax3.fill_between(times_seg, 0, errors_seg, color=color, alpha=alpha, 
                        hatch='///', edgecolor='black', linewidth=0.5)

# Plot integral value curve
ax3_twin = ax3.twinx()
ax3_twin.plot(extended_times, I_terms, 'g-', linewidth=4, label='Integral Value')
ax3_twin.scatter(key_times, key_I_terms, color='darkgreen', s=100, zorder=5)
ax3_twin.set_ylabel('Integral Term (degrees)', color='green')
ax3_twin.tick_params(axis='y', labelcolor='green')

ax3.set_xlabel('Time (seconds)')
ax3.set_ylabel('Error (pixels)', color='blue')
ax3.set_title(f'3. Integral: I = {Ki} × ∫e(τ)dτ', fontweight='bold', fontsize=12)
ax3.grid(True, alpha=0.3)
ax3.set_ylim(-2, 30)
ax3_twin.set_ylim(0, 8)

# Annotate integral progression
for i, (t, i_val) in enumerate(zip(key_times, key_I_terms)):
    if i % 2 == 0:  # Every other point
        ax3_twin.annotate(f'{i_val:.1f}°', xy=(t, i_val), xytext=(t+0.01, i_val+0.3),
                         arrowprops=dict(arrowstyle='->', color='darkgreen'),
                         fontsize=9, ha='center', fontweight='bold')

# 4) D term -------------------------------------------------------------------
ax4 = fig.add_subplot(gs[1, 0])
ax4.plot(extended_times, D_terms, color="purple", lw=3)
ax4.scatter(key_times, key_D_terms, c="indigo", s=80)
ax4.axhline(0, color="black", lw=0.8, alpha=0.4)
ax4.set_xlabel("Time (s)")
ax4.set_ylabel("Control (deg)")
ax4.set_title(f"4. Derivative: D = {Kd} × de/dt")
ax4.grid(alpha=0.3)


# 5) Combined PID -------------------------------------------------------------
ax5 = fig.add_subplot(gs[1, 1])
ax5.plot(extended_times, P_terms, "r-", lw=2, alpha=0.6, label="P")
ax5.plot(extended_times, I_terms_interpolated, "g-", lw=2, alpha=0.6, label="I")
ax5.plot(extended_times, D_terms, color="purple", lw=2, alpha=0.6, label="D")
ax5.plot(extended_times, Total_terms, "k-", lw=3, label="Total PID")
ax5.scatter(key_times, key_totals, c="orange", s=100, zorder=5)
ax5.axhline(30, color="red", ls="--", alpha=0.7, label="±30° limit")
ax5.axhline(-30, color="red", ls="--", alpha=0.7)
ax5.set_xlabel("Time (s)")
ax5.set_ylabel("Control (deg)")
ax5.set_title("5. Combined PID Output")
ax5.grid(alpha=0.3)
ax5.legend(fontsize=9)

# 6) Robot response -----------------------------------------------------------
ax6 = fig.add_subplot(gs[1, 2])
line_x = robot_x
line_y = np.ones_like(line_x) * 1.5
curve_mask = (line_x >= 2.0) & (line_x <= 4.0)
line_y[curve_mask] += 0.3 * np.sin(np.pi * (line_x[curve_mask] - 2.0) / 2.0)

ax6.plot(line_x, line_y, "k--", lw=3, label="Target Line")
ax6.plot(robot_x, robot_y + 1.5, "r-", lw=3, label="Robot Path")
for i, t in enumerate(key_times):
    idx = np.searchsorted(robot_x, 6 * t / extended_times[-1])
    if i % 2 == 0:
        ax6.scatter(robot_x[idx], robot_y[idx] + 1.5, s=120, c="orange", zorder=5)
        ax6.annotate(f"Frame {frame_labels[i]}",
                     xy=(robot_x[idx], robot_y[idx] + 1.5),
                     xytext=(robot_x[idx], robot_y[idx] + 1.8),
                     arrowprops=dict(arrowstyle="->", color="orange"),
                     ha="center", fontsize=9, fontweight="bold")
ax6.set_xlabel("Distance (m)")
ax6.set_ylabel("Lateral position (m)")
ax6.set_title("6. Robot Response – Complete Curve Navigation")
ax6.set_ylim(1.0, 2.0)
ax6.grid(alpha=0.3)
ax6.legend()

fig.suptitle("Extended PID Line Following: Complete Curve Navigation Cycle",
             fontsize=16, fontweight="bold", y=0.95)
plt.tight_layout()

# Save figure
plt.savefig(FIG_NAME, dpi=FIG_DPI, bbox_inches="tight")
print(f"Figure saved as {FIG_NAME}")

# ---------------------------------------------------------------------------
# Verification table (key frames)
# ---------------------------------------------------------------------------
print("\nKey‑frame PID calculations:")
print("Frame   t(s)  e(px)   P(°)  I(°)   D(°)   Total(°)")
print("———   ————  —————  —————  —————  —————  ——————")
for lbl, t, e, p, i_val, d, tot in zip(frame_labels, key_times,
                                       key_errors, key_P_terms,
                                       key_I_terms, key_D_terms,
                                       key_totals):
    print(f"{lbl:5s}  {t:+.1f}  {e:+5.1f}  {p:+6.1f}  {i_val:+6.1f}  {d:+6.1f}  {tot:+7.1f}")

```


################################################################################
# FILE: teaching_tools/canny_filter.py
################################################################################

```python
import tkinter as tk
from tkinter import ttk, messagebox
import cv2
import numpy as np
from PIL import Image, ImageTk, ImageDraw
import sys
import os


class CannyDemo:
    def __init__(self, image_path):
        self.root = tk.Tk()
        self.root.title("Canny Edge Detection Demo")
        self.root.geometry("1200x1000")
        
        # Load and validate image
        if not os.path.exists(image_path):
            messagebox.showerror("Error", f"Image file not found: {image_path}")
            sys.exit(1)
            
        self.original_image = cv2.imread(image_path)
        if self.original_image is None:
            messagebox.showerror("Error", f"Unable to load image: {image_path}")
            sys.exit(1)
            
        # Convert to grayscale for processing
        self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_BGR2GRAY)
        
        # Resize image if too large for display
        self.display_width = 400
        self.display_height = 300
        self.gray_resized = cv2.resize(self.gray_image, (self.display_width, self.display_height))
        
        # Initialize parameters
        self.low_threshold = tk.IntVar(value=50)
        self.high_threshold = tk.IntVar(value=150)
        self.blur_enabled = tk.BooleanVar(value=False)
        self.blur_kernel = tk.IntVar(value=5)
        
        # Matrix window parameters
        self.matrix_size = 12
        self.window_x = tk.IntVar(value=50)  # Top-left x position
        self.window_y = tk.IntVar(value=50)  # Top-left y position
        
        # Set slider ranges based on image size
        max_x = max(0, self.display_width - self.matrix_size)
        max_y = max(0, self.display_height - self.matrix_size)
        self.max_window_x = max_x
        self.max_window_y = max_y
        
        # Section expansion states
        self.original_expanded = tk.BooleanVar(value=True)
        self.gradient_expanded = tk.BooleanVar(value=True)
        self.canny_expanded = tk.BooleanVar(value=True)
        
        # Left panel image section expansion states
        self.original_image_expanded = tk.BooleanVar(value=True)
        self.processed_image_expanded = tk.BooleanVar(value=True)
        self.canny_image_expanded = tk.BooleanVar(value=True)
        
        self.setup_ui()
        self.update_display()
        
    def setup_ui(self):
        # Main frame with scrollbar
        main_canvas = tk.Canvas(self.root)
        scrollbar = ttk.Scrollbar(self.root, orient="vertical", command=main_canvas.yview)
        scrollable_frame = ttk.Frame(main_canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: main_canvas.configure(scrollregion=main_canvas.bbox("all"))
        )
        
        main_canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        main_canvas.configure(yscrollcommand=scrollbar.set)
        
        main_canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Main content frame
        main_frame = ttk.Frame(scrollable_frame, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Configure grid weights
        main_frame.columnconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=0)
        
        # Left panel for images and controls
        left_panel = ttk.Frame(main_frame)
        left_panel.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(0, 20))
        left_panel.columnconfigure(0, weight=1)
        
        # Original image section
        self.original_image_button = tk.Button(left_panel,
                                              text="▼ Original Image (Grayscale)",
                                              font=('Arial', 12, 'bold'),
                                              command=self.toggle_original_image_section,
                                              relief='flat', anchor='w')
        self.original_image_button.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Original image display
        self.original_image_frame = ttk.Frame(left_panel)
        self.original_image_frame.grid(row=1, column=0, pady=(0, 15))
        
        self.original_label = ttk.Label(self.original_image_frame)
        self.original_label.grid(row=0, column=0)
        
        # Processed image section
        self.processed_image_button = tk.Button(left_panel,
                                               text="▼ Processed Image (with/without Blur)",
                                               font=('Arial', 12, 'bold'),
                                               command=self.toggle_processed_image_section,
                                               relief='flat', anchor='w')
        self.processed_image_button.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Processed image display
        self.processed_image_frame = ttk.Frame(left_panel)
        self.processed_image_frame.grid(row=3, column=0, pady=(0, 15))
        
        self.processed_label = ttk.Label(self.processed_image_frame)
        self.processed_label.grid(row=0, column=0)
        
        # Canny image section
        self.canny_image_button = tk.Button(left_panel,
                                           text="▼ Canny Edge Detection",
                                           font=('Arial', 12, 'bold'),
                                           command=self.toggle_canny_image_section,
                                           relief='flat', anchor='w')
        self.canny_image_button.grid(row=4, column=0, sticky=(tk.W, tk.E), pady=(0, 5))
        
        # Canny image display
        self.canny_image_frame = ttk.Frame(left_panel)
        self.canny_image_frame.grid(row=5, column=0, pady=(0, 20))
        
        self.canny_label = ttk.Label(self.canny_image_frame)
        self.canny_label.grid(row=0, column=0)
        
        # Controls frame
        controls_frame = ttk.LabelFrame(left_panel, text="Canny Parameters", padding="10")
        controls_frame.grid(row=6, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        controls_frame.columnconfigure(1, weight=1)
        
        # Low threshold slider
        ttk.Label(controls_frame, text="Low Threshold:").grid(row=0, column=0, sticky=tk.W, padx=(0, 10))
        self.low_scale = ttk.Scale(controls_frame, from_=0, to=255, 
                                  variable=self.low_threshold, orient=tk.HORIZONTAL,
                                  command=self.on_parameter_change)
        self.low_scale.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.low_value_label = ttk.Label(controls_frame, text="50")
        self.low_value_label.grid(row=0, column=2, sticky=tk.W)
        
        # High threshold slider
        ttk.Label(controls_frame, text="High Threshold:").grid(row=1, column=0, sticky=tk.W, padx=(0, 10))
        self.high_scale = ttk.Scale(controls_frame, from_=0, to=255, 
                                   variable=self.high_threshold, orient=tk.HORIZONTAL,
                                   command=self.on_parameter_change)
        self.high_scale.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.high_value_label = ttk.Label(controls_frame, text="150")
        self.high_value_label.grid(row=1, column=2, sticky=tk.W)
        
        # Gaussian blur checkbox
        self.blur_checkbox = ttk.Checkbutton(controls_frame, text="Gaussian Blur", 
                                           variable=self.blur_enabled,
                                           command=self.on_parameter_change)
        self.blur_checkbox.grid(row=2, column=0, sticky=tk.W, pady=(10, 0))
        
        # Blur kernel slider
        ttk.Label(controls_frame, text="Blur Kernel Size:").grid(row=3, column=0, sticky=tk.W, padx=(0, 10))
        self.kernel_scale = ttk.Scale(controls_frame, from_=1, to=15, 
                                     variable=self.blur_kernel, orient=tk.HORIZONTAL,
                                     command=self.on_kernel_change)
        self.kernel_scale.grid(row=3, column=1, sticky=(tk.W, tk.E), padx=(0, 10))
        self.kernel_value_label = ttk.Label(controls_frame, text="5")
        self.kernel_value_label.grid(row=3, column=2, sticky=tk.W)
        
        # Initially disable kernel slider
        self.update_kernel_state()
        
        # Right panel for matrix display
        right_panel = ttk.Frame(main_frame)
        right_panel.grid(row=0, column=1, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Matrix panel title
        ttk.Label(right_panel, text="Pixel Value Analysis (12x12)", 
                 font=('Arial', 12, 'bold')).grid(row=0, column=0, pady=(0, 10))
        
        # Window position controls
        position_frame = ttk.LabelFrame(right_panel, text="Window Position", padding="5")
        position_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        
        ttk.Label(position_frame, text="X:").grid(row=0, column=0, sticky=tk.W)
        self.x_scale = ttk.Scale(position_frame, from_=0, to=self.max_window_x,
                                variable=self.window_x, orient=tk.HORIZONTAL,
                                command=self.on_window_change)
        self.x_scale.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(5, 5))
        self.x_value_label = ttk.Label(position_frame, text="50")
        self.x_value_label.grid(row=0, column=2)
        
        ttk.Label(position_frame, text="Y:").grid(row=1, column=0, sticky=tk.W)
        self.y_scale = ttk.Scale(position_frame, from_=0, to=self.max_window_y,
                                variable=self.window_y, orient=tk.HORIZONTAL,
                                command=self.on_window_change)
        self.y_scale.grid(row=1, column=1, sticky=(tk.W, tk.E), padx=(5, 5))
        self.y_value_label = ttk.Label(position_frame, text="50")
        self.y_value_label.grid(row=1, column=2)
        
        position_frame.columnconfigure(1, weight=1)
        
        # Create expandable sections
        self.create_expandable_sections(right_panel)
        
    def create_expandable_sections(self, parent):
        current_row = 2
        
        # Original Values Section
        self.original_section_button = tk.Button(parent, 
                                                text="▼ Original Grayscale Values",
                                                font=('Arial', 10, 'bold'),
                                                command=self.toggle_original_section,
                                                relief='flat', anchor='w')
        self.original_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(10, 0))
        
        self.original_section_frame = ttk.Frame(parent)
        self.original_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        current_row += 2
        
        # Gradient Calculations Section
        self.gradient_section_button = tk.Button(parent,
                                                text="▼ Gradient Calculations", 
                                                font=('Arial', 10, 'bold'),
                                                command=self.toggle_gradient_section,
                                                relief='flat', anchor='w')
        self.gradient_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(0, 0))
        
        self.gradient_section_frame = ttk.Frame(parent)
        self.gradient_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        current_row += 2
        
        # Final Edge Detection Section
        self.canny_section_button = tk.Button(parent,
                                             text="▼ Final Edge Detection",
                                             font=('Arial', 10, 'bold'), 
                                             command=self.toggle_canny_section,
                                             relief='flat', anchor='w')
        self.canny_section_button.grid(row=current_row, column=0, sticky=(tk.W, tk.E), pady=(0, 0))
        
        self.canny_section_frame = ttk.Frame(parent)
        self.canny_section_frame.grid(row=current_row+1, column=0, pady=(5, 10))
        
        # Create matrix grids
        self.create_matrix_grids()
        
    def create_matrix_grids(self):
        # Create original values matrix
        self.original_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(self.original_section_frame, text="0", 
                               width=5, height=2, relief="solid", borderwidth=1,
                               font=('Courier', 10), bg='white')
                label.grid(row=i, column=j, padx=1, pady=1)
                row.append(label)
            self.original_matrix_labels.append(row)
            
        # Create gradient matrices (Gx, Gy, Magnitude)
        gradient_container = ttk.Frame(self.gradient_section_frame)
        gradient_container.grid(row=0, column=0)
        
        # Gx matrix
        gx_frame = ttk.Frame(gradient_container)
        gx_frame.grid(row=1, column=0, padx=(0, 10))
        ttk.Label(gx_frame, text="Gx (Horizontal)", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.gx_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(gx_frame, text="0", width=5, height=2, 
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.gx_matrix_labels.append(row)
            
        # Gy matrix
        gy_frame = ttk.Frame(gradient_container)
        gy_frame.grid(row=1, column=1, padx=(0, 10))
        ttk.Label(gy_frame, text="Gy (Vertical)", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.gy_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(gy_frame, text="0", width=5, height=2,
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.gy_matrix_labels.append(row)
            
        # Magnitude matrix
        mag_frame = ttk.Frame(gradient_container)
        mag_frame.grid(row=1, column=2)
        ttk.Label(mag_frame, text="Magnitude", font=('Arial', 8, 'bold')).grid(row=0, column=0, pady=(0, 2))
        
        self.mag_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(mag_frame, text="0", width=5, height=2,
                               relief="solid", borderwidth=1, font=('Courier', 9), bg='white')
                label.grid(row=i+1, column=j, padx=1, pady=1)
                row.append(label)
            self.mag_matrix_labels.append(row)
            
        # Create canny result matrix
        self.canny_matrix_labels = []
        for i in range(self.matrix_size):
            row = []
            for j in range(self.matrix_size):
                label = tk.Label(self.canny_section_frame, text="0", 
                               width=5, height=2, relief="solid", borderwidth=1,
                               font=('Courier', 10), bg='white')
                label.grid(row=i, column=j, padx=1, pady=1)
                row.append(label)
            self.canny_matrix_labels.append(row)
            
    def toggle_original_image_section(self):
        self.original_image_expanded.set(not self.original_image_expanded.get())
        if self.original_image_expanded.get():
            self.original_image_frame.grid()
            self.original_image_button.config(text="▼ Original Image (Grayscale)")
        else:
            self.original_image_frame.grid_remove()
            self.original_image_button.config(text="▶ Original Image (Grayscale)")
            
    def toggle_processed_image_section(self):
        self.processed_image_expanded.set(not self.processed_image_expanded.get())
        if self.processed_image_expanded.get():
            self.processed_image_frame.grid()
            self.processed_image_button.config(text="▼ Processed Image (with/without Blur)")
        else:
            self.processed_image_frame.grid_remove()
            self.processed_image_button.config(text="▶ Processed Image (with/without Blur)")
            
    def toggle_canny_image_section(self):
        self.canny_image_expanded.set(not self.canny_image_expanded.get())
        if self.canny_image_expanded.get():
            self.canny_image_frame.grid()
            self.canny_image_button.config(text="▼ Canny Edge Detection")
        else:
            self.canny_image_frame.grid_remove()
            self.canny_image_button.config(text="▶ Canny Edge Detection")
            
    def toggle_original_section(self):
        self.original_expanded.set(not self.original_expanded.get())
        if self.original_expanded.get():
            self.original_section_frame.grid()
            self.original_section_button.config(text="▼ Original Grayscale Values")
        else:
            self.original_section_frame.grid_remove()
            self.original_section_button.config(text="▶ Original Grayscale Values")
            
    def toggle_gradient_section(self):
        self.gradient_expanded.set(not self.gradient_expanded.get())
        if self.gradient_expanded.get():
            self.gradient_section_frame.grid()
            self.gradient_section_button.config(text="▼ Gradient Calculations")
        else:
            self.gradient_section_frame.grid_remove()
            self.gradient_section_button.config(text="▶ Gradient Calculations")
            
    def toggle_canny_section(self):
        self.canny_expanded.set(not self.canny_expanded.get())
        if self.canny_expanded.get():
            self.canny_section_frame.grid()
            self.canny_section_button.config(text="▼ Final Edge Detection")
        else:
            self.canny_section_frame.grid_remove()
            self.canny_section_button.config(text="▶ Final Edge Detection")
        
    def on_kernel_change(self, value):
        # Ensure kernel size is odd
        kernel_size = int(float(value))
        if kernel_size % 2 == 0:
            kernel_size += 1
            self.blur_kernel.set(kernel_size)
        
        self.kernel_value_label.config(text=str(kernel_size))
        self.update_display()
        
    def on_parameter_change(self, value=None):
        # Update value labels
        self.low_value_label.config(text=str(int(self.low_threshold.get())))
        self.high_value_label.config(text=str(int(self.high_threshold.get())))
        
        # Update kernel slider state
        self.update_kernel_state()
        
        # Update display
        self.update_display()
        
    def on_window_change(self, value=None):
        # Update position labels
        self.x_value_label.config(text=str(int(self.window_x.get())))
        self.y_value_label.config(text=str(int(self.window_y.get())))
        
        # Update display
        self.update_display()
        
    def update_kernel_state(self):
        if self.blur_enabled.get():
            self.kernel_scale.config(state='normal')
            self.kernel_value_label.config(foreground='black')
        else:
            self.kernel_scale.config(state='disabled')
            self.kernel_value_label.config(foreground='gray')
            
    def draw_rectangle_on_image(self, image_array, x, y, size):
        """Draw a red rectangle on the image to show the analysis window"""
        # Convert grayscale to RGB for colored rectangle
        if len(image_array.shape) == 2:
            rgb_image = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)
        else:
            rgb_image = image_array.copy()
            
        # Draw rectangle
        cv2.rectangle(rgb_image, (x, y), (x + size, y + size), (255, 0, 0), 2)
        return rgb_image
    
    def compute_gradients(self, image):
        """Compute Sobel gradients"""
        # Sobel X (horizontal edges)
        gx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
        # Sobel Y (vertical edges)  
        gy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
        # Magnitude
        magnitude = np.sqrt(gx**2 + gy**2)
        
        return gx, gy, magnitude
            
    def update_display(self):
        # Get current window position
        x = int(self.window_x.get())
        y = int(self.window_y.get())
        
        # Display original grayscale image with rectangle
        original_with_rect = self.draw_rectangle_on_image(self.gray_resized, x, y, self.matrix_size)
        original_pil = Image.fromarray(original_with_rect)
        original_photo = ImageTk.PhotoImage(original_pil)
        self.original_label.config(image=original_photo)
        self.original_label.image = original_photo  # Keep a reference
        
        # Apply Gaussian blur if enabled
        processed_image = self.gray_resized.copy()
        if self.blur_enabled.get():
            kernel_size = int(self.blur_kernel.get())
            # Ensure kernel size is odd
            if kernel_size % 2 == 0:
                kernel_size += 1
            processed_image = cv2.GaussianBlur(processed_image, (kernel_size, kernel_size), 0)
        
        # Display processed image with rectangle
        processed_with_rect = self.draw_rectangle_on_image(processed_image, x, y, self.matrix_size)
        processed_pil = Image.fromarray(processed_with_rect)
        processed_photo = ImageTk.PhotoImage(processed_pil)
        self.processed_label.config(image=processed_photo)
        self.processed_label.image = processed_photo  # Keep a reference
        
        # Compute gradients
        gx, gy, magnitude = self.compute_gradients(processed_image)
        
        # Apply Canny edge detection
        low_thresh = int(self.low_threshold.get())
        high_thresh = int(self.high_threshold.get())
        
        # Ensure high threshold is greater than low threshold
        if high_thresh <= low_thresh:
            high_thresh = low_thresh + 1
            
        canny_edges = cv2.Canny(processed_image, low_thresh, high_thresh)
        
        # Display Canny result with rectangle
        canny_with_rect = self.draw_rectangle_on_image(canny_edges, x, y, self.matrix_size)
        canny_pil = Image.fromarray(canny_with_rect)
        canny_photo = ImageTk.PhotoImage(canny_pil)
        self.canny_label.config(image=canny_photo)
        self.canny_label.image = canny_photo  # Keep a reference
        
        # Update matrices
        self.update_matrices(x, y, processed_image, gx, gy, magnitude, canny_edges)
        
    def update_matrices(self, x, y, processed_image, gx, gy, magnitude, canny_edges):
        """Update all pixel value matrices"""
        # Extract the window regions
        original_window = self.gray_resized[y:y+self.matrix_size, x:x+self.matrix_size]
        gx_window = gx[y:y+self.matrix_size, x:x+self.matrix_size]
        gy_window = gy[y:y+self.matrix_size, x:x+self.matrix_size]
        mag_window = magnitude[y:y+self.matrix_size, x:x+self.matrix_size]
        canny_window = canny_edges[y:y+self.matrix_size, x:x+self.matrix_size]
        
        # Update original values matrix
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < original_window.shape[0] and j < original_window.shape[1]:
                    value = int(original_window[i, j])
                    self.original_matrix_labels[i][j].config(text=str(value))
                    # Color code: darker background for higher values
                    intensity = min(255, max(0, value))
                    bg_color = f'#{intensity:02x}{intensity:02x}{intensity:02x}'
                    text_color = 'white' if intensity < 128 else 'black'
                    self.original_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                else:
                    self.original_matrix_labels[i][j].config(text="", bg='white')
                    
        # Update gradient matrices
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < gx_window.shape[0] and j < gx_window.shape[1]:
                    # Gx values
                    gx_val = int(gx_window[i, j])
                    self.gx_matrix_labels[i][j].config(text=str(gx_val))
                    # Color code: blue for negative, red for positive, white for zero
                    if gx_val > 0:
                        intensity = min(255, abs(gx_val) * 2)
                        bg_color = f'#{255:02x}{255-intensity//2:02x}{255-intensity//2:02x}'  # Red scale
                        text_color = 'black' if intensity < 128 else 'white'
                    elif gx_val < 0:
                        intensity = min(255, abs(gx_val) * 2)
                        bg_color = f'#{255-intensity//2:02x}{255-intensity//2:02x}{255:02x}'  # Blue scale
                        text_color = 'black' if intensity < 128 else 'white'
                    else:
                        bg_color = 'white'
                        text_color = 'black'
                    self.gx_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                    
                    # Gy values
                    gy_val = int(gy_window[i, j])
                    self.gy_matrix_labels[i][j].config(text=str(gy_val))
                    # Same color coding as Gx
                    if gy_val > 0:
                        intensity = min(255, abs(gy_val) * 2)
                        bg_color = f'#{255:02x}{255-intensity//2:02x}{255-intensity//2:02x}'  # Red scale
                        text_color = 'black' if intensity < 128 else 'white'
                    elif gy_val < 0:
                        intensity = min(255, abs(gy_val) * 2)
                        bg_color = f'#{255-intensity//2:02x}{255-intensity//2:02x}{255:02x}'  # Blue scale
                        text_color = 'black' if intensity < 128 else 'white'
                    else:
                        bg_color = 'white'
                        text_color = 'black'
                    self.gy_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                    
                    # Magnitude values
                    mag_val = int(mag_window[i, j])
                    self.mag_matrix_labels[i][j].config(text=str(mag_val))
                    # Grayscale based on magnitude
                    intensity = min(255, max(0, mag_val))
                    bg_color = f'#{intensity:02x}{intensity:02x}{intensity:02x}'
                    text_color = 'white' if intensity < 128 else 'black'
                    self.mag_matrix_labels[i][j].config(bg=bg_color, fg=text_color)
                else:
                    self.gx_matrix_labels[i][j].config(text="", bg='white')
                    self.gy_matrix_labels[i][j].config(text="", bg='white')
                    self.mag_matrix_labels[i][j].config(text="", bg='white')
                    
        # Update canny result matrix
        for i in range(self.matrix_size):
            for j in range(self.matrix_size):
                if i < canny_window.shape[0] and j < canny_window.shape[1]:
                    value = int(canny_window[i, j])
                    self.canny_matrix_labels[i][j].config(text=str(value))
                    # Color code: white for edges (255), black for non-edges (0)
                    if value > 0:
                        self.canny_matrix_labels[i][j].config(bg='white', fg='black')
                    else:
                        self.canny_matrix_labels[i][j].config(bg='black', fg='white')
                else:
                    self.canny_matrix_labels[i][j].config(text="", bg='white')
        
    def run(self):
        self.root.mainloop()


def main():
    if len(sys.argv) != 2:
        print("Usage: python canny_demo.py <image_path>")
        print("Example: python canny_demo.py sample.jpg")
        sys.exit(1)
        
    image_path = sys.argv[1]
    app = CannyDemo(image_path)
    app.run()


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week1_line_following/__init__.py
################################################################################

```python
"""Week 1: Line Following Implementation"""

```


################################################################################
# FILE: week1_line_following/line_follower.py
################################################################################

```python
#!/usr/bin/env python3

"""
Line Following Module - Week 1 Assignment
=========================================

STUDENT IMPLEMENTATION FOCUS:
Students implement these core computer vision functions:
1. _extract_roi() - Manually crop image region
2. _convert_to_grayscale() - Convert BGR to grayscale
3. _detect_line_center() - Find the center of line in image
4. _calculate_error() - Compute steering error

The debug system works immediately with dummy values, then improves as 
students implement each function properly.
"""

import cv2
import numpy as np
import time

class LineFollower:
    def __init__(self):
        """Initialize the line follower"""
        
        # =================================================================
        # PID CONTROLLER (PROVIDED - Students focus on computer vision)
        # =================================================================
        self.Kp = 0.8    # Proportional gain
        self.Ki = 0.1    # Integral gain  
        self.Kd = 0.3    # Derivative gain
        
        # PID state variables
        self.previous_error = 0.0
        self.integral_error = 0.0
        self.last_time = time.time()
        
        # =================================================================
        # STUDENT TUNABLE PARAMETERS
        # Students can adjust these values to improve performance
        # =================================================================
        self.roi_top = 0.6        # Start ROI at 60% down the image
        self.roi_bottom = 0.9     # End ROI at 90% down the image
        self.roi_left = 0.1       # Start ROI at 10% from left
        self.roi_right = 0.9      # End ROI at 90% from left
        
        self.canny_low = 50       # Lower threshold for edge detection
        self.canny_high = 150     # Upper threshold for edge detection
        
        # =================================================================
        # STUDENT IMPLEMENTATION VARIABLES
        # These start as dummy values - students implement the functions that set them
        # =================================================================
        
        # Region of Interest - students implement _extract_roi()
        self.current_roi = None           # Will hold cropped image
        self.roi_bounds = (0, 0, 0, 0)   # (top, bottom, left, right) pixel coordinates
        
        # Grayscale conversion - students implement _convert_to_grayscale()  
        self.gray_image = None            # Will hold grayscale version
        
        # Line detection - students implement _detect_line_center()
        self.line_center_x = None         # X coordinate of detected line center
        self.line_center_y = None         # Y coordinate of detected line center
        
        # Error calculation - students implement _calculate_error()
        self.current_error = 0.0          # Pixel error (image_center - line_center)
        self.image_center_x = None        # Center X coordinate of ROI
        
        # =================================================================
        # DEBUG SYSTEM (PROVIDED - Works with dummy values)
        # =================================================================
        self.debug_frame = None
        self.current_debug_data = {
            'error_px': 0.0,
            'steering_angle': 0.0,
            'lines_detected': 0,
            'implementation_status': {
                'roi_extraction': False,
                'grayscale_conversion': False,
                'line_detection': False,
                'error_calculation': False
            }
        }
        
        print("✅ Line follower initialized")
        print("🎯 TODO: Implement the 4 core computer vision functions")
    
    def compute_steering_angle(self, frame, debug_level=0):
        """
        Main processing pipeline - calls student-implemented functions
        """
        if frame is None:
            return 0.0
        
        try:
            # Step 1: Extract Region of Interest (STUDENT IMPLEMENTS)
            self._extract_roi(frame)
            
            # Step 2: Convert to grayscale (STUDENT IMPLEMENTS)
            self._convert_to_grayscale()
            
            # Step 3: Detect line center (STUDENT IMPLEMENTS)
            self._detect_line_center()
            
            # Step 4: Calculate error (STUDENT IMPLEMENTS)
            self._calculate_error()
            
            # Step 5: Apply PID control (PROVIDED)
            steering_angle = self._apply_pid_control()
            
            # Step 6: Update debug visualization (PROVIDED)
            self._update_debug_display(frame, steering_angle, debug_level)
            
            return steering_angle
            
        except Exception as e:
            print(f"Processing error: {e}")
            return 0.0
    
    # =========================================================================
    # STUDENT IMPLEMENTATION SECTION - COMPLETE THESE 4 FUNCTIONS
    # =========================================================================
    
    def _extract_roi(self, frame):
        """
        STUDENT TODO: Extract Region of Interest from the image
        
        Your task:
        1. Calculate pixel coordinates from the percentage parameters above
        2. Use array slicing to crop the image: frame[top:bottom, left:right]
        3. Store the result in self.current_roi
        4. Store the bounds in self.roi_bounds for debug visualization
        
        Example:
            height, width = frame.shape[:2]
            top_px = int(height * self.roi_top)
            # ... calculate other bounds
            self.current_roi = frame[top_px:bottom_px, left_px:right_px]
        """
        
        # STUDENT CODE HERE:
        # Remove this dummy implementation and write your own
        
        height, width = frame.shape[:2]
        
        # TODO: Calculate pixel coordinates from percentages
        # top_px = int(height * self.roi_top)
        # bottom_px = int(height * self.roi_bottom)  
        # left_px = int(width * self.roi_left)
        # right_px = int(width * self.roi_right)
        
        # TODO: Extract ROI using array slicing
        # self.current_roi = frame[top_px:bottom_px, left_px:right_px]
        # self.roi_bounds = (top_px, bottom_px, left_px, right_px)
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        self.current_roi = frame  # Just use full frame for now
        self.roi_bounds = (0, height, 0, width)
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['roi_extraction'] = False
    
    def _convert_to_grayscale(self):
        """
        STUDENT TODO: Convert the ROI to grayscale
        
        Your task:
        1. Check if self.current_roi exists and is not None
        2. Convert from BGR color to grayscale
        3. Store result in self.gray_image
        
        Methods you can use:
        - cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        - Or manual conversion: gray = 0.299*R + 0.587*G + 0.114*B
        
        Example:
            if self.current_roi is not None:
                self.gray_image = cv2.cvtColor(self.current_roi, cv2.COLOR_BGR2GRAY)
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Convert ROI to grayscale
        # if self.current_roi is not None:
        #     self.gray_image = cv2.cvtColor(self.current_roi, cv2.COLOR_BGR2GRAY)
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.current_roi is not None:
            # Just make a black image for now
            height, width = self.current_roi.shape[:2]
            self.gray_image = np.zeros((height, width), dtype=np.uint8)
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['grayscale_conversion'] = False
    
    def _detect_line_center(self):
        """
        STUDENT TODO: Find the center of the line in the grayscale image
        
        Your task:
        1. Use edge detection to find the line (cv2.Canny is recommended)
        2. Find the center point of the detected line
        3. Store coordinates in self.line_center_x and self.line_center_y
        
        Suggested approach:
        1. Apply Canny edge detection: cv2.Canny(gray, low_thresh, high_thresh)
        2. Find white pixels: np.where(edges > 0)
        3. Calculate mean position of white pixels
        
        Example:
            if self.gray_image is not None:
                edges = cv2.Canny(self.gray_image, self.canny_low, self.canny_high)
                white_pixels = np.where(edges > 0)
                if len(white_pixels[0]) > 0:
                    self.line_center_y = int(np.mean(white_pixels[0]))
                    self.line_center_x = int(np.mean(white_pixels[1]))
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Apply edge detection and find line center
        # if self.gray_image is not None:
        #     edges = cv2.Canny(self.gray_image, self.canny_low, self.canny_high)
        #     white_pixels = np.where(edges > 0)
        #     if len(white_pixels[0]) > 0:
        #         self.line_center_y = int(np.mean(white_pixels[0]))
        #         self.line_center_x = int(np.mean(white_pixels[1]))
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.gray_image is not None:
            height, width = self.gray_image.shape
            # Just put line center in middle of image for now
            self.line_center_x = width // 2
            self.line_center_y = height // 2
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['line_detection'] = False
    
    def _calculate_error(self):
        """
        STUDENT TODO: Calculate steering error
        
        Your task:
        1. Find the center X coordinate of the ROI image
        2. Calculate error as: image_center_x - line_center_x
        3. Store result in self.current_error
        
        The error tells us:
        - Positive error: line is to the LEFT of center → steer RIGHT
        - Negative error: line is to the RIGHT of center → steer LEFT  
        - Zero error: line is centered → go straight
        
        Example:
            if self.current_roi is not None and self.line_center_x is not None:
                roi_width = self.current_roi.shape[1]
                self.image_center_x = roi_width // 2
                self.current_error = self.image_center_x - self.line_center_x
        """
        
        # STUDENT CODE HERE:
        
        # TODO: Calculate error between image center and line center
        # if self.current_roi is not None and self.line_center_x is not None:
        #     roi_width = self.current_roi.shape[1]  
        #     self.image_center_x = roi_width // 2
        #     self.current_error = self.image_center_x - self.line_center_x
        
        # DUMMY IMPLEMENTATION (REPLACE THIS):
        if self.current_roi is not None:
            roi_width = self.current_roi.shape[1]
            self.image_center_x = roi_width // 2
            # Dummy error - just use 0 for now
            self.current_error = 0.0
        
        # Update implementation status
        # Set this to True when you implement the function properly
        self.current_debug_data['implementation_status']['error_calculation'] = False
    
    # =========================================================================
    # PROVIDED FUNCTIONS - Students don't need to modify these
    # =========================================================================
    
    def _apply_pid_control(self):
        """Apply PID control to convert error to steering angle (PROVIDED)"""
        
        current_time = time.time()
        dt = current_time - self.last_time
        
        if dt <= 0:
            dt = 0.1
        
        # PID calculation
        proportional = self.Kp * self.current_error
        
        self.integral_error += self.current_error * dt
        integral = self.Ki * self.integral_error
        
        derivative_error = (self.current_error - self.previous_error) / dt
        derivative = self.Kd * derivative_error
        
        # Combine terms and limit output
        steering_angle = proportional + integral + derivative
        steering_angle = np.clip(steering_angle, -30, 30)
        
        # Update state
        self.previous_error = self.current_error
        self.last_time = current_time
        
        return steering_angle
    
    def _update_debug_display(self, original_frame, steering_angle, debug_level):
        """Update debug visualization and data (PROVIDED)"""
        
        # Update sidebar debug data
        lines_detected = 1 if (self.line_center_x is not None and 
                              self.current_debug_data['implementation_status']['line_detection']) else 0
        
        self.current_debug_data.update({
            'error_px': round(self.current_error, 1),
            'steering_angle': round(steering_angle, 1),
            'lines_detected': lines_detected
        })
        
        # Create debug frame
        if debug_level > 0:
            self.debug_frame = self._create_debug_visualization(
                original_frame, steering_angle, debug_level
            )
        else:
            self.debug_frame = original_frame.copy()
    
    def _create_debug_visualization(self, frame, steering_angle, debug_level):
        """Create visual debug overlay (PROVIDED)"""
        
        debug_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Level 1: Show implementation status
        if debug_level >= 1:
            status_y = 30
            for func_name, implemented in self.current_debug_data['implementation_status'].items():
                color = (0, 255, 0) if implemented else (0, 0, 255)
                status = "✓" if implemented else "✗"
                text = f"{status} {func_name.replace('_', ' ').title()}"
                cv2.putText(debug_frame, text, (10, status_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                status_y += 20
            
            # Show current values
            values_text = [
                f"Error: {self.current_error:.1f}px",
                f"Steering: {steering_angle:.1f}°"
            ]
            
            for i, text in enumerate(values_text):
                cv2.putText(debug_frame, text, (10, height - 40 + i*20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Level 2: Show ROI bounds
        if debug_level >= 2 and len(self.roi_bounds) == 4:
            top, bottom, left, right = self.roi_bounds
            cv2.rectangle(debug_frame, (left, top), (right, bottom), (0, 255, 255), 2)
            cv2.putText(debug_frame, "ROI", (left, top-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # Level 3: Show detected centers
        if debug_level >= 3:
            # Show image center (blue)
            if self.image_center_x is not None and len(self.roi_bounds) == 4:
                top, bottom, left, right = self.roi_bounds
                center_global_x = left + self.image_center_x
                center_global_y = (top + bottom) // 2
                cv2.circle(debug_frame, (center_global_x, center_global_y), 8, (255, 0, 0), -1)
                cv2.putText(debug_frame, "IMG CENTER", 
                           (center_global_x-40, center_global_y-15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)
            
            # Show line center (green)
            if (self.line_center_x is not None and self.line_center_y is not None and 
                len(self.roi_bounds) == 4):
                top, bottom, left, right = self.roi_bounds
                line_global_x = left + self.line_center_x
                line_global_y = top + self.line_center_y
                cv2.circle(debug_frame, (line_global_x, line_global_y), 8, (0, 255, 0), -1)
                cv2.putText(debug_frame, "LINE CENTER", 
                           (line_global_x-40, line_global_y+25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
                
                # Draw error line (red)
                if self.image_center_x is not None:
                    cv2.line(debug_frame, 
                           (center_global_x, center_global_y),
                           (line_global_x, line_global_y), 
                           (0, 0, 255), 2)
        
        return debug_frame
    
    def get_debug_frame(self):
        """Return current debug frame"""
        return self.debug_frame
    
    def update_parameters(self, **kwargs):
        """Update parameters during runtime"""
        if 'kp' in kwargs and kwargs['kp'] is not None:
            self.Kp = kwargs['kp']
        if 'ki' in kwargs and kwargs['ki'] is not None:
            self.Ki = kwargs['ki']
        if 'kd' in kwargs and kwargs['kd'] is not None:
            self.Kd = kwargs['kd']

# =========================================================================
# STUDENT TESTING SECTION
# =========================================================================

def test_implementation():
    """Test your implementation step by step"""
    
    print("🧪 Testing Line Follower Implementation")
    print("="*50)
    
    # Create test instance
    lf = LineFollower()
    
    # Create test image with white line
    test_image = np.zeros((240, 320, 3), dtype=np.uint8)
    cv2.line(test_image, (100, 50), (100, 200), (255, 255, 255), 10)
    
    print("📸 Processing test image...")
    
    # Test each function step by step
    print("\n1. Testing ROI extraction...")
    lf._extract_roi(test_image)
    roi_status = "✅ Working" if lf.current_roi is not None else "❌ Not implemented"
    print(f"   ROI: {roi_status}")
    
    print("\n2. Testing grayscale conversion...")
    lf._convert_to_grayscale()
    gray_status = "✅ Working" if lf.gray_image is not None else "❌ Not implemented"
    print(f"   Grayscale: {gray_status}")
    
    print("\n3. Testing line detection...")
    lf._detect_line_center()
    line_status = "✅ Working" if lf.line_center_x is not None else "❌ Not implemented"
    print(f"   Line detection: {line_status}")
    
    print("\n4. Testing error calculation...")
    lf._calculate_error()
    error_status = "✅ Working" if lf.image_center_x is not None else "❌ Not implemented"
    print(f"   Error calculation: {error_status}")
    
    # Test full pipeline
    print("\n5. Testing full pipeline...")
    steering = lf.compute_steering_angle(test_image, debug_level=1)
    print(f"   Steering angle: {steering:.1f}°")
    
    print(f"\n📊 Implementation status:")
    for func, status in lf.current_debug_data['implementation_status'].items():
        icon = "✅" if status else "⭕"
        print(f"   {icon} {func.replace('_', ' ').title()}")
    
    print(f"\n🎯 Next steps:")
    print("1. Implement the functions marked with ⭕")
    print("2. Set implementation_status to True when complete")  
    print("3. Test with real camera via web interface")
    print("4. Tune parameters for better performance")

if __name__ == "__main__":
    test_implementation()

```


################################################################################
# FILE: week2_object_detection/sign_detector.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import os
import time
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core.utils.console_logger import console_logger


class SignDetector:
    """
    WEEK 2: PERCEPTION FOR AUTONOMOUS SYSTEMS
    ==========================================
    
    THE BIG PICTURE:
    In autonomous vehicles, we utilize multiple AI models to compose complex decision making. 
    Think about our porgram as a conductor in an orchestra, telling different instuments (tools) to come in and provide
    their sound (functionality) at the necessary moments of the song (decison process).
    
    PERCEPTION PIPELINE:
    Camera → Object Detection → Depth Estimation → Decision Logic → Robot Action
    
    TODAY'S JOURNEY:
    1. 'detect_signs()' - Use YOLO model to find objects in camera images
    2. 'detect_signs()' - Understand model outputs and tune confidence thresholds  
    3. 'should_stop()' - Make stopping decisions using bounding box size
    4. '_advanced_depth_stopping()' - Upgrade stopping decision to use the MiDaS monocular depth estimateion model
    
    LEARNING GOALS:
    - AI models are tools: understand inputs/outputs and how to apply these tools to navigate complex problems
    - Multiple models can work together
    - Threshold tuning is critical for real-world performance
    - Understand the perfomrance of simple implmentations before adding complexity

    NOTE: Use console_logger.stop() instead of print() for important messages - they'll appear in the web console!
    """
    
    def __init__(self):
        """Initialize ONNX model and detection parameters"""
        
        # Model and detection setup
        self.model_path = os.path.join(os.path.dirname(__file__), 'models', 'best.onnx')
        self.class_names = ['Stop_Sign', 'TU_Logo', 'Stahp', 'Falling_Cows']
        
        # STUDENT TUNABLE PARAMETERS - Experiment with these values
        self.confidence_threshold = 0.05      # Minimum confidence to trust detections
        self.simple_area_threshold = 5000    # Pixel area threshold for stopping
        self.depth_distance_threshold = 1.5  # Distance threshold in meters (advanced)
        
        # Model input specifications (determined by training)
        self.input_size = 640  # Model expects 640x640 input
        
        # Initialize caching for debug visualization
        self._cached_depth_map = None
        self._last_depth_inference_time = 0
        
        # Load the ONNX model
        self._load_model()
        
        # Try to load depth estimation model (optional for advanced section)
        self._load_depth_model()
        
        print("SignDetector initialized - Students: Implement detect_signs() and should_stop()!")

    def detect_signs(self, camera_frame):
        """
        STEP 1: OBJECT DETECTION PIPELINE
        =================================
        
        CONCEPT: Transform camera image → list of detected objects
        
        THE PROCESS:
        1. Preprocess: Camera format → Model format (we provide this)
        2. Inference: Run neural network (YOU implement this)  
        3. Investigate: Explore what the model outputs (YOU discover this)
        4. Parse: Extract useful information (YOU implement this)
        5. Filter: Keep only confident detections (YOU decide thresholds)
        """
        
        if self.session is None:
            return []  # No model loaded
        
        try:
            # STEP 1: Preprocessing (provided for you)
            # Converts 320x240 BGR camera image → to the tensor format needed for our Yolov8 nanon model
            input_tensor = self._preprocess_frame(camera_frame)
            if input_tensor is None:
                return []
            
            print(f"Preprocessed frame shape: {input_tensor.shape}")
            
            # STEP 2: INFERENCE - YOU IMPLEMENT THIS LINE
            # TASK: Run the neural network on your preprocessed image
            # TODO: Call self.session.run() with the input tensor
            # HINT: outputs = self.session.run(None, {self.input_name: input_tensor})
            
            outputs = None  # TODO: Replace this line with actual inference call
            
            if outputs is None:
                return [] 
            
            # STEP 3: INVESTIGATE THE OUTPUTS
            # DISCOVERY TASK: What did the model give us back?
            # print(f"Model output shape: {outputs[0].shape}")
            # print(f"Output data type: {outputs[0].dtype}")
            
            # QUESTION: What do you think these dimensions mean?
            # Our model vocabulary: ['Stop_Sign', 'TU_Logo', 'Stahp', 'Falling_Cows'] (4 classes)
            # Expected shape: [1, 8, 8400] = [batch, coordinates+classes, detections]
            
            # STEP 4: PARSE THE OUTPUTS - YOU IMPLEMENT THIS
            # The output format is: [center_x, center_y, width, height, class0_conf, class1_conf, class2_conf, class3_conf]
            
            detections = [] # This is where we will store properly formated boundary boxes we detect
            
            # TODO: Reshape the outputs to work with them. Currenlty our outputs is shapped like [1,8, n], but we want ot pupulate our 'detections' list iwht list structed as [n, 8] 
            # HINT: One way of going from [1, 8, n] to [n, 8] ......
            # pred = outputs[0]  # Remove batch dimension
            # pred = np.transpose(pred, (1, 0))  # Transpose to [8400, 8]
            
            # TODO: Loop through each detection
            # for detection in pred:
            #     # TODO: Extract coordinates (first 4 values from detection)
            #     # center_x, center_y, width, height = 
            #     
            #     # TODO: Extract confidence scores (last 4 values from detection)  
            #     # class_confidences =
            #     
            #     # TODO: Find the class with highest confidence
            #     # max_confidence = np.max(class_confidences)
            #     # predicted_class_id = np.argmax(class_confidences)
            #     
            #     # TODO: Filter by confidence threshold
            #     # if max_confidence > self.confidence_threshold:
            #         # TODO: Convert center format to corner format and scale coordinates
            #         # converted_bbox = self._convert_coordinates(center_x, center_y, width, height, camera_frame.shape)
            #         # 
            #         # TODO: Add to detections list in required format
            #         # detections.append({
            #         #     'bbox': converted_bbox,
            #         #     'confidence': float(max_confidence),
            #         #     'class_name': self.class_names[predicted_class_id]
            #         # })
            
            # REQUIRED OUTPUT FORMAT:
            # [{'bbox': [x, y, w, h], 'confidence': 0.95, 'class_name': 'Stop_Sign'}, ...]
            
            return detections
            
        except Exception as e:
            print(f"Detection error: {e}")
            return []
    
    def should_stop(self, detected_signs, camera_frame):
        """
        STEP 2: DECISION LOGIC PIPELINE  
        ===============================
        
        CONCEPT: Convert perception data to robot actions
        
        THE CORE INSIGHT:
        How do we estimate distance from a single 2D camera image?
        
        APPROACH 1 - SIZE-BASED REASONING (Simple & Effective):
        - Objects appear larger when closer to camera
        - Measure bounding box area in pixels  
        - Large area = close object = STOP
        - Small area = far object = CONTINUE
        
        APPROACH 2 - DEPTH ESTIMATION (Advanced):
        - Use an AI model to estimate depth at every pixel
        - Sample depths within the bounding box
        - If depth estimates show we're close to the object...... STOP!
        
        IMPORTANT: We must check ALL detected objects
        - Multiple signs might be detected in one frame
        - If any object is close enough, stop the robot
        - Iterate through the entire detected_signs list
        """
        
        if not detected_signs:
            return False  # No objects detected, safe to continue
        
        # APPROACH 1: SIZE-BASED STOPPING (Implement this first)
        # CONCEPT: Bigger bounding box = closer object
        
        # TASK: Check ALL detected objects to see if ANY are close enough to stop
        # TODO: Loop through detected_signs and check each detection
        # for detection in detected_signs:
        #     bbox = detection['bbox']  # [x, y, width, height]
        #     
        #     # TODO: Calculate area of this detection using the width and the heigh
        #     # area = 
        #     
        #     # TODO: Compare to threshold
        #     # if area > self.simple_area_threshold:
        #         # print(f"STOPPING: {detection['class_name']} area {area} > threshold {self.simple_area_threshold}")
        #         # return True
        
        # PLACEHOLDER: Remove this when you implement the loop above
        largest_detection = None  # Replace this
        largest_area = 0
        
        # TODO: Calculate area and compare to threshold
        # EXPERIMENT: Try different thresholds (start with 5000 pixels)
        # - Too low = stops too far away
        # - Too high = doesn't stop until very close
        
        area_threshold = self.simple_area_threshold  # Tune this parameter
        
        if largest_area > area_threshold:
            print(f"🛑 STOPPING: Object area {largest_area} > threshold {area_threshold}")
            return True
        
        # APPROACH 2: DEPTH-BASED STOPPING (Advanced - implement after area works)
        # CONCEPT: Use actual relative distance measurements that come from our monocular depth estimation package
        
        # UNCOMMENT THIS SECTION WHEN USING THE ADVANCED APPROACH, also comment out the return statmeent above that's based on the area threshold
        # return self._advanced_depth_stopping(detected_signs, camera_frame)
        
        return False
    
    def _advanced_depth_stopping(self, detected_signs, camera_frame):
        """
        ADVANCED: DEPTH-BASED DISTANCE ESTIMATION
        =========================================
        
        CONCEPT: Get actual distance measurements using depth estimation
        
        THE PROCESS:
        1. Run depth estimation model on camera frame
        2. For each detected object, sample depth within its bounding box
        3. Calculate average/median depth → real distance
        4. Stop if distance < threshold
        
        DEBUGGING INTEGRATION:
        - This function caches depth results for visualization
        - When you implement this, depth maps will appear in debug panel
        """
        
        if self.depth_estimator is None:
            print("⚠️  Depth estimator not available, falling back to area-based method")
            return False
        
        try:
            # STEP 1: RUN DEPTH MODEL - YOU IMPLEMENT THIS LINE
            # TODO: Get depth map from MiDaS model
            # HINT: depth_map = self.depth_estimator.predict(camera_frame)
            
            depth_map = None  # TODO: Replace this line with actual depth inference
            
            if depth_map is None:
                return False  # Fallback to area-based method
            
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # VERY IMPORTANT! VERY IMPORTANT FOR DEBUGGING!
            # DEBUGGING: Cache depth results for visualization 
            # UNCOMMENT the line below to see depth maps in debug panel
            # self._cached_depth_map = depth_map
            
            # STEP 2: SAMPLE DEPTH VALUES FOR EACH DETECTION
            # DISCOVERY QUESTION: Where in the depth map should you look?
            # ANSWER: Look at pixels inside the detected bounding boxes!
            
            # IMPORTANT: Check ALL detected objects, not just the largest one
            for detection in detected_signs:
                bbox = detection['bbox']  # [x, y, w, h]
                x, y, w, h = bbox
                
                # TODO: Extract depth values within bounding box
                # HINT: depth_region = depth_map[y:y+h, x:x+w]
                
                # DISCOVERY QUESTION: How do you convert depth map values to real distances?
                # EXPERIMENT: Print depth values and see what range they have
                # HINT: Smaller depth values often mean closer objects
                
                # TODO: Calculate representative depth (mean, median, minimum?)
                # representative_depth = np.mean(depth_region)  # or np.median, np.min
                
                # TODO: Convert to real-world distance if needed
                # estimated_distance = self._depth_to_distance(representative_depth)
                
                # TODO: Compare to distance threshold
                # if estimated_distance < self.depth_distance_threshold:
                #     console_logger.stop(f"🛑 STOPPING: {detection['class_name']} at {estimated_distance:.1f}m < {self.depth_distance_threshold}m")
                #     return True
            
            return False  # No objects close enough to stop
            
        except Exception as e:
            print(f"Depth analysis error: {e}")
            return False
    
    def _convert_coordinates(self, center_x, center_y, width, height, original_shape):
        """
        Helper function: Convert model coordinates to image coordinates (provided)
        
        TECHNICAL DETAILS:
        - Convert center format [cx, cy, w, h] to corner format [x, y, w, h]
        - Scale from model size (640x640) back to original camera size (320x240)
        """
        orig_height, orig_width = original_shape[:2]
        
        # Convert center format to corner format
        x = center_x - width / 2
        y = center_y - height / 2
        
        # Scale coordinates from model size to original image size
        scale_x = orig_width / self.input_size
        scale_y = orig_height / self.input_size
        
        x = int(x * scale_x)
        y = int(y * scale_y)
        width = int(width * scale_x)
        height = int(height * scale_y)
        
        # Ensure coordinates are within image bounds
        x = max(0, min(x, orig_width - 1))
        y = max(0, min(y, orig_height - 1))
        width = min(width, orig_width - x)
        height = min(height, orig_height - y)
        
        return [x, y, width, height]

    def _depth_to_distance(self, depth_value):
        """
        Helper function: Convert depth map value to real-world distance (provided)
        
        TECHNICAL NOTE:
        This conversion depends on the specific depth model and calibration.
        MiDaS outputs relative depth, so this is a simplified conversion.
        """
        # Simple conversion - students can experiment with this
        # MiDaS typically outputs inverse depth, so smaller values = closer objects
        if depth_value <= 0:
            return float('inf')  # Invalid depth
        
        # Empirical conversion (you may need to calibrate this for your setup)
        estimated_distance = 1.0 / (depth_value + 0.1)  # Avoid division by zero
        return estimated_distance
    
    def _load_model(self):
        """Load the ONNX model (provided helper function)"""
        try:
            import onnxruntime as ort
            
            if os.path.exists(self.model_path):
                # Optimize for CPU performance
                providers = ['CPUExecutionProvider']
                sess_options = ort.SessionOptions()
                sess_options.inter_op_num_threads = 2
                sess_options.intra_op_num_threads = 2
                sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                self.session = ort.InferenceSession(
                    self.model_path, 
                    sess_options=sess_options,
                    providers=providers
                )
                
                self.input_name = self.session.get_inputs()[0].name
                self.input_shape = self.session.get_inputs()[0].shape
                
                print(f"✅ ONNX model loaded: {self.model_path}")
                print(f"   Model input shape: {self.input_shape}")
                print(f"   Model classes: {self.class_names}")
            else:
                print(f"❌ Model file not found: {self.model_path}")
                self.session = None
                
        except ImportError:
            print("❌ ONNX Runtime not available. Please install: pip install onnxruntime")
            self.session = None
        except Exception as e:
            print(f"❌ Error loading ONNX model: {e}")
            self.session = None
    
    def _load_depth_model(self):
        """Try to load depth estimation model (optional for advanced section)"""
        try:
            # Students will install this when they reach the advanced section
            from midas_depth import DepthEstimator
            self.depth_estimator = DepthEstimator()
            print("✅ Depth estimation model loaded (advanced features available)")
        except ImportError:
            self.depth_estimator = None
            print("⚪ Depth estimation not available (install midas_depth for advanced features)")
        except Exception as e:
            self.depth_estimator = None
            print(f"⚠️  Depth model loading failed: {e}")
    
    def _preprocess_frame(self, camera_frame):
        """
        Preprocess camera frame for ONNX model input (provided helper function)
        
        TECHNICAL DETAILS (students don't need to implement this):
        - Resize 320x240 camera frame to 640x640 model input
        - Convert BGR to RGB color space
        - Normalize pixel values from 0-255 to 0.0-1.0
        - Rearrange dimensions for neural network format
        """
        if camera_frame is None:
            return None
            
        # Resize to model input size
        resized = cv2.resize(camera_frame, (self.input_size, self.input_size))
        
        # Convert to float and normalize
        input_tensor = resized.astype(np.float32)
        input_tensor /= 255.0
        
        # Rearrange dimensions: HWC → CHW → BCHW
        input_tensor = np.transpose(input_tensor, (2, 0, 1))  # HWC to CHW
        input_tensor = np.expand_dims(input_tensor, axis=0)   # Add batch dimension
        
        return input_tensor
    
    def get_cached_depth_map(self):
        """Return cached depth map for debug visualization (provided)"""
        return self._cached_depth_map
```


################################################################################
# FILE: week3_speed_estimation/calibration_script.py
################################################################################

```python
#!/usr/bin/env python3
"""
Week 3 Speed Estimation - Calibration Data Collection Script
===========================================================

This script automatically collects optical flow calibration data by:
1. Recording video while driving the robot at specified power for a known distance
2. Processing optical flow calculations offline from the recorded video
3. Calculating the relationship between optical flow and real speed
4. Saving results to cumulative CSV file

Usage:
    python calibration_script.py --dist 2.0 --pow 30
    python calibration_script.py --dist 2.5 --pow 40
    python calibration_script.py --dist 3.0 --pow 50
"""

import argparse
import time
import cv2
import numpy as np
import pandas as pd
import os
from datetime import datetime
from pathlib import Path
import sys

# Add the parent directory to Python path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import robot hardware modules
from core.camera_manager import camera
from core.utils.movement_controls import MovementController

class CalibrationCollector:
    def __init__(self):
        """Initialize calibration data collector"""
        
        # File paths
        self.csv_file = Path("calibration_data.csv")
        self.videos_dir = Path("videos")
        self.videos_dir.mkdir(exist_ok=True)
        
        # Hardware setup
        self.movement_controller = MovementController()
        
        # Ultrasonic sensor setup
        self.ultrasonic_stop_distance = 0.2  # meters from wall
        
        # Optical flow parameters
        self.feature_params = {
            'maxCorners': 100,
            'qualityLevel': 0.2,
            'minDistance': 10,
            'blockSize': 7
        }
        
        self.lk_params = {
            'winSize': (15, 15),
            'maxLevel': 2,
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Ensure CSV exists with headers
        self._initialize_csv()
        
        print("✅ Calibration collector initialized")
        print(f"📁 Data will be saved to: {self.csv_file}")
        print(f"🎥 Videos will be saved to: {self.videos_dir}/")

    def _initialize_csv(self):
        """Create CSV file with headers if it doesn't exist"""
        if not self.csv_file.exists():
            headers = [
                'run_id', 'timestamp', 'distance', 'motor_power', 
                'recorded_time', 'calculated_speed', 'avg_optical_flow', 
                'num_features', 'include_in_calibration',
                'cumulative_slope', 'cumulative_intercept', 'cumulative_r_squared', 'cumulative_n_points'
            ]
            
            df = pd.DataFrame(columns=headers)
            df.to_csv(self.csv_file, index=False)
            print(f"📝 Created new calibration data file: {self.csv_file}")
        else:
            print(f"📖 Using existing calibration data file: {self.csv_file}")

    def get_ultrasonic_distance(self):
        """Get distance from PiCar-X ultrasonic sensor"""
        try:
            # Using PiCar-X ultrasonic sensor through movement controller
            distance = self.movement_controller.picar.ultrasonic.read()
            return distance / 100.0  # Convert cm to meters
        except Exception as e:
            print(f"Ultrasonic sensor error: {e}")
            return 1.0  # Default safe distance

    def wait_for_camera_ready(self):
        """Wait for camera to be properly initialized and streaming with real frames"""
        print("📷 Waiting for camera to be ready...")
        
        # First, explicitly start streaming if not already running
        if not camera.is_running:
            print("   Starting camera streaming...")
            camera.start_streaming()
            time.sleep(2)  # Give camera time to initialize
        
        # Wait for actual camera frames (not placeholder frames)
        max_attempts = 15
        for attempt in range(max_attempts):
            try:
                frame = camera.get_frame()
                if frame is not None and frame.shape[0] > 0 and frame.shape[1] > 0:
                    # Check if this is a real camera frame, not a placeholder
                    # Placeholder frames are solid color or have text - real frames have more variation
                    
                    # Convert to grayscale to check variance
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    variance = np.var(gray)
                    
                    # Real camera frames should have significant pixel variance
                    # Placeholder frames typically have very low variance
                    if variance > 100:  # Threshold for real vs placeholder frames
                        print("✅ Camera ready with real frames")
                        print(f"   Frame variance: {variance:.1f} (good)")
                        return True
                    else:
                        print(f"   Attempt {attempt + 1}/{max_attempts} - placeholder frame detected (variance: {variance:.1f})")
                else:
                    print(f"   Attempt {attempt + 1}/{max_attempts} - no frame received")
            except Exception as e:
                print(f"   Attempt {attempt + 1}/{max_attempts} - error: {e}")
            
            time.sleep(1)
        
        print("❌ Camera failed to provide real frames")
        print("   Still getting placeholder frames after initialization")
        return False

    def record_video_during_movement(self, distance, motor_power):
        """Record video while robot moves, return video path and movement stats"""
        
        # Get next run ID
        if self.csv_file.exists():
            df = pd.read_csv(self.csv_file)
            run_id = len(df) + 1
        else:
            run_id = 1
        
        # Setup video recording
        video_filename = f"run_{run_id:03d}_{distance:.1f}m_{motor_power}pct_raw.mp4"
        video_path = self.videos_dir / video_filename
        
        # Make sure camera is ready
        if not self.wait_for_camera_ready():
            print("❌ Cannot proceed without camera")
            return None, None
        
        # Setup for data collection
        print(f"🎯 Position robot {distance}m from wall")
        print(f"🛑 Robot will auto-stop at {self.ultrasonic_stop_distance}m from wall")
        input("👆 Press Enter when robot is positioned correctly...")
        
        # Verify initial distance
        initial_distance = self.get_ultrasonic_distance()
        print(f"📡 Initial distance reading: {initial_distance:.2f}m")
        
        if abs(initial_distance - distance) > 0.5:
            print(f"⚠️  Warning: Expected {distance}m but reading {initial_distance:.2f}m")
            print("📝 Continuing with actual distance reading...")
        
        # Get first frame to setup video writer
        first_frame = camera.get_frame()
        if first_frame is None:
            print("❌ Cannot get camera frame")
            return None, None
            
        height, width = first_frame.shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        fps = 20
        
        out = cv2.VideoWriter(str(video_path), fourcc, fps, (width, height))
        
        print(f"🎬 Starting video recording...")
        print(f"🚀 Starting motor at {motor_power}% power...")
        
        # Start robot movement
        self.movement_controller.picar.set_dir_servo_angle(0)  # Straight
        self.movement_controller.picar.forward(motor_power)
        self.movement_controller.picar.set_cam_tilt_angle(-10)
        
        start_time = time.time()
        frame_count = 0
        
        try:
            while True:
                # Get current frame
                current_frame = camera.get_frame()
                if current_frame is not None:
                    # Record raw frame without any text overlays
                    out.write(current_frame)
                    frame_count += 1
                
                # Check if we should stop (ultrasonic sensor)
                current_distance = self.get_ultrasonic_distance()
                if current_distance <= self.ultrasonic_stop_distance:
                    print(f"🛑 Stopping - reached {current_distance:.2f}m from wall")
                    break
                
                # Safety timeout
                current_time = time.time() - start_time
                if current_time > 15.0:  # 15 second max run time
                    print("⏰ Timeout - stopping for safety")
                    break
                
                time.sleep(0.05)  # 20 FPS recording
                
        finally:
            # Stop the robot and close video
            self.movement_controller.picar.stop()
            out.release()
            print("✋ Robot stopped")
        
        # Calculate movement results
        total_time = time.time() - start_time
        actual_distance = distance - self.ultrasonic_stop_distance
        calculated_speed = actual_distance / total_time
        
        movement_stats = {
            'run_id': run_id,
            'total_time': total_time,
            'actual_distance': actual_distance,
            'calculated_speed': calculated_speed,
            'frame_count': frame_count
        }
        
        print(f"📊 MOVEMENT RESULTS:")
        print(f"   Time: {total_time:.2f}s")
        print(f"   Distance: {actual_distance:.2f}m")
        print(f"   Speed: {calculated_speed:.3f} m/s")
        print(f"   Frames recorded: {frame_count}")
        print(f"🎥 Raw video saved: {video_path}")
        
        return video_path, movement_stats

    def calculate_optical_flow(self, prev_frame, curr_frame):
        """Calculate optical flow magnitude between two frames"""
        
        # Convert to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        
        # Detect features in previous frame
        features = cv2.goodFeaturesToTrack(prev_gray, **self.feature_params)
        
        if features is None or len(features) == 0:
            return 0.0, 0, []
        
        # Track features using Lucas-Kanade
        new_features, status, error = cv2.calcOpticalFlowPyrLK(
            prev_gray, curr_gray, features, None, **self.lk_params)
        
        # Calculate flow magnitudes for successfully tracked features
        flow_magnitudes = []
        good_features = []
        
        for i, (prev_pt, curr_pt) in enumerate(zip(features, new_features)):
            if status[i] == 1:  # Successfully tracked
                dx = curr_pt[0][0] - prev_pt[0][0]
                dy = curr_pt[0][1] - prev_pt[0][1]
                magnitude = np.sqrt(dx*dx + dy*dy)
                flow_magnitudes.append(magnitude)
                good_features.append((prev_pt, curr_pt, magnitude))
        
        avg_flow = np.mean(flow_magnitudes) if flow_magnitudes else 0.0
        return avg_flow, len(good_features), good_features

    def create_flow_visualization(self, frame, features_data):
        """Create visualization of optical flow on frame"""
        vis_frame = frame.copy()
        
        # Draw tracked features and flow vectors
        for prev_pt, curr_pt, magnitude in features_data:
            # Draw feature points
            cv2.circle(vis_frame, tuple(prev_pt[0].astype(int)), 3, (0, 255, 0), -1)
            cv2.circle(vis_frame, tuple(curr_pt[0].astype(int)), 3, (0, 0, 255), -1)
            
            # Draw flow vector
            cv2.arrowedLine(vis_frame, 
                          tuple(prev_pt[0].astype(int)), 
                          tuple(curr_pt[0].astype(int)), 
                          (255, 0, 255), 2, tipLength=0.3)
        
        return vis_frame

    def process_optical_flow_from_video(self, video_path, movement_stats):
        """Process optical flow from recorded video"""
        
        print(f"🔄 Processing optical flow from video...")
        
        # Open video file
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print("❌ Could not open video file")
            return None
        
        # Setup output video with flow visualization
        flow_video_path = video_path.parent / video_path.name.replace('_raw.mp4', '_flow.mp4')
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(flow_video_path), fourcc, fps, (width, height))
        
        # Process frames
        flow_data = []
        prev_frame = None
        frame_number = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_number += 1
            
            if prev_frame is not None:
                # Calculate optical flow
                flow_mag, num_features, features_list = self.calculate_optical_flow(prev_frame, frame)
                
                if flow_mag > 0:
                    flow_data.append({
                        'flow_magnitude': flow_mag,
                        'num_features': num_features,
                        'frame_number': frame_number
                    })
                
                # Create visualization
                vis_frame = self.create_flow_visualization(frame, features_list)
                
                # Add flow info to frame
                cv2.putText(vis_frame, f"Flow: {flow_mag:.1f} px/frame", 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(vis_frame, f"Features: {num_features}", 
                           (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                out.write(vis_frame)
            else:
                # First frame - just write as is
                out.write(frame)
            
            prev_frame = frame
        
        cap.release()
        out.release()
        
        # Calculate average optical flow
        if flow_data:
            avg_optical_flow = np.mean([f['flow_magnitude'] for f in flow_data])
            avg_features = np.mean([f['num_features'] for f in flow_data])
        else:
            avg_optical_flow = 0.0
            avg_features = 0
            print("⚠️  No optical flow data calculated!")
        
        print(f"✅ Optical flow processing complete")
        print(f"   Average flow: {avg_optical_flow:.2f} px/frame")
        print(f"   Average features: {avg_features:.0f}")
        print(f"🎥 Flow visualization saved: {flow_video_path}")
        
        return {
            'avg_optical_flow': avg_optical_flow,
            'num_features': avg_features,
            'flow_video_path': flow_video_path
        }

    def run_calibration(self, distance, motor_power):
        """Run a single calibration data collection sequence"""
        
        print(f"\n{'='*50}")
        print(f"🚗 CALIBRATION RUN")
        print(f"📏 Target distance: {distance}m")
        print(f"⚡ Motor power: {motor_power}%")
        print(f"{'='*50}")
        
        # Step 1: Record video during movement
        video_path, movement_stats = self.record_video_during_movement(distance, motor_power)
        
        if video_path is None or movement_stats is None:
            print("❌ Video recording failed")
            return
        
        # Step 2: Process optical flow from recorded video
        flow_results = self.process_optical_flow_from_video(video_path, movement_stats)
        
        if flow_results is None:
            print("❌ Optical flow processing failed")
            return
        
        # Step 3: Save results to CSV
        self._save_to_csv(
            movement_stats['run_id'],
            distance,
            motor_power,
            movement_stats['total_time'],
            movement_stats['calculated_speed'],
            flow_results['avg_optical_flow'],
            flow_results['num_features']
        )
        
        print(f"💾 Data saved to CSV")

    def _save_to_csv(self, run_id, distance, motor_power, recorded_time, 
                     calculated_speed, avg_optical_flow, num_features):
        """Save run data to CSV and calculate cumulative statistics"""
        
        # Create new row data
        new_data = {
            'run_id': run_id,
            'timestamp': datetime.now().isoformat(),
            'distance': distance,
            'motor_power': motor_power,
            'recorded_time': recorded_time,
            'calculated_speed': calculated_speed,
            'avg_optical_flow': avg_optical_flow,
            'num_features': num_features,
            'include_in_calibration': True
        }
        
        # Load existing data
        if self.csv_file.exists():
            df = pd.read_csv(self.csv_file)
        else:
            df = pd.DataFrame()
        
        # Add new row
        new_row = pd.DataFrame([new_data])
        df = pd.concat([df, new_row], ignore_index=True)
        
        # Calculate cumulative statistics
        calibration_data = df[df['include_in_calibration'] == True]
        
        if len(calibration_data) >= 2:
            flows = calibration_data['avg_optical_flow'].values
            speeds = calibration_data['calculated_speed'].values
            
            # Linear regression: speed = slope * flow + intercept
            slope, intercept = np.polyfit(flows, speeds, 1)
            
            # Calculate R-squared
            predicted_speeds = slope * flows + intercept
            ss_res = np.sum((speeds - predicted_speeds) ** 2)
            ss_tot = np.sum((speeds - np.mean(speeds)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            n_points = len(calibration_data)
        else:
            slope, intercept, r_squared, n_points = 0, 0, 0, len(calibration_data)
        
        # Update the last row with cumulative stats
        df.loc[df.index[-1], 'cumulative_slope'] = slope
        df.loc[df.index[-1], 'cumulative_intercept'] = intercept
        df.loc[df.index[-1], 'cumulative_r_squared'] = r_squared
        df.loc[df.index[-1], 'cumulative_n_points'] = n_points
        
        # Save CSV
        df.to_csv(self.csv_file, index=False)
        
        # Print cumulative results
        print(f"\n🔄 CUMULATIVE CALIBRATION ({n_points} runs):")
        if n_points >= 2:
            print(f"   Linear fit: speed = {slope:.6f} × flow + ({intercept:.6f})")
            print(f"   R-squared: {r_squared:.3f}")
            print(f"\n📋 Copy to speed_estimator.py:")
            print(f"   self.flow_to_speed_slope = {slope:.6f}")
            print(f"   self.flow_to_speed_intercept = {intercept:.6f}")
        else:
            print(f"   Need at least 2 runs for calibration")


def main():
    parser = argparse.ArgumentParser(description='Collect optical flow calibration data')
    parser.add_argument('--dist', type=float, required=True,
                       help='Distance from wall in meters (e.g., 2.0)')
    parser.add_argument('--pow', type=int, required=True,
                       help='Motor power percentage (e.g., 30)')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.dist <= 0 or args.dist > 5:
        print("❌ Error: Distance must be between 0 and 5 meters")
        return
    
    if args.pow <= 0 or args.pow > 100:
        print("❌ Error: Power must be between 1 and 100 percent")
        return
    
    # Run calibration
    collector = CalibrationCollector()
    collector.run_calibration(args.dist, args.pow)


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week3_speed_estimation/show_stats.py
################################################################################

```python
#!/usr/bin/env python3
"""
Week 3 Speed Estimation - Calibration Statistics Viewer
=======================================================

This script displays current calibration statistics without running the robot.
Shows individual run data and cumulative calibration parameters.

Usage:
    python show_stats.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import sys

class CalibrationStatsViewer:
    def __init__(self):
        """Initialize calibration statistics viewer"""
        self.csv_file = Path("calibration_data.csv")
        
        if not self.csv_file.exists():
            print("❌ No calibration data found!")
            print("   Run calibration_script.py first to collect data")
            sys.exit(1)
        
        # Load data
        self.df = pd.read_csv(self.csv_file)
        print(f"📖 Loaded calibration data from: {self.csv_file}")

    def show_summary_stats(self):
        """Display summary statistics"""
        total_runs = len(self.df)
        calibration_runs = len(self.df[self.df['include_in_calibration'] == True])
        
        print(f"\n{'='*60}")
        print(f"📊 CALIBRATION STATISTICS SUMMARY")
        print(f"{'='*60}")
        print(f"Total runs: {total_runs}")
        print(f"Runs used for calibration: {calibration_runs}")
        
        if total_runs > 0:
            last_updated = self.df['timestamp'].iloc[-1]
            print(f"Last updated: {last_updated}")

    def show_current_calibration(self):
        """Display current calibration parameters"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) < 2:
            print(f"\n⚠️  INSUFFICIENT DATA FOR CALIBRATION")
            print(f"   Need at least 2 runs, currently have {len(calibration_data)}")
            return
        
        # Get latest cumulative stats
        latest_row = self.df.iloc[-1]
        slope = latest_row['cumulative_slope']
        intercept = latest_row['cumulative_intercept']
        r_squared = latest_row['cumulative_r_squared']
        n_points = int(latest_row['cumulative_n_points'])
        
        print(f"\n🎯 CURRENT CALIBRATION:")
        print(f"   speed = {slope:.6f} × flow + ({intercept:.6f})")
        print(f"   R-squared: {r_squared:.3f}")
        print(f"   Based on {n_points} data points")
        
        # Quality assessment
        if r_squared >= 0.95:
            quality = "Excellent! 🌟"
        elif r_squared >= 0.90:
            quality = "Very Good ✅"
        elif r_squared >= 0.80:
            quality = "Good 👍"
        elif r_squared >= 0.70:
            quality = "Fair ⚠️"
        else:
            quality = "Poor - collect more data ❌"
        
        print(f"   Calibration quality: {quality}")
        
        print(f"\n📋 COPY TO SPEED_ESTIMATOR.PY:")
        print(f"   self.flow_to_speed_slope = {slope:.6f}")
        print(f"   self.flow_to_speed_intercept = {intercept:.6f}")

    def show_individual_runs(self):
        """Display individual run data"""
        print(f"\n📝 INDIVIDUAL RUN DATA:")
        print(f"{'Run':<4} {'Distance':<8} {'Power':<6} {'Speed':<8} {'Flow':<8} {'Features':<9} {'Include'}")
        print(f"{'-'*60}")
        
        for _, row in self.df.iterrows():
            run_id = int(row['run_id'])
            distance = row['distance']
            power = int(row['motor_power'])
            speed = row['calculated_speed']
            flow = row['avg_optical_flow']
            features = int(row['num_features'])
            include = "✓" if row['include_in_calibration'] else "✗"
            
            print(f"{run_id:<4} {distance:<8.1f} {power:<6}% {speed:<8.3f} {flow:<8.1f} {features:<9} {include}")

    def show_data_distribution(self):
        """Show data distribution statistics"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) == 0:
            return
        
        print(f"\n📈 DATA DISTRIBUTION:")
        
        # Speed range
        speed_min = calibration_data['calculated_speed'].min()
        speed_max = calibration_data['calculated_speed'].max()
        speed_mean = calibration_data['calculated_speed'].mean()
        
        print(f"   Speed range: {speed_min:.3f} - {speed_max:.3f} m/s (avg: {speed_mean:.3f})")
        
        # Flow range
        flow_min = calibration_data['avg_optical_flow'].min()
        flow_max = calibration_data['avg_optical_flow'].max()
        flow_mean = calibration_data['avg_optical_flow'].mean()
        
        print(f"   Flow range: {flow_min:.1f} - {flow_max:.1f} px/frame (avg: {flow_mean:.1f})")
        
        # Distance and power coverage
        distances = calibration_data['distance'].unique()
        powers = calibration_data['motor_power'].unique()
        
        print(f"   Distances tested: {sorted(distances)} meters")
        print(f"   Power levels tested: {sorted(powers)}%")

    def create_scatter_plot(self, save_plot=False):
        """Create scatter plot of flow vs speed"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) < 2:
            print("⚠️  Not enough data for scatter plot")
            return
        
        flows = calibration_data['avg_optical_flow'].values
        speeds = calibration_data['calculated_speed'].values
        
        # Get calibration line
        latest_row = self.df.iloc[-1]
        slope = latest_row['cumulative_slope']
        intercept = latest_row['cumulative_intercept']
        
        # Create plot
        plt.figure(figsize=(10, 6))
        plt.scatter(flows, speeds, c='blue', alpha=0.7, s=100, edgecolors='black')
        
        # Plot calibration line
        flow_range = np.linspace(flows.min() * 0.9, flows.max() * 1.1, 100)
        speed_line = slope * flow_range + intercept
        plt.plot(flow_range, speed_line, 'r--', linewidth=2, 
                label=f'Calibration: speed = {slope:.4f} × flow + {intercept:.4f}')
        
        plt.xlabel('Optical Flow (pixels/frame)')
        plt.ylabel('Speed (m/s)')
        plt.title('Optical Flow vs Speed Calibration')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add R² to plot
        r_squared = latest_row['cumulative_r_squared']
        plt.text(0.05, 0.95, f'R² = {r_squared:.3f}', transform=plt.gca().transAxes,
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        if save_plot:
            plot_path = Path("calibration_plot.png")
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            print(f"📊 Plot saved to: {plot_path}")
        
        plt.show()

    def show_recommendations(self):
        """Show recommendations for improving calibration"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        print(f"\n💡 RECOMMENDATIONS:")
        
        if len(calibration_data) < 3:
            print("   • Collect more data points (aim for 5-8 runs)")
        
        if len(calibration_data) > 0:
            # Check speed range
            speed_range = calibration_data['calculated_speed'].max() - calibration_data['calculated_speed'].min()
            if speed_range < 0.3:
                print("   • Try more varied motor power settings for wider speed range")
            
            # Check R-squared
            if len(calibration_data) >= 2:
                r_squared = self.df.iloc[-1]['cumulative_r_squared']
                if r_squared < 0.90:
                    print("   • Consider collecting more data to improve fit quality")
                    print("   • Check videos for consistent feature tracking")
        
        # Check distance variety
        distances = calibration_data['distance'].unique() if len(calibration_data) > 0 else []
        if len(distances) < 2:
            print("   • Test multiple distance settings (2.0m, 2.5m, 3.0m)")
        
        # Check power variety
        powers = calibration_data['motor_power'].unique() if len(calibration_data) > 0 else []
        if len(powers) < 2:
            print("   • Test multiple power settings (30%, 40%, 50%)")

    def toggle_run_inclusion(self, run_id):
        """Toggle whether a specific run is included in calibration"""
        if run_id < 1 or run_id > len(self.df):
            print(f"❌ Invalid run ID: {run_id}")
            return
        
        # Toggle inclusion status
        current_status = self.df.loc[self.df['run_id'] == run_id, 'include_in_calibration'].iloc[0]
        new_status = not current_status
        self.df.loc[self.df['run_id'] == run_id, 'include_in_calibration'] = new_status
        
        # Recalculate cumulative statistics
        self._recalculate_cumulative_stats()
        
        # Save updated CSV
        self.df.to_csv(self.csv_file, index=False)
        
        status_text = "included" if new_status else "excluded"
        print(f"✅ Run {run_id} {status_text} from calibration")

    def _recalculate_cumulative_stats(self):
        """Recalculate cumulative statistics after toggling run inclusion"""
        calibration_data = self.df[self.df['include_in_calibration'] == True]
        
        if len(calibration_data) >= 2:
            flows = calibration_data['avg_optical_flow'].values
            speeds = calibration_data['calculated_speed'].values
            
            # Linear regression
            slope, intercept = np.polyfit(flows, speeds, 1)
            
            # Calculate R-squared
            predicted_speeds = slope * flows + intercept
            ss_res = np.sum((speeds - predicted_speeds) ** 2)
            ss_tot = np.sum((speeds - np.mean(speeds)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            n_points = len(calibration_data)
        else:
            slope, intercept, r_squared, n_points = 0, 0, 0, len(calibration_data)
        
        # Update all rows with new cumulative stats
        self.df['cumulative_slope'] = slope
        self.df['cumulative_intercept'] = intercept
        self.df['cumulative_r_squared'] = r_squared
        self.df['cumulative_n_points'] = n_points

    def interactive_menu(self):
        """Display interactive menu for data management"""
        while True:
            print(f"\n{'='*40}")
            print("📋 CALIBRATION DATA MANAGER")
            print(f"{'='*40}")
            print("1. Show summary")
            print("2. Show current calibration")
            print("3. Show individual runs")
            print("4. Show data distribution")
            print("5. Create scatter plot")
            print("6. Toggle run inclusion")
            print("7. Show recommendations")
            print("8. Exit")
            
            try:
                choice = input("\nSelect option (1-8): ").strip()
                
                if choice == '1':
                    self.show_summary_stats()
                elif choice == '2':
                    self.show_current_calibration()
                elif choice == '3':
                    self.show_individual_runs()
                elif choice == '4':
                    self.show_data_distribution()
                elif choice == '5':
                    try:
                        import matplotlib.pyplot as plt
                        self.create_scatter_plot(save_plot=True)
                    except ImportError:
                        print("⚠️  Matplotlib not available for plotting")
                elif choice == '6':
                    self.show_individual_runs()
                    try:
                        run_id = int(input("Enter run ID to toggle: "))
                        self.toggle_run_inclusion(run_id)
                    except ValueError:
                        print("❌ Please enter a valid run ID number")
                elif choice == '7':
                    self.show_recommendations()
                elif choice == '8':
                    print("👋 Goodbye!")
                    break
                else:
                    print("❌ Invalid choice. Please select 1-8.")
                    
            except KeyboardInterrupt:
                print("\n👋 Goodbye!")
                break
            except Exception as e:
                print(f"❌ Error: {e}")


def main():
    """Main function - show all stats by default or interactive mode"""
    import sys
    
    viewer = CalibrationStatsViewer()
    
    # If run with --interactive flag, show menu
    if len(sys.argv) > 1 and sys.argv[1] == '--interactive':
        viewer.interactive_menu()
    else:
        # Default: show all statistics
        viewer.show_summary_stats()
        viewer.show_current_calibration()
        viewer.show_individual_runs()
        viewer.show_data_distribution()
        viewer.show_recommendations()


if __name__ == "__main__":
    main()
```


################################################################################
# FILE: week3_speed_estimation/speed_estimator.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import json
import os

class SpeedEstimator:
    """
    Week 3 Implementation: Optical Flow Speed Estimation
    
    Students will implement optical flow analysis to estimate robot speed
    from camera frames, following the Lucas-Kanade method from the academic paper.
    """
    
    def __init__(self):
        """Initialize optical flow parameters and load calibration"""
        
        # Optical flow parameters (students may need to tune these)
        self.feature_params = {
            'maxCorners': 100,      # Maximum number of features to track
            'qualityLevel': 0.3,    # Quality level for corner detection
            'minDistance': 7,       # Minimum distance between features
            'blockSize': 7          # Size of averaging block for corner detection
        }
        
        self.lk_params = {
            'winSize': (15, 15),    # Window size for Lucas-Kanade
            'maxLevel': 2,          # Maximum pyramid levels
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Speed calibration parameters (loaded from calibration file)
        self.calibration_loaded = False
        self.flow_to_speed_slope = 1.0
        self.flow_to_speed_intercept = 0.0
        
        # Internal state for tracking
        self.previous_gray = None
        self.previous_features = None

        self.speed_history = []
        self.max_history_length = 10
        
        # Exponential smoothing for speed
        self.smoothed_speed = 0.0
        self.alpha = 0.3  # Smoothing factor: 0.1=very smooth, 0.9=very responsive

        # Load calibration parameters
        self._load_calibration()
        
        print("SpeedEstimator initialized - Students: Implement estimate_speed()!")
    
    def _load_calibration(self):
        """Load speed calibration parameters from file"""
        try:
            calibration_file = os.path.join(os.path.dirname(__file__), 'calibration_params.json')
            
            if os.path.exists(calibration_file):
                with open(calibration_file, 'r') as f:
                    params = json.load(f)
                
                self.flow_to_speed_slope = params.get('slope', 1.0)
                self.flow_to_speed_intercept = params.get('intercept', 0.0)
                self.calibration_loaded = True
                
                print(f"✅ Calibration loaded: slope={self.flow_to_speed_slope:.3f}, intercept={self.flow_to_speed_intercept:.3f}")
            else:
                print(f"⚠️  No calibration file found: {calibration_file}")
                print("   Using default parameters. Run speed calibration first!")
                
        except Exception as e:
            print(f"❌ Error loading calibration: {e}")
    
    def estimate_speed(self, current_frame, previous_frame):
        """
        Main speed estimation function students must implement
        
        Args:
            current_frame: numpy array of shape (240, 320, 3) - current RGB image
            previous_frame: numpy array of shape (240, 320, 3) - previous RGB image
            
        Returns:
            speed: float - estimated speed in units/second (calibrated)
        """
        
        if previous_frame is None:
            return 0.0
        
        try:
            # =============================================================
            # STEP 1: CONVERT TO GRAYSCALE (following paper section 1.3)
            # Optical flow works on grayscale images
            # =============================================================
            
            # Convert both frames to grayscale using cv2.cvtColor()
            current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
            previous_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
            
            # =============================================================
            # STEP 2: FEATURE DETECTION (following paper)
            # Find good features to track in the previous frame
            # =============================================================
            
            # Use cv2.goodFeaturesToTrack() to find corner features
            # This identifies distinct points that can be reliably tracked
            features_prev = cv2.goodFeaturesToTrack(
                previous_gray,
                maxCorners=self.feature_params['maxCorners'],
                qualityLevel=self.feature_params['qualityLevel'],
                minDistance=self.feature_params['minDistance'],
                blockSize=self.feature_params['blockSize']
            )
            
            if features_prev is None or len(features_prev) == 0:
                return 0.0  # No features to track
            
            # =============================================================
            # STEP 3: OPTICAL FLOW CALCULATION (following paper section 1.1)
            # Use Lucas-Kanade method to track features between frames
            # =============================================================
            
            # Use cv2.calcOpticalFlowPyrLK() to track features from previous to current frame
            # This implements the Lucas-Kanade algorithm with pyramids
            features_curr, status, error = cv2.calcOpticalFlowPyrLK(
                previous_gray,
                current_gray,
                features_prev,
                None,
                **self.lk_params
            )
            
            # =============================================================
            # STEP 4: FILTER GOOD FEATURES
            # Only use features that were successfully tracked
            # =============================================================
            
            if features_curr is None or status is None:
                return 0.0
            
            # Filter out features where tracking failed (status == 0)
            # Keep only good features for flow calculation
            good_prev = []
            good_curr = []
            
            for i, (status_flag, err) in enumerate(zip(status, error)):
                if status_flag == 1:  # Good tracking
                    good_prev.append(features_prev[i])
                    good_curr.append(features_curr[i])
            
            if len(good_prev) < 10:  # Need minimum features for reliable estimation
                return 0.0
            
            # =============================================================
            # STEP 5: CALCULATE FLOW MAGNITUDE
            # Compute the magnitude of optical flow vectors
            # =============================================================
            
            # For each tracked feature, calculate the displacement vector
            # Compute the magnitude (length) of each displacement
            # Take the average magnitude as the overall flow measure
            flow_magnitudes = []
            
            # Calculate flow magnitude for each good feature pair
            for prev_pt, curr_pt in zip(good_prev, good_curr):
                dx = curr_pt[0][0] - prev_pt[0][0]
                dy = curr_pt[0][1] - prev_pt[0][1]
                magnitude = np.sqrt(dx*dx + dy*dy)
                flow_magnitudes.append(magnitude)
            
            if not flow_magnitudes:
                return 0.0
            
            # Average flow magnitude
            avg_flow_magnitude = np.mean(flow_magnitudes)
            
            # =============================================================
            # STEP 6: CONVERT FLOW TO SPEED (using calibration)
            # Apply calibration parameters to get real-world speed
            # =============================================================
            
            # Apply linear calibration: speed = slope * flow + intercept
            # This converts pixel flow to real-world speed units
            if self.calibration_loaded:
                speed = self.flow_to_speed_slope * avg_flow_magnitude + self.flow_to_speed_intercept
            else:
                # Without calibration, just return raw flow magnitude
                speed = avg_flow_magnitude
            
            # Ensure speed is non-negative
            speed = max(0.0, speed)

            # Apply exponential smoothing (only smooth valid readings)
            if speed > 0.02:  # Only smooth speeds above 0.02 m/s (noise threshold)
                self.smoothed_speed = self.alpha * speed + (1 - self.alpha) * self.smoothed_speed
            elif speed < 0.01:  # If speed is very low, gradually decay smoothed speed
                self.smoothed_speed *= 0.95  # Slow decay when stopped

            # Store speed in history for smoothing
            self.speed_history.append(speed)
            if len(self.speed_history) > self.max_history_length:
                self.speed_history.pop(0)
            
            return float(speed)
            
        except Exception as e:
            print(f"Speed estimation error: {e}")
            return 0.0
    
    def is_calibrated(self):
        """Check if speed calibration parameters are available"""
        return self.calibration_loaded
    
    def get_speed_history(self):
        """Get speed history for web interface and smoothing"""
        if not self.speed_history:
            return {
                'current_speed': 0.0,
                'smoothed_speed': 0.0,
                'speed_history': [],
                'avg_speed': 0.0
            }
        
        return {
            'current_speed': self.speed_history[-1],
            'smoothed_speed': self.smoothed_speed,  # ← Fixed: added 'self.'
            'speed_history': self.speed_history.copy(),
            'avg_speed': sum(self.speed_history) / len(self.speed_history)
        }

# =============================================================================
# HELPFUL REFERENCE CODE (for students to adapt)
# =============================================================================

"""
EXAMPLE FEATURE DETECTION:
features = cv2.goodFeaturesToTrack(
    gray_image,
    maxCorners=100,
    qualityLevel=0.3,
    minDistance=7,
    blockSize=7
)

EXAMPLE OPTICAL FLOW:
features_next, status, error = cv2.calcOpticalFlowPyrLK(
    old_gray, 
    new_gray, 
    features_prev, 
    None, 
    **lk_params
)

EXAMPLE FLOW MAGNITUDE CALCULATION:
for i, (prev_pt, curr_pt) in enumerate(zip(good_prev, good_curr)):
    if status[i] == 1:  # Successfully tracked
        dx = curr_pt[0][0] - prev_pt[0][0]
        dy = curr_pt[0][1] - prev_pt[0][1]
        magnitude = np.sqrt(dx*dx + dy*dy)
        flow_magnitudes.append(magnitude)

avg_flow = np.mean(flow_magnitudes)
"""
```


################################################################################
# FILE: week3_speed_estimation/speed_estimator_skeleton.py
################################################################################

```python
#!/usr/bin/env python3

import cv2
import numpy as np
import json
import os

class SpeedEstimator:
    """
    Week 3 Implementation: Optical Flow Speed Estimation
    
    Students will implement optical flow analysis to estimate robot speed
    from camera frames, following the Lucas-Kanade method from the academic paper.
    """
    
    def __init__(self):
        """Initialize optical flow parameters and load calibration"""
        
        # Optical flow parameters (students may need to tune these)
        self.feature_params = {
            'maxCorners': 100,      # Maximum number of features to track
            'qualityLevel': 0.3,    # Quality level for corner detection
            'minDistance': 7,       # Minimum distance between features
            'blockSize': 7          # Size of averaging block for corner detection
        }
        
        self.lk_params = {
            'winSize': (15, 15),    # Window size for Lucas-Kanade
            'maxLevel': 2,          # Maximum pyramid levels
            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        }
        
        # Speed calibration parameters (loaded from calibration file)
        self.calibration_loaded = False
        self.flow_to_speed_slope = 1.0
        self.flow_to_speed_intercept = 0.0
        
        # Internal state for tracking
        self.previous_gray = None
        self.previous_features = None

        self.speed_history = []
        self.max_history_length = 10

        # Load calibration parameters
        self._load_calibration()
        
        print("SpeedEstimator initialized - Students: Implement estimate_speed()!")
    
    def _load_calibration(self):
        """Load speed calibration parameters from file"""
        try:
            calibration_file = os.path.join(os.path.dirname(__file__), 'calibration_params.json')
            
            if os.path.exists(calibration_file):
                with open(calibration_file, 'r') as f:
                    params = json.load(f)
                
                self.flow_to_speed_slope = params.get('slope', 1.0)
                self.flow_to_speed_intercept = params.get('intercept', 0.0)
                self.calibration_loaded = True
                
                print(f"✅ Calibration loaded: slope={self.flow_to_speed_slope:.3f}, intercept={self.flow_to_speed_intercept:.3f}")
            else:
                print(f"⚠️  No calibration file found: {calibration_file}")
                print("   Using default parameters. Run speed calibration first!")
                
        except Exception as e:
            print(f"❌ Error loading calibration: {e}")
    
    def estimate_speed(self, current_frame, previous_frame):
        """
        Main speed estimation function students must implement
        
        Args:
            current_frame: numpy array of shape (240, 320, 3) - current RGB image
            previous_frame: numpy array of shape (240, 320, 3) - previous RGB image
            
        Returns:
            speed: float - estimated speed in units/second (calibrated)
        """
        
        if previous_frame is None:
            return 0.0
        
        try:
            # =============================================================
            # STEP 1: CONVERT TO GRAYSCALE (following paper section 1.3)
            # Optical flow works on grayscale images
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Convert both frames to grayscale using cv2.cvtColor()
            
            current_gray = None   # TODO: Convert current frame to grayscale
            previous_gray = None  # TODO: Convert previous frame to grayscale
            
            # =============================================================
            # STEP 2: FEATURE DETECTION (following paper)
            # Find good features to track in the previous frame
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Use cv2.goodFeaturesToTrack() to find corner features
            # This identifies distinct points that can be reliably tracked
            
            features_prev = None  # TODO: Detect features in previous frame
            
            if features_prev is None or len(features_prev) == 0:
                return 0.0  # No features to track
            
            # =============================================================
            # STEP 3: OPTICAL FLOW CALCULATION (following paper section 1.1)
            # Use Lucas-Kanade method to track features between frames
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Use cv2.calcOpticalFlowPyrLK() to track features from previous to current frame
            # This implements the Lucas-Kanade algorithm with pyramids
            
            features_curr = None  # TODO: Track features using Lucas-Kanade
            status = None         # TODO: Get tracking status for each feature
            error = None          # TODO: Get tracking error for each feature
            
            # =============================================================
            # STEP 4: FILTER GOOD FEATURES
            # Only use features that were successfully tracked
            # =============================================================
            
            if features_curr is None or status is None:
                return 0.0
            
            # STUDENTS IMPLEMENT:
            # Filter out features where tracking failed (status == 0)
            # Keep only good features for flow calculation
            
            good_prev = []  # TODO: Filter previous features
            good_curr = []  # TODO: Filter current features
            
            # for i, (status_flag, err) in enumerate(zip(status, error)):
            #     if status_flag == 1:  # Good tracking
            #         good_prev.append(features_prev[i])
            #         good_curr.append(features_curr[i])
            
            if len(good_prev) < 10:  # Need minimum features for reliable estimation
                return 0.0
            
            # =============================================================
            # STEP 5: CALCULATE FLOW MAGNITUDE
            # Compute the magnitude of optical flow vectors
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # For each tracked feature, calculate the displacement vector
            # Compute the magnitude (length) of each displacement
            # Take the average magnitude as the overall flow measure
            
            flow_magnitudes = []
            
            # TODO: Calculate flow magnitude for each good feature pair
            # for prev_pt, curr_pt in zip(good_prev, good_curr):
            #     dx = curr_pt[0] - prev_pt[0]
            #     dy = curr_pt[1] - prev_pt[1]
            #     magnitude = np.sqrt(dx*dx + dy*dy)
            #     flow_magnitudes.append(magnitude)
            
            if not flow_magnitudes:
                return 0.0
            
            # Average flow magnitude
            avg_flow_magnitude = np.mean(flow_magnitudes)
            
            # =============================================================
            # STEP 6: CONVERT FLOW TO SPEED (using calibration)
            # Apply calibration parameters to get real-world speed
            # =============================================================
            
            # STUDENTS IMPLEMENT:
            # Apply linear calibration: speed = slope * flow + intercept
            # This converts pixel flow to real-world speed units
            
            if self.calibration_loaded:
                speed = self.flow_to_speed_slope * avg_flow_magnitude + self.flow_to_speed_intercept
            else:
                # Without calibration, just return raw flow magnitude
                speed = avg_flow_magnitude
            
            # Ensure speed is non-negative
            speed = max(0.0, speed)

            self.speed_history.append(speed)
            if len(self.speed_history) > self.max_history_length:
                self.speed_history.pop(0)
            
            return float(speed)
            
        except Exception as e:
            print(f"Speed estimation error: {e}")
            return 0.0
    
    def is_calibrated(self):
        """Check if speed calibration parameters are available"""
        return self.calibration_loaded
    
    def get_speed_history(self):
        """Get speed history for web interface and smoothing"""
        if not self.speed_history:
            return {
                'current_speed': 0.0,
                'smoothed_speed': 0.0,
                'speed_history': [],
                'avg_speed': 0.0
            }
        
        # Calculate smoothed speed (average of last 3 readings)
        smoothing_window = min(3, len(self.speed_history))
        recent_speeds = self.speed_history[-smoothing_window:]
        smoothed_speed = sum(recent_speeds) / len(recent_speeds)
        
        return {
            'current_speed': self.speed_history[-1],
            'smoothed_speed': smoothed_speed,
            'speed_history': self.speed_history.copy(),
            'avg_speed': sum(self.speed_history) / len(self.speed_history)
        }

# =============================================================================
# HELPFUL REFERENCE CODE (for students to adapt)
# =============================================================================

"""
EXAMPLE FEATURE DETECTION:
features = cv2.goodFeaturesToTrack(
    gray_image,
    maxCorners=100,
    qualityLevel=0.3,
    minDistance=7,
    blockSize=7
)

EXAMPLE OPTICAL FLOW:
features_next, status, error = cv2.calcOpticalFlowPyrLK(
    old_gray, 
    new_gray, 
    features_prev, 
    None, 
    **lk_params
)

EXAMPLE FLOW MAGNITUDE CALCULATION:
for i, (prev_pt, curr_pt) in enumerate(zip(good_prev, good_curr)):
    if status[i] == 1:  # Successfully tracked
        dx = curr_pt[0][0] - prev_pt[0][0]
        dy = curr_pt[0][1] - prev_pt[0][1]
        magnitude = np.sqrt(dx*dx + dy*dy)
        flow_magnitudes.append(magnitude)

avg_flow = np.mean(flow_magnitudes)
"""
```